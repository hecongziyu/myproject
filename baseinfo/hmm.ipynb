{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sambaiga.github.io/2017/05/03/hmm-intro.html\n",
    "https://hmmlearn.readthedocs.io/en/latest/tutorial.html#\n",
    "1、Given the model parameters and observed data, estimate the optimal sequence of hidden states.\n",
    "2、Given the model parameters and observed data, calculate the model likelihood.\n",
    "3、Given just the observed data, estimate the model parameters.\n",
    "    The first and the second problem can be solved by the dynamic programming algorithms known as the Viterbi algorithm and the Forward-Backward algorithm, respectively. The last one can be solved by an iterative Expectation-Maximization (EM) algorithm, known as the Baum-Welch algorithm.\n",
    "    \n",
    "https://github.com/tostq/Easy_HMM  https://www.cnblogs.com/fangbei/p/8409110.html\n",
    "\n",
    "https://www.jb51.net/article/137068.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.11188012  0.31890218]\n",
      " [ 1.57921282  0.76743473]\n",
      " [-0.58087813 -0.52516981]\n",
      " [ 0.24196227 -1.91328024]\n",
      " [ 0.81644508 -1.523876  ]\n",
      " [-0.90802408 -1.4123037 ]\n",
      " [-0.62947496  0.59772047]\n",
      " [-0.54438272  0.11092259]\n",
      " [-0.60025385  0.94743982]\n",
      " [-0.60170661  1.85227818]]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "GaussianHMM(algorithm='viterbi', covariance_type='full', covars_prior=0.01,\n",
      "      covars_weight=1, init_params='stmc', means_prior=0, means_weight=0,\n",
      "      min_covar=0.001, n_components=3, n_iter=10, params='stmc',\n",
      "      random_state=None, startprob_prior=1.0, tol=0.01, transmat_prior=1.0,\n",
      "      verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Building HMM and generating samples\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "np.random.seed(42)\n",
    "model = hmm.GaussianHMM(n_components=3, covariance_type=\"full\")\n",
    "model.startprob_ = np.array([0.6, 0.3, 0.1])\n",
    "model.transmat_ = np.array([[0.7, 0.2, 0.1],\n",
    "                            [0.3, 0.5, 0.2],\n",
    "                            [0.3, 0.3, 0.4]])\n",
    "model.means_ = np.array([[0.0, 0.0], [3.0, -3.0], [5.0, 10.0]])\n",
    "model.covars_ = np.tile(np.identity(2), (3, 1, 1))\n",
    "X, Z = model.sample(10)\n",
    "print(X)\n",
    "print(Z)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "https://www.cnblogs.com/pinard/p/7001397.html\n",
    "https://www.cnblogs.com/pinard/p/6945257.html\n",
    "\n",
    "    hmmlearn实现了三种HMM模型类，按照观测状态是连续状态还是离散状态，可以分为两类。GaussianHMM和GMMHMM是连续观测状态的HMM模型，而MultinomialHMM是离散观测状态的模型，也是我们在HMM原理系列篇里面使用的模型。\n",
    "\n",
    "    对于MultinomialHMM的模型，使用比较简单，\"startprob_\"参数对应我们的隐藏状态初始分布ΠΠ, \"transmat_\"对应我们的状态转移矩阵AA, \"emissionprob_\"对应我们的观测状态概率矩阵BB。\n",
    "    \n",
    "    对于连续观测状态的HMM模型，GaussianHMM类假设观测状态符合高斯分布，而GMMHMM类则假设观测状态符合混合高斯分布。一般情况下我们使用GaussianHMM即高斯分布的观测状态即可。以下对于连续观测状态的HMM模型，我们只讨论GaussianHMM类。\n",
    "　　\n",
    "      在GaussianHMM类中，\"startprob_\"参数对应我们的隐藏状态初始分布ΠΠ, \"transmat_\"对应我们的状态转移矩阵AA, 比较特殊的是观测状态概率的表示方法，此时由于观测状态是连续值，我们无法像MultinomialHMM一样直接给出矩阵BB。而是采用给出各个隐藏状态对应的观测状态高斯分布的概率密度函数的参数。\n",
    "　　　\n",
    "       如果观测序列是一维的，则观测状态的概率密度函数是一维的普通高斯分布。如果观测序列是NN维的，则隐藏状态对应的观测状态的概率密度函数是NN维高斯分布。高斯分布的概率密度函数参数可以用μμ表示高斯分布的期望向量，ΣΣ表示高斯分布的协方差矩阵。在GaussianHMM类中，“means”用来表示各个隐藏状态对应的高斯分布期望向量μμ形成的矩阵，而“covars”用来表示各个隐藏状态对应的高斯分布协方差矩阵ΣΣ形成的三维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立HMM的模型\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "states = [\"box 1\", \"box 2\", \"box3\"]\n",
    "n_states = len(states)\n",
    "\n",
    "observations = [\"red\", \"white\"]\n",
    "n_observations = len(observations)\n",
    "\n",
    "# 隐藏状态初始分布 从第一个盒子抽球的概率是0.2，从第二个盒子抽球的概率是0.4，从第三个盒子抽球的概率是0.4\n",
    "start_probability = np.array([0.2, 0.4, 0.4])\n",
    "\n",
    "# 状态转移矩阵 规则是：如果当前抽球的盒子是第一个盒子，则以0.5的概率仍然留在第一个盒子继续抽球，以0.2的\n",
    "# 概率去第二个盒子抽球，以0.3的概率去第三个盒子抽球。如果当前抽球的盒子是第二个盒子，则以0.5的概率仍然\n",
    "# 留在第二个盒子继续抽球，以0.3的概率去第一个盒子抽球，以0.2的概率去第三个盒子抽球。如果当前抽球的盒子\n",
    "# 是第三个盒子，则以0.5的概率仍然留在第三个盒子继续抽球，以0.2的概率去第一个盒子抽球，以0.3的概率去第二个盒子抽球\n",
    "\n",
    "transition_probability = np.array([\n",
    "  [0.5, 0.2, 0.3],\n",
    "  [0.3, 0.5, 0.2],\n",
    "  [0.2, 0.3, 0.5]\n",
    "])\n",
    "\n",
    "# 观测状态概率矩阵\n",
    "emission_probability = np.array([\n",
    "  [0.5, 0.5],\n",
    "  [0.4, 0.6],\n",
    "  [0.7, 0.3]\n",
    "])\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=n_states)\n",
    "model.startprob_=start_probability\n",
    "model.transmat_=transition_probability\n",
    "model.emissionprob_=emission_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ball picked: white, red, red\n",
      "The hidden box box3, box3, box3\n",
      "-4.219907785197447\n",
      "The ball picked: white, red, red\n",
      "The hidden box box3, box3, box3\n",
      "-2.0481134318001732\n"
     ]
    }
   ],
   "source": [
    "seen = np.array([[1,0,0]]).T\n",
    "logprob, box = model.decode(seen, algorithm=\"viterbi\")\n",
    "\n",
    "print(\"The ball picked:\", \", \".join(map(lambda x: observations[x], seen.T[0])))\n",
    "print(\"The hidden box\", \", \".join(map(lambda x: states[x], box)))\n",
    "print(logprob)\n",
    "\n",
    "box2 = model.predict(seen)\n",
    "print(\"The ball picked:\", \", \".join(map(lambda x: observations[x], seen.T[0])))\n",
    "print(\"The hidden box\", \", \".join(map(lambda x: states[x], box2)))\n",
    "\n",
    "# 要注意的是score函数返回的是以自然对数为底的对数概率值，我们在HMM问题一中手动计算的结果是未取对数的原始概率是0.13022。对比一下\n",
    "print(model.score(seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start prob --> [2.89022329e-25 1.09413666e-06 9.99998906e-01]\n",
      "trans mat --> [[3.40773388e-01 1.79017210e-01 4.80209402e-01]\n",
      " [2.24271166e-01 4.61275912e-01 3.14452923e-01]\n",
      " [8.81098008e-01 1.18868352e-01 3.36406690e-05]]\n",
      "emiss prob --> [[6.68345428e-02 9.33165457e-01]\n",
      " [6.93541723e-01 3.06458277e-01]\n",
      " [9.99395293e-01 6.04707365e-04]]\n",
      "score --> -6.7050721844728915\n",
      "start prob --> [1.00000000e+00 3.96767136e-17 6.82676142e-24]\n",
      "trans mat --> [[4.15293605e-08 4.34476155e-01 5.65523804e-01]\n",
      " [8.85924170e-01 6.03941147e-02 5.36817150e-02]\n",
      " [4.32469213e-01 2.88216408e-01 2.79314378e-01]]\n",
      "emiss prob --> [[9.99919972e-01 8.00276697e-05]\n",
      " [2.98240092e-01 7.01759908e-01]\n",
      " [3.13422007e-02 9.68657799e-01]]\n",
      "score --> -6.342000292727823\n",
      "start prob --> [1.00000000e+00 1.55834643e-54 3.27495267e-15]\n",
      "trans mat --> [[3.38646266e-11 6.91669561e-01 3.08330439e-01]\n",
      " [5.50973960e-01 4.31614399e-01 1.74116411e-02]\n",
      " [9.82210652e-01 1.30012398e-02 4.78810828e-03]]\n",
      "emiss prob --> [[9.99995100e-01 4.89994811e-06]\n",
      " [2.40372567e-04 9.99759627e-01]\n",
      " [6.17054395e-01 3.82945605e-01]]\n",
      "score --> -6.034108603033876\n"
     ]
    }
   ],
   "source": [
    "# HMM问题二，求解模型参数的问题。由于鲍姆-韦尔奇算法是基于EM算法的近似算法，所以\n",
    "# 我们需要多跑几次，比如下面我们跑三次，选择一个比较优的模型参数\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "states = [\"box 1\", \"box 2\", \"box3\"]\n",
    "n_states = len(states)\n",
    "\n",
    "observations = [\"red\", \"white\"]\n",
    "n_observations = len(observations)\n",
    "model2 = hmm.MultinomialHMM(n_components=n_states, n_iter=20, tol=0.01)\n",
    "X2 = np.array([[0,1,0,1],[0,0,0,1],[1,0,1,1]])\n",
    "model2.fit(X2)\n",
    "print('start prob -->',model2.startprob_)\n",
    "print('trans mat -->', model2.transmat_)\n",
    "print('emiss prob -->', model2.emissionprob_)\n",
    "print('score -->', model2.score(X2))\n",
    "model2.fit(X2)\n",
    "print('start prob -->',model2.startprob_)\n",
    "print('trans mat -->', model2.transmat_)\n",
    "print('emiss prob -->', model2.emissionprob_)\n",
    "print('score -->', model2.score(X2))\n",
    "model2.fit(X2)\n",
    "print('start prob -->',model2.startprob_)\n",
    "print('trans mat -->', model2.transmat_)\n",
    "print('emiss prob -->', model2.emissionprob_)\n",
    "print('score -->', model2.score(X2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "-41.121128137687\n"
     ]
    }
   ],
   "source": [
    "# GaussianHMM实例\n",
    "# 观测状态是二维的，而隐藏状态有4个。因此我们的“means”参数是4×2的矩阵，而“covars”协方差参数是4×2×2的张量\n",
    "startprob = np.array([0.6, 0.3, 0.1, 0.0])\n",
    "# The transition matrix, note that there are no transitions possible\n",
    "# between component 1 and 3\n",
    "transmat = np.array([[0.7, 0.2, 0.0, 0.1],\n",
    "                     [0.3, 0.5, 0.2, 0.0],\n",
    "                     [0.0, 0.3, 0.5, 0.2],\n",
    "                     [0.2, 0.0, 0.2, 0.6]])\n",
    "# The means of each component\n",
    "means = np.array([[0.0,  0.0],\n",
    "                  [0.0, 11.0],\n",
    "                  [9.0, 10.0],\n",
    "                  [11.0, -1.0]])\n",
    "# The covariance of each component\n",
    "covars = .5 * np.tile(np.identity(2), (4, 1, 1))\n",
    "\n",
    "# Build an HMM instance and set parameters\n",
    "model3 = hmm.GaussianHMM(n_components=4, covariance_type=\"full\")\n",
    "\n",
    "# Instead of fitting it from the data, we directly set the estimated\n",
    "# parameters, the means and covariance of the components\n",
    "model3.startprob_ = startprob\n",
    "model3.transmat_ = transmat\n",
    "model3.means_ = means\n",
    "model3.covars_ = covars\n",
    "\n",
    "seen = np.array([[1.1,2.0],[-1,2.0],[3,7]])\n",
    "logprob, state = model3.decode(seen, algorithm=\"viterbi\")\n",
    "print(state)\n",
    "# 对数概率\n",
    "print(model3.score(seen))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
