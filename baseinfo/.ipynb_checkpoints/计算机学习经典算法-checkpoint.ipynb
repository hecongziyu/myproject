{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/weixin_42039090/article/category/7699914/4?\n",
    "\n",
    "\n",
    "https://blog.csdn.net/zhangzhengyi03539/article/details/76783352   计算机科学中最重要的32个算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 一 KNN算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    分类思想比较简单，从训练样本中找出 K 个与其最相近的样本，然后看这 k 个样本中哪个类别的样本多，则待判定的值（或说抽样）就属于这个类别。\n",
    "    缺点：\n",
    "    1）K 值需要预先设定，而不能自适应，一般选择20，如果数据量小的话，可以调参。\n",
    "    2）当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的 K 个邻居中大容量类的样本占多数。这个缺点是ＫＮＮ算法不可避免的。\n",
    "    该算法适用于对样本容量比较大的类域进行自动分类。\n",
    "\n",
    "    算法步骤：\n",
    "    step.1---初始化距离为最大值\n",
    "    step.2---计算未知样本和每个训练样本的距离dist\n",
    "    step.3---得到目前K个最临近样本中的最大距离maxdist\n",
    "    step.4---如果dist小于maxdist，则将该训练样本作为K-最近  邻样本\n",
    "    step.5---重复步骤2、3、4，直到未知样本和所有训练样本的  距离都算完\n",
    "    step.6---统计K个最近邻样本中每个类别出现的次数\n",
    "    step.7---选择出现频率最大的类别作为未知样本的类别   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import neighbors\n",
    "# neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform',algorithm='auto', leaf_size=30,p=2, metric=’minkowski’,\n",
    "# metric_params=None,n_jobs=1)\n",
    "# knn=KNeighborsClassifier()\n",
    "# X_train,X_test, y_train,y_test = cross_validation.train_test_split(train_data,train_target,test_size=0.4,random_state=0)\n",
    "# 参数解释： train_data：所要划分的样本特征集 train_target：所要划分的样本结果 test_size：样本占比，如果是整数的话就是样本的数量 \n",
    "# random_state：是随机数的种子。 \n",
    "# knn.fit( X_train, y_train)\n",
    "# 预测  这里输入X一个数组，形式类似于(如果是一个二维特征的话)：[ [0,1 ],[2,1]...]\n",
    "# knn.predict(X)\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors importKNeighborsClassifier\n",
    "X_train =np.array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])  \n",
    "y_train=['0','1','1','1']\n",
    "knn=KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train,y_train)\n",
    "knn.predict([[5,0],[4,0]])#要注意哦，预测的时候也要上使用数组形式的\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 二 K-Means算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    k-meansalgorithm算法是一个聚类算法，把n的对象根据他们的属性分为k个分割，k < n。假设对象属性来自于空间向量，并且目标是使各个群组内部的均方误差总和最小。假设有k个群组Si, i=1,2,...,k。μi是群组Si内所有元素xj的质心，或叫中心点。k平均聚类发明于1956年， 该算法最常见的形式是采用被称为劳埃德算法(Lloydalgorithm)的迭代式改进探索法。劳埃德算法首先把输入点分成k个初始化分组，可以是随机的或者使用一些启发式数据。然后计算每组的中心点，根据中心点的位置把对象分到离它最近的中心，重新确定分组。继续重复不断地计算中心并重新分组，直到收敛，即对象不再改变分组（中心点位置不再改变）。\n",
    "    从算法的表现上来说，它并不保证一定得到全局最优解，最终解的质量很大程度上取决于初始化的分组。由于该算法的速度很快，因此常用的一种方法是多次运行k平均算法，选择最优解。k平均算法的一个缺点是，分组的数目k是一个输入参数，不合适的k可能返回较差的结果。另外，算法还假设均方误差是计算群组分散度的最佳参数。\n",
    "\n",
    "    基本方法：\n",
    "    一、从数据中随机抽取k个点作为初始聚类的中心，由这个中心代表各个聚类。\n",
    "    二、计算数据中所有的样本点到这k个点的距离，将每个样本点都归到最近的聚类里。\n",
    "    三、重新计算聚类中心点，方法就是将第二步中的中心移动到聚类的几何中心（即平均值）处。\n",
    "    四、重复第2步直到聚类的中心不再移动，此时算法收敛。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import numpy as np\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "kmeans = KMeans(init='k-means++',n_clusters=num_clusters, n_init=10)\n",
    "kmeans.fit(data)\n",
    "# Step size of the mesh\n",
    "step = 0.05\n",
    "# Plot the boundaries\n",
    "x_min, x_max = min(data[:, 0]) - 1,max(data[:, 0]) + 1\n",
    "y_min, y_max = min(data[:, 1]) - 1,max(data[:, 1]) + 1\n",
    "x_values, y_values =np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step))\n",
    "predicted_labels =predicted_labels.reshape(x_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 三 CART树算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    Classification And Regression(回归) Tree(CART)是决策树的一种，并且是非常重要的决策树，属于Top Ten Machine Learning Algorithm。顾名思义，CART算法既可以用于创建分类树（ClassificationTree），也可以用于创建回归树（Regression Tree）、模型树（Model Tree），两者在建树的过程稍有差异。采用基于最小距离的基尼指数估计函数，用来决定由该子数据集生成的决策树的拓展形。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。\n",
    "    \n",
    "    CART算法的重要基础包含以下三个方面:\n",
    "    （1）二分(BinarySplit)：在每次判断过程中，都是对观察变量进行二分。\n",
    "        CART算法采用一种二分递归分割的技术，算法总是将当前样本集分割为两个子样本集，使得生成的决策树的每个非叶结点都只有两个分枝。因此CART算法生成的决策树是结构简洁的二叉树。因此CART算法适用于样本特征的取值为是或非的场景，对于连续特征的处理则与C4.5算法相似。\n",
    "    （2）单变量分割(SplitBased on One Variable)：每次最优划分都是针对单个变量。\n",
    "    （3）剪枝策略：CART算法的关键点，也是整个Tree-Based算法的关键步骤。 剪枝过程特别重要，所以在最优决策树生成过程中占有重要地位。有研究表明，剪枝过程的重要性要比树生成过程更为重要，对于不同的划分标准生成的最大树(Maximum Tree)，在剪枝之后都能够保留最重要的属性划分，差别不大。反而是剪枝方法对于最优树的生成更为关键。\n",
    "    \n",
    "    优点：\n",
    "    1）非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。\n",
    "    2）在面对诸如存在缺失值、变量数多等问题时 CART 显得非常稳健。    \n",
    "    \n",
    "    基本思路：\n",
    "    1.  决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；\n",
    "    2.  决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。\n",
    "    \n",
    "    CART决策树的生成就是递归地构建二叉决策树的过程。CART决策树既可以用于分类也可以用于回归。本文我们仅讨论用于分类的CART。对分类树而言，CART用Gini系数最小化准则来进行特征选择，生成二叉树。 CART生成算法如下：\n",
    "\n",
    "    根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：\n",
    "     1.设结点的训练数据集为D，计算现有特征对该数据集的Gini系数。此时，对每一个特征A，对其可能取的每个值a，根据样本点对A=a的测试为“是”或 “否”将D分割成D1和D2两部分，计算A=a时的Gini系数。\n",
    "     2.在所有可能的特征A以及它们所有可能的切分点a中，选择Gini系数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。\n",
    "     3.对两个子结点递归地调用步骤l~2，直至满足停止条件。\n",
    "     4.生成CART决策树。\n",
    "     算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的Gini系数小于预定阈值（样本基本属于同一类），或者没有更多特征。\n",
    "     \n",
    "     https://blog.csdn.net/yeziand01/article/details/80731078 Gini系数\n",
    "     \n",
    "     关于过拟合以及剪枝问题：决策树很容易发生过拟合，也就是对训练数据集适应得非常好，但却在测试数据集上表现得不尽人如意。这个时候我们可以通过阈值控制终止条件避免树形结构分支过细，也可以通过对已经形成的决策树进行剪枝来避免过拟合。另外一个克服过拟合的手段就是采用基于Bootstrap的思想建立随机森林（Random Forest）实现分类问题。关于剪枝的内容可以参考其他文献。\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "#clf = tree.DecisionTreeClassifier(criterion='gini')\n",
    "class MYCartClassifier:\n",
    "    def __init__(self,Modul=tree.DecisionTreeClassifier,dataset=np.empty((3,3))):\n",
    "        self.modul=Modul       \n",
    "        self.dataset=dataset\n",
    "    defReadFile(self,input_file):\n",
    "        # Reading the data\n",
    "        X = [];\n",
    "        with open(input_file,'r') as f:\n",
    "            for line inf.readlines():\n",
    "#               data=line.split(',')\n",
    "                data =line[:-1].split(',')                \n",
    "                X.append(data)\n",
    "        X = np.array(X)\n",
    "        # Convert string datato numerical data\n",
    "        label_encoder = []\n",
    "        X_encoded =np.empty(X.shape)\n",
    "        for i,item inenumerate(X[0]):\n",
    "           label_encoder.append(preprocessing.LabelEncoder())\n",
    "            X_encoded[:, i] =label_encoder[-1].fit_transform(X[:, i])\n",
    "        X =X_encoded.astype(int)\n",
    "        self.dataset=X\n",
    "    def SetModule(self,Modul):\n",
    "        self.modul=Modul\n",
    "    def FitModule(self):\n",
    "        X=self.dataset[:,:-1]\n",
    "        y=self.dataset[:,-1]\n",
    "        self.modul.fit(X,y)\n",
    "        from sklearn importmodel_selection\n",
    "        accuracy =model_selection.cross_val_score(self.modul,X, y, scoring='accuracy', cv=3)\n",
    "        print(\"Accuracyof the classifier: \" + str(round(100*accuracy.mean(), 2)) + \"%\")\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini')\n",
    "MY=MYCartClassifier(Modul=clf)\n",
    "MY.ReadFile(\"d:\\\\car.data.txt\")\n",
    "MY.FitModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四 朴素贝叶斯算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。算法的基础是概率问题，分类原理是通过某对象的先验概率，利用贝叶斯公式计算出其后验概率，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。朴素贝叶斯假设是约束性很强的假设，假设特征条件独立，但朴素贝叶斯算法简单，快速，具有较小的出错率。\n",
    "    贝叶斯定理提供了一种从P(h)、P(D)和 P(D|h) 计算后验概率 P(h|D) 的方法\n",
    "    P ( h|D) 是已知预示变量（属性）的前提下，类（目标）的后验概率\n",
    "    P ( h ) 是类的先验概率\n",
    "    P ( D|h) 是可能性，即已知类的前提下，预示变量的概率\n",
    "    P ( D ) 是预示变量的先验概率\n",
    "    \n",
    "    算法优点：\n",
    "    1.优点：对于在小数据集上有显著特征的相关对象，朴素贝叶斯方法可对其进行快速分类\n",
    "    2.场景举例：情感分析、消费者分类\n",
    "    \n",
    "    朴素贝叶斯算法基本步骤：\n",
    "    1）、找到一个已知分类的待分类项集合，这个集合叫做训练样本集。\n",
    "    2）、统计得到在各类别下各个特征属性的条件概率估计。即。P(a1|y1),P(a2|y1),…,P(am|y1),P(a1|y2),P(a1|y2),P(am|y2),…,P(am|y2)\n",
    "    3）、如果各个特征属性是条件独立的，则根据贝叶斯定理有如下推导 ：P(y_{i}|x)=\\frac{P(x|y_{i})P(y_{i})}{P(x)}\n",
    "    \n",
    "    https://blog.csdn.net/sh199210/article/details/51778028  协方差    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "class MyBayesClassifier:\n",
    "    def__init__(self,moudle=GaussianNB()):\n",
    "        self.moudle=moudle\n",
    "    def loadSet(self):\n",
    "        input_file ='d:\\\\test.txt'\n",
    "        X = []\n",
    "        y = []\n",
    "        with open(input_file,'r') as f:\n",
    "            for line inf.readlines():\n",
    "                data =[float(x) for x in line.split(',')]\n",
    "                X.append(data[:-1])\n",
    "                y.append(data[-1])\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "\n",
    "    def data_select_train_test(self): \n",
    "        from sklearn importcross_validation\n",
    "        self.X_train,self.X_test, self.y_train, self.y_test =cross_validation.train_test_split(self.X,self.y, test_size=0.25, random_state=5)\n",
    "\n",
    "    def fit_X_Complete(self):   \n",
    "        self.moudle.fit(self.X, self.y)\n",
    "        self.y_pred =self.moudle.predict(self.X)\n",
    "\n",
    "    def fit_X_train(self):   \n",
    "        self.moudle.fit(self.X_train, self.y_train)\n",
    "        self.y_test_pred =self.moudle.predict(self.X_test)\n",
    "\n",
    "    def __plot_classifier(self,X,y):   \n",
    "        x_min, x_max =min(X[:, 0]) - 1.0, max(X[:, 0]) + 1.0\n",
    "        y_min, y_max =min(X[:, 1]) - 1.0, max(X[:, 1]) + 1.0\n",
    "        step_size = 0.01\n",
    "        x_values, y_values =np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max,step_size))\n",
    "        mesh_output =self.moudle.predict(np.c_[x_values.ravel(), y_values.ravel()])\n",
    "        mesh_output =mesh_output.reshape(x_values.shape)\n",
    "        plt.figure()\n",
    "        plt.pcolormesh(x_values, y_values, mesh_output, cmap=plt.cm.gray)\n",
    "        plt.scatter(X[:, 0],X[:, 1], c=y, s=80, edgecolors='black', linewidth=1, cmap=plt.cm.Paired)\n",
    "        plt.xlim(x_values.min(), x_values.max())\n",
    "        plt.ylim(y_values.min(),y_values.max())\n",
    "        plt.xticks((np.arange(int(min(X[:, 0])-1), int(max(X[:, 0])+1), 1.0)))\n",
    "       plt.yticks((np.arange(int(min(X[:, 1])-1), int(max(X[:, 1])+1), 1.0)))\n",
    "\n",
    "        plt.show()   \n",
    "\n",
    "    defassess_module_plot(self,isComplete=True): #isComplete表示是否为全集还是训练集。\n",
    "\n",
    "        if isComplete:\n",
    "\n",
    "            accuracy = 100.0 *(self.y ==self.y_pred).sum() / self.X.shape[0]\n",
    "\n",
    "           print(\"Accuracy of the classifier =\", round(accuracy, 2),\"%\")\n",
    "\n",
    "           self.__plot_classifier(self.X, self.y)\n",
    "\n",
    "        else:\n",
    "\n",
    "            accuracy = 100.0 *(self.y_test ==self.y_test_pred).sum() / self.X_test.shape[0]\n",
    "\n",
    "           print(\"Accuracy of the classifier =\", round(accuracy, 2),\"%\")\n",
    "\n",
    "           self.__plot_classifier(self.X_test, self.y_test)            \n",
    "\n",
    "    def asscess_module_score(self,isComplete=True):\n",
    "        from sklearn importcross_validation\n",
    "        num_validations = 5\n",
    "        MethodList=['accuracy','f1_weighted','precision_weighted','recall_weighted']\n",
    "        if isComplete:\n",
    "            for yy inMethodList:\n",
    "                tt=cross_validation.cross_val_score(self.moudle,self.X, self.y,scoring=yy, cv=num_validations)\n",
    "                print(yy+\" \"+ str(round(100*tt.mean(), 2)) + \"%\\n\")\n",
    "        else:\n",
    "            for yy in MethodList:\n",
    "                tt=cross_validation.cross_val_score(self.moudle,self.X_test,self.y_test,scoring=yy, cv=num_validations)\n",
    "                print(yy+\" \"+ str(round(100*tt.mean(), 2)) +\"%\\n\")           \n",
    "moudle=GaussianNB()\n",
    "MyBayes=MyBayesClassifier(moudle)\n",
    "MyBayes.loadSet()\n",
    "MyBayes.data_select_train_test()\n",
    "MyBayes.fit_X_Complete()\n",
    "MyBayes.fit_X_train()\n",
    "MyBayes.assess_module_plot(True)\n",
    "MyBayes.asscess_module_score(True)\n",
    "MyBayes.assess_module_plot(False)\n",
    "MyBayes.asscess_module_score(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 五 AdaBoost算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    Adaboost 是一种迭代算法，本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器(强分类器)。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。\n",
    "    \n",
    "    整个过程如下所示：\n",
    "    1. 先通过对 N 个训练样本的学习得到第一个弱分类器 ；\n",
    "    2. 将分错的样本和其他的新数据一起构成一个新的N 个的训练样本，通过对这个样本的学习得到第二个弱分类器；\n",
    "    3. 将第一个和第二个都分错了的样本加上其他的新样本构成另一个新的 N个的训练样本，通过对这个样本的学习得到第三个弱分类器；\n",
    "    4. 如此反复，最终得到经过提升的强分类器。\n",
    "    \n",
    "    Adaboost是一种有很高精度的分类器，优点：\n",
    "    （1）可以使用各种方法构建子分类器,Adaboost算法提供的是框架。\n",
    "    （2）利用简单分类器进行组合，计算出的结果是可以理解的。\n",
    "    （3）简单，不用做特征筛选。\n",
    "    （4）不用担心overfitting(过拟合)。\n",
    "    \n",
    "    AdaBoost算法实现原理：\n",
    "    1. 给定训练样本集S，其中X和Y分别对应于正例样本和负例样本；T为训练的最大循环次数；\n",
    "    2. 初始化样本权重为1/n ，即为训练样本的初始概率分布；　　\n",
    "    3. 第一次迭代：\n",
    "        (1)训练样本的概率分布相当，训练弱分类器;\n",
    "        (2)计算弱分类器的错误率;\n",
    "        (3)选取合适阈值，使得误差最小；\n",
    "        (4)更新样本权重； 　　\n",
    "        经T次循环后，得到T个弱分类器，按更新的权重叠加，最终得到的强分类器，具体思路大家可以参考相关文献资料，这里不赘述了。\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 六 SVM算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    SupportVector Machine（支持向量机）。基于统计学习理论的一种机器学习方法。SVM是建立在统计学习理论的VC维理论和结构风险最小原理基础上的，根据有限的样本信息在模型的复杂性之间寻求最佳折衷，以期获得最好的推广能力（或泛化能力）。支持向量：支持或支撑平面上把两类类别划分开来的超平面的向量点。简单的说，就是将数据单元表示在多维空间中，然后对这个空间做划分的算法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。\n",
    "    基于分类边界的分类算法的目标是，通过训练，找到这些分类之间的边界（直线的――称为线性划分，曲线的――称为非线性划分）。对于多维数据（如 N 维），可以将它们视为 N 维空间中的点，而分类边界就是 N 维空间中的面，称为超面（超面比 N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。\n",
    "     支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。SVM 在解决小样本、非线性及高维模式识别问题中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。\n",
    "     \n",
    "    基础步骤：\n",
    "    https://blog.csdn.net/bbbeoy/article/details/72468868"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 七 Apriori算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    Apriori算法是经典的挖掘频繁项集和关联规则的数据挖掘算法。A priori在拉丁语中指\"来自以前\"。当定义问题时，通常会使用先验知识或者假设，这被称作\"一个先验\"（a priori）。Apriori算法的名字正是基于这样的事实：算法使用频繁项集性质的先验性质，即频繁项集的所有非空子集也一定是频繁的。Apriori算法使用一种称为逐层搜索的迭代方法，其中k项集用于探索(k+1)项集。首先，通过扫描数据库，累计每个项的计数，并收集满足最小支持度的项，找出频繁1项集的集合。该集合记为L1。然后，使用L1找出频繁2项集的集合L2，使用L2找出L3，如此下去，直到不能再找到频繁k项集。每找出一个Lk需要一次数据库的完整扫描。Apriori算法使用频繁项集的先验性质来压缩搜索空间。\n",
    "    \n",
    "    Apriori 算法分为两个阶段：\n",
    "    （1）寻找频繁项集\n",
    "    （2）由频繁项集找关联规则\n",
    "    \n",
    "    该算法的基本思想是：首先找出所有的频集，这些项集出现的频繁性至少和预定义的最小支持度一样。然后由频集产生强关联规则，这些规则必须满足最小支持度和最小可信度。然后使用第1步找到的频集产生期望的规则，产生只包含集合的项的所有规则，其中每一条规则的右部只有一项，这里采用的是中规则的定义。一旦这些规则被生成，那么只有那些大于用户给定的最小可信度的规则才被留下来。为了生成所有频集，使用了递推的方法\n",
    "\n",
    "    算法缺点：\n",
    "      （1） 在每一步产生侯选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；\n",
    "      （2） 每次计算项集的支持度时，都对数据库中 的全部记录进行了一遍扫描比较，\n",
    "\n",
    "    算法基本步骤：\n",
    "        ① 首先单趟扫描数据集，计算各个一项集的支持度，根据给定的最小支持度闵值，得到一项频繁集L1。\n",
    "        ② 然后通过连接运算，得到二项候选集，对每个候选集再次扫描数据集，得出每个候选集的支持度，再与最小支持度比较。得到二项频繁集L2。\n",
    "        ③ 如此进行下去，直到不能连接产生新的候选集为止。\n",
    "        ④ 对于找到的所有频繁集，用规则提取算法进行关联规则的提取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-2-61d7177b6f66>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-61d7177b6f66>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def load_data_set():\n",
    "    '''\n",
    "    Load a sample data set (From Data Mining: Concepts and Techniques, 3th Edition)\n",
    "    Returns:\n",
    "        A data set: A list of transactions. Each transaction contains several items.\n",
    "    '''\n",
    "    data_set = [['l1', 'l2', 'l5'], ['l2', 'l4'], ['l2', 'l3'],\n",
    "            ['l1', 'l2', 'l4'], ['l1', 'l3'], ['l2', 'l3'],\n",
    "            ['l1', 'l3'], ['l1', 'l2', 'l3', 'l5'], ['l1', 'l2', 'l3']]\n",
    "    return data_set\n",
    "\n",
    "def create_C1(data_set):\n",
    "    \"\"\"\n",
    "    Create frequent candidate 1-itemset C1 by scaning data set.\n",
    "    Args:\n",
    "        data_set: A list of transactions. Each transaction contains several items.\n",
    "    Returns:\n",
    "        C1: A set which contains all frequent candidate 1-itemsets\n",
    "    \"\"\"\n",
    "    C1 = set()\n",
    "    for t in data_set:\n",
    "        for item in t:\n",
    "            item_set = frozenset([item])\n",
    "            C1.add(item_set)\n",
    "    return C1\n",
    "\n",
    "def is_apriori(Ck_item, Lksub1):\n",
    "    \"\"\"\n",
    "    Judge whether a frequent candidate k-itemset satisfy Apriori property.\n",
    "    Args:\n",
    "        Ck_item: a frequent candidate k-itemset in Ck which contains all frequent\n",
    "                 candidate k-itemsets.\n",
    "        Lksub1: Lk-1, a set which contains all frequent candidate (k-1)-itemsets.\n",
    "    Returns:\n",
    "        True: satisfying Apriori property.\n",
    "        False: Not satisfying Apriori property.\n",
    "    \"\"\"\n",
    "    for item in Ck_item:\n",
    "        sub_Ck = Ck_item - frozenset([item])\n",
    "        if sub_Ck not in Lksub1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def create_Ck(Lksub1, k):\n",
    "    \"\"\"\n",
    "    Create Ck, a set which contains all all frequent candidate k-itemsets\n",
    "    by Lk-1's own connection operation.\n",
    "    Args:\n",
    "        Lksub1: Lk-1, a set which contains all frequent candidate (k-1)-itemsets.\n",
    "        k: the item number of a frequent itemset.\n",
    "    Return:\n",
    "        Ck: a set which contains all all frequent candidate k-itemsets.\n",
    "    \"\"\"\n",
    "    Ck = set()\n",
    "    len_Lksub1 = len(Lksub1)\n",
    "    list_Lksub1 = list(Lksub1)\n",
    "    for i in range(len_Lksub1):\n",
    "        for j in range(1, len_Lksub1):\n",
    "            l1 = list(list_Lksub1[i])\n",
    "            l2 = list(list_Lksub1[j])\n",
    "            l1.sort()\n",
    "            l2.sort()\n",
    "            if l1[0:k-2] == l2[0:k-2]:\n",
    "                Ck_item = list_Lksub1[i] | list_Lksub1[j]\n",
    "                # pruning\n",
    "                if is_apriori(Ck_item, Lksub1):\n",
    "                    Ck.add(Ck_item)\n",
    "    return Ck\n",
    "\n",
    "def generate_Lk_by_Ck(data_set, Ck, min_support, support_data):\n",
    "    \"\"\"\n",
    "    Generate Lk by executing a delete policy from Ck.\n",
    "    Args:\n",
    "        data_set: A list of transactions. Each transaction contains several items.\n",
    "        Ck: A set which contains all all frequent candidate k-itemsets.\n",
    "        min_support: The minimum support.\n",
    "        support_data: A dictionary. The key is frequent itemset and the value is support.\n",
    "    Returns:\n",
    "        Lk: A set which contains all all frequent k-itemsets.\n",
    "    \"\"\"\n",
    "    Lk = set()\n",
    "    item_count = {}\n",
    "    for t in data_set:\n",
    "        for item in Ck:\n",
    "            if item.issubset(t):\n",
    "                if item not in item_count:\n",
    "                    item_count[item] = 1\n",
    "                else:\n",
    "                    item_count[item] += 1\n",
    "    t_num = float(len(data_set))\n",
    "    for item in item_count:\n",
    "        if (item_count[item] / t_num) >= min_support:\n",
    "            Lk.add(item)\n",
    "            support_data[item] = item_count[item] / t_num\n",
    "    return Lk\n",
    "\n",
    "def generate_L(data_set, k, min_support):\n",
    "    \"\"\"\n",
    "    Generate all frequent itemsets.\n",
    "    Args:\n",
    "        data_set: A list of transactions. Each transaction contains several items.\n",
    "        k: Maximum number of items for all frequent itemsets.\n",
    "        min_support: The minimum support.\n",
    "    Returns:\n",
    "        L: The list of Lk.\n",
    "        support_data: A dictionary. The key is frequent itemset and the value is support.\n",
    "    \"\"\"\n",
    "    support_data = {}\n",
    "    C1 = create_C1(data_set)\n",
    "    L1 = generate_Lk_by_Ck(data_set, C1, min_support, support_data)\n",
    "    Lksub1 = L1.copy()\n",
    "    L = []\n",
    "    L.append(Lksub1)\n",
    "    for i in range(2, k+1):\n",
    "        Ci = create_Ck(Lksub1, i)\n",
    "        Li = generate_Lk_by_Ck(data_set, Ci, min_support, support_data)\n",
    "        Lksub1 = Li.copy()\n",
    "        L.append(Lksub1)\n",
    "    return L, support_data\n",
    "\n",
    "def generate_big_rules(L, support_data, min_conf):\n",
    "    \"\"\"\n",
    "    Generate big rules from frequent itemsets.\n",
    "    Args:\n",
    "        L: The list of Lk.\n",
    "        support_data: A dictionary. The key is frequent itemset and the value is support.\n",
    "        min_conf: Minimal confidence.\n",
    "    Returns:\n",
    "        big_rule_list: A list which contains all big rules. Each big rule is represented\n",
    "                       as a 3-tuple.\n",
    "    \"\"\"\n",
    "    big_rule_list = []\n",
    "    sub_set_list = []\n",
    "    for i in range(0, len(L)):\n",
    "        for freq_set in L[i]:\n",
    "            for sub_set in sub_set_list:\n",
    "                if sub_set.issubset(freq_set):\n",
    "                    conf = support_data[freq_set] / support_data[freq_set - sub_set]\n",
    "                    big_rule = (freq_set - sub_set, sub_set, conf)\n",
    "                    if conf >= min_conf and big_rule not in big_rule_list:\n",
    "                        # print freq_set-sub_set, \" => \", sub_set, \"conf: \", conf\n",
    "                        big_rule_list.append(big_rule)\n",
    "            sub_set_list.append(freq_set)\n",
    "    return big_rule_list\n",
    "\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"\n",
    "    Test\n",
    "    \"\"\"\n",
    "    data_set = load_data_set()\n",
    "    L, support_data = generate_L(data_set, k=3, min_support=0.2)\n",
    "    big_rules_list = generate_big_rules(L, support_data, min_conf=0.7)\n",
    "    for Lk in L:\n",
    "        print(\"=\"*50)\n",
    "        print(\"frequent \" + str(len(list(Lk)[0])) + \"-itemsets\\t\\tsupport\")\n",
    "        print(\"=\"*50)\n",
    "        for freq_set in Lk:\n",
    "            print (freq_set, support_data[freq_set])\n",
    "    print(\"Big Rules\")\n",
    "    for item in big_rules_list:\n",
    "        print(item[0], \"=>\", item[1], \"conf: \", item[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 八 PageRank算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "     PageRank是一种在搜索引擎中根据网页之间相互的链接关系计算网页排名的技术。其级别从1到10级，PR值越高说明该网页越受欢迎（越重要）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 九 EM算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    https://blog.csdn.net/quicmous/article/details/52154527  简析EM算法（最大期望算法）\n",
    "    https://blog.csdn.net/luolang_103/article/details/81178018\n",
    "    https://blog.csdn.net/zb1165048017/article/details/48577891   HMM——前向算法与后向算法\n",
    "    \n",
    "    EM 算法是Dempster，Laind，Rubin于1977年提出的求参数极大似然估计的一种方法，它可以从非完整数据集中对参数进行MLE估计，是一种非常简单实用的学习算法。这种方法可以广泛地应用于处理缺损数据、截尾数据以及带有噪声等所谓的不完全数据。具体地说，我们可以利用EM算法来填充样本中的缺失数据、发现隐藏变量的值、估计HMM中的参数、估计有限混合分布中的参数以及可以进行无监督聚类等等。\n",
    "\n",
    "    最大期望算法（Expectation Maximization Algorithm，又译为：期望最大化算法），是一种迭代算法，用于含有隐变量（hidden variable）的概率参数模型的最大似然估计或极大后验概率估计。\n",
    "\n",
    "    在统计计算中，最大期望（EM）算法是在概率（probabilistic）模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐藏变量（Latent Variable）。最大期望经常用在机器学习和计算机视觉的数据聚类（Data Clustering）领域。\n",
    "\n",
    "    最大期望算法经过两个步骤交替进行计算，第一步是计算期望（E），也就是将隐藏变量象能够观测到的一样包含在内从而计算最大似然的期望值；另外一步是最大化（M），也就是最大化在 E 步上找到的最大似然的期望值从而计算参数的最大似然估计。M 步上找到的参数然后用于另外一个 E 步计算，这个过程不断交替进行。\n",
    "    \n",
    "    基本步骤（具体公式及推导，读者参考其他文献）:\n",
    "    1、参数初始化,对需要估计的参数进行初始赋值，包括均值、方差、混合系数以及期望。\n",
    "    2、E-Step计算，利用概率分布公式计算后验概率，即期望。\n",
    "    3、M-step计算，重新估计参数，包括均值、方差、混合系数并且估计此参数下的期望值。\n",
    "    4、收敛性判断，将新的与旧的值进行比较，并与设置的阈值进行对比，判断迭代是否结束，若不符合条件，则返回到第2步，重新进行计算，直到收敛符合条件结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用python实现EM算法过程，下面以投硬币为例，供读者研究\n",
    "TEST=[[5,5],[9,1],[8,2],[4,6],[7,3]];\n",
    "#投出来的结果，前面是正面向上的次数，每组结果后面数字表示反面向上的次数。\n",
    "#由于每次投币要不选择A或者B，且仅从单个样本数据，无法获知，EM算法的主要目标是：通过大量计算和统计，将数据分离或寻找隐含变量。\n",
    "print(TEST)#整个考虑可以从正面入手，反面配合。\n",
    "\n",
    "def P(sA,sB,t1,t2):\n",
    "    #s是初始的概率，t1取正面的个数；t2取反面的个数，以概率密度为准。\n",
    "    PA=Cmn(t1+t2,t1)*(sA**t1)*((1-sA)**t2) #计算A出现的概率。\n",
    "    PB=Cmn(t1+t2,t1)*(sB**t1)*((1-sB)**t2) #计算B出现的概率。\n",
    "    return round(PA/(PA+PB),2)     #求出新的概率值，然后根据这个概率值进行后面期望值的计算；且保留2位有效数字。\n",
    "\n",
    "#阶乘。\n",
    "def fac(n):\n",
    "    f=1\n",
    "    for y in range(2,n+1):\n",
    "        f=f*y\n",
    "    return f\n",
    "\n",
    "def Cmn(m,n):#先定义cmn后面利用这个概率密度函数，会使用排列组合关系。当Cm，n=Cm，m-n。减少计算量。\n",
    "    s = m - n\n",
    "    if s < n:\n",
    "        n = s\n",
    "    f=1;\n",
    "    t=m\n",
    "    for y in range(0,n):\n",
    "        f=f*t\n",
    "        t=t-1\n",
    "    return f/fac(n)\n",
    "\n",
    "def CoinAB(oldoA,oldoB):  #计算期望值，通过一次期望值的求解，再重新迭代概率值。\n",
    "    UA1=0;UA2=0;       #UA1和UA2是A硬币投出的期望值。\n",
    "    t3=0;t4=0;t5=0;t6=0;  #t3和t4、t5和t6都是为计算硬币A和B的期望值。\n",
    "    for y in TEST:         #遍历所有样本数据。\n",
    "        UA1,UA2=y         #取当前值，前面表示正面，后面表示反面。\n",
    "        oA=P(oldoA,oldoB,UA1,UA2)  #计算出出A硬币的概率。\n",
    "        print(oA,1-oA)    \n",
    "        t3=t3+UA1*round(oA,2)        #计算A期望值（针对正面这个事实开始讨论）\n",
    "        t4=t4+UA2*round(oA,2)\n",
    "        t5=t5+UA1*round((1-oA),2)    #计算B期望值（针对正面这个事实开始讨论）\n",
    "        t6=t6+UA2*round((1-oA),2)     \n",
    "    return round(t3/(t3+t4),2),round(t5/(t5+t6),2)  #返回迭代新一轮的A、B的概率。\n",
    "\n",
    "def EM(oA,oB):\n",
    "    y=0;  #计迭代次数。\n",
    "    while(1):\n",
    "        y=y+1\n",
    "        oldoA=oA;oldoB=oB       #先存储迭代数据，为了计算收敛值，结束条件。\n",
    "        oA,oB=CoinAB(oldoA,oldoB)  #分别赋值，为了下次使用。\n",
    "        print(\"----y={},oA={},oB={}\".format(y,oA,oB))\n",
    "        if (oldoA-oA)**2+(oldoB-oB)**2<0.005:#自己设置收敛条件，目的为终止循环。\n",
    "            break\n",
    "\n",
    "    print(\"oA=\",oA,\"oB=\",oB)   \n",
    "#oA,oB=eval(input(\"请输入初始值-oA,oB,逗号隔开：\\n\"))\n",
    "oA=0.6\n",
    "oB=0.4\n",
    "EM(oA,oB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 十 随机森林算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    在机器学习中，随机森林是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。 Leo Breiman和Adele Cutler发展出推论出随机森林的算法。而 \"Random Forests\" 是他们的商标。 这个术语是1995年由贝尔实验室的Tin Kam Ho所提出的随机决策森林（random decision forests）而来的。这个方法则是结合Breimans 的 \"Bootstrap aggregating\" 想法和 Ho 的\"randomsubspace method\"以建造决策树的集合。\n",
    "\n",
    "    根据下列算法而建造每棵树 ：\n",
    "    1.用M来表示训练用例（样本）的个数，N表示特征数目。\n",
    "    2.输入特征数目n，用于确定决策树上一个节点的决策结果；其中n应远小于N。\n",
    "    3.从M个训练用例（样本）中以有放回抽样的方式，取样k次，形成一个训练集（即bootstrap取样），并用未抽到的用例（样本）作预测，评估其误差。\n",
    "    4.对于每一个节点，随机选择n个特征，每棵决策树上每个节点的决定都是基于这些特征确定的。根据这n个特征，计算其最佳的分裂方式。\n",
    "    5.每棵树都会完整成长而不会剪枝，这有可能在建完一棵正常树状分类器后会被采用。\n",
    "    6.最后测试数据，根据每棵树，以多胜少方式决定分类。\n",
    "    \n",
    "    优点：\n",
    "\n",
    "    随机森林的既可以用于回归也可以用于分类任务，并且很容易查看模型的输入特征的相对重要性。随机森林算法被认为是一种非常方便且易于使用的算法，因为它是默认的超参数通常会产生一个很好的预测结果。超参数的数量也不是那么多，而且它们所代表的含义直观易懂。\n",
    "\n",
    "    随机森林有足够多的树，分类器就不会产生过度拟合模型。\n",
    "\n",
    "    缺点：\n",
    "\n",
    "    由于使用大量的树会使算法变得很慢，并且无法做到实时预测。一般而言，这些算法训练速度很快，预测十分缓慢。越准确的预测需要越多的树，这将导致模型越慢。在大多数现实世界的应用中，随机森林算法已经足够快，但肯定会遇到实时性要求很高的情况，那就只能首选其他方法。当然，随机森林是一种预测性建模工具，而不是一种描述性工具。也就是说，如果您正在寻找关于数据中关系的描述，那建议首选其他方法。\n",
    "    \n",
    "    适用范围：\n",
    "\n",
    "    随机森林算法可被用于很多不同的领域，如银行，股票市场，医药和电子商务。在银行领域，它通常被用来检测那些比普通人更高频率使用银行服务的客户，并及时偿还他们的债务。同时，它也会被用来检测那些想诈骗银行的客户。在金融领域，它可用于预测未来股票的趋势。在医疗保健领域，它可用于识别药品成分的正确组合，分析患者的病史以识别疾病。除此之外，在电子商务领域中，随机森林可以被用来确定客户是否真的喜欢某个产品。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_dataset(filename):\n",
    "    file_reader = csv.reader(open(filename,'rb'), delimiter=',')\n",
    "    X, y = [], []\n",
    "    for row in file_reader:\n",
    "        X.append(row[2:13])\n",
    "        y.append(row[-1])\n",
    "    # Extract feature names\n",
    "    feature_names = np.array(X[0])\n",
    "    return np.array(X[1:]).astype(np.float32),np.array(y[1:]).astype(np.float32), feature_names\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y, feature_names =load_dataset(\"d:\\\\bike_day.csv\")\n",
    "    X, y = shuffle(X, y, random_state=7)\n",
    "    num_training = int(0.9 * len(X))\n",
    "    X_train, y_train = X[:num_training],y[:num_training]\n",
    "    X_test, y_test = X[num_training:],y[num_training:]\n",
    "    rf_regressor =RandomForestRegressor(n_estimators=1000, max_depth=10, min_samples_split=1)\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "    y_pred = rf_regressor.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    evs = explained_variance_score(y_test,y_pred)\n",
    "    from AdaBoostRegressor import plot_feature_importances\n",
    "    plot_feature_importances(rf_regressor.feature_importances_, 'RandomForest regressor', feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 十一 条件随机场"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    条件随机域（场）（conditionalrandom fields，简称 CRF，或CRFs），是一种判别式概率模型，是随机场的一种。条件随机场(CRF)由Lafferty等人于2001年提出，结合了最大熵模型和隐马尔可夫模型的特点，是一种无向图模型，近年来在分词、词性标注和命名实体识别等序列标注任务中取得了很好的效果。常用于标注或分析序列资料，如自然语言文字或是生物序列。如同马尔可夫随机场，条件随机场为具有无向的图模型，无向图中的顶点代表随机变量，顶点间的连线代表随机变量间的相依关系，在条件随机场中，随机变量 Y 的分布为条件机率，给定的观察值则为随机变量 X。原则上，条件随机场的图模型布局是可以任意给定的，一般常用的布局是链结式的架构，链结式架构不论在训练（training）、推论（inference）、或是解码（decoding）上，都存在效率较高的算法可供演算。\n",
    "    \n",
    "    CRF被用于中文分词和词性标注等词法分析工作，一般序列分类模型常常采用隐马尔可夫模型（HMM），像基于类的中文分词。但隐马尔可夫模型中存在两个假设：输出独立性假设和马尔可夫性假设。其中，输出独立性假设要求序列数据严格相互独立才能保证推导的正确性，而事实上大多数序列数据不能被表示成一系列独立事件。而条件随机场则使用一种概率图模型，具有表达长距离依赖性和交叠性特征的能力，能够较好地解决标注（分类）偏置等问题的优点，而且所有特征可以进行全局归一化，能够求得全局的最优解。\n",
    "      \n",
    "    缺点：模型复杂，算法时间长。\n",
    "      \n",
    "    ？？？\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 二十 逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    logistic回归又称logistic回归分析，是一种广义的线性回归分析模型，常用于数据挖掘，疾病自动诊断，广告投放、邮件判断等领域。\n",
    "\n",
    "    该算法可根据已知的一系列因变量估计离散数值（比方说二进制数值 0 或 1 ，是或否，真或假）。简单来说，它通过将数据拟合进一个逻辑函数来预估一个事件出现的概率。因此，它也被叫做逻辑回归。因为它预估的是概率，所以它的输出值大小在 0 和 1 之间（正如所预计的一样）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 二十五 最小二乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "     最小二乘法，也叫最小平方法，在古汉语中“平方”称为“二乘”，“最小”指的是参数的估计值要保证各个观测点与估计点的距离的平方和达到最小。最小二乘作为一种损失函数，在这整个解决方案中，最小二乘法演算为每一方程式的结果中，将残差平方和的总和最小化。最重要的应用是在曲线拟合上。最小平方所涵义的最佳拟合，即残差（残差为：观测值与模型提供的拟合值之间的差距）平方总和的最小化。当问题在自变量有重大不确定性时，那么使用简易回归和最小二乘法会发生问题；在这种情况下，须另外考虑变量-误差-拟合模型所需的方法，而不是最小二乘法。\n",
    "\n",
    "        最小平方问题分为两种：线性或普通的最小二乘法，和非线性的最小二乘法，取决于在所有未知数中的残差是否为线性。线性的最小平方问题发生在统计回归分析中；它有一个封闭形式的解决方案。非线性的问题通常经由迭代细致化来解决；在每次迭代中，系统由线性近似，因此在这两种情况下核心演算是相同的。\n",
    "\n",
    "        最小二乘法所得出的多项式，即以拟合曲线的函数来描述自变量与预计应变量的变异数关系。当观测值来自指数族且满足轻度条件时，最小平方估计和最大似然估计是相同的。最小二乘法也能从动差法得出。回归分析的最初目的是估计模型的参数以便达到对数据的最佳拟合。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 二十六 最大似然估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    最大似然法（MaximumLikelihood，ML）也称为最大概似估计，也叫极大似然估计，是一种具有理论性的点估计法，此方法的基本思想是：当从模型总体随机抽取n组样本观测值后，最合理的参数估计量应该使得从模型中抽取该n组样本观测值的概率最大，而不是像最小二乘估计法旨在得到使得模型能最好地拟合样本数据的参数估计量。\n",
    "\n",
    "    最大似然估计是一种统计方法，它用来求一个样本集的相关概率密度函数的参数。这个方法最早是遗传学家以及统计学家罗纳德·费雪爵士在 1912 年至1922 年间开始使用的。\n",
    "    \n",
    "    最大似然估计的一般过程为：\n",
    "\n",
    "    1. 写出似然函数；\n",
    "    2. 如果无法直接求导的话，对似然函数取对数；\n",
    "    3. 求导数 ；\n",
    "    4. 求解模型中参数的最优值。\n",
    "\n",
    "    最大似然估计的特点：\n",
    "\n",
    "    1.比其他估计方法更加简单；\n",
    "    2.收敛性：无偏或者渐近无偏，当样本数目增加时，收敛性质会更好；\n",
    "    3.如果假设的类条件概率模型正确，则通常能获得较好的结果。但如果假设模型出现偏差，将导致非常差的估计结果。\n",
    "    \n",
    "    https://blog.csdn.net/u014182497/article/details/82252456    \n",
    "    \n",
    "    在统计学中，似然函数（likelihood function，通常简写为likelihood，似然）是一个非常重要的内容，在非正式场合似然和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。概率是在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性，比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的；而似然刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数），还是抛硬币的例子，假设我们随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上（实际情况一般不会这么理想，这里只是举个例子），我们很容易判断这是一枚标准的硬币，两面朝上的概率均为50%，这个过程就是我们根据结果来判断这个事情本身的性质（参数），也就是似然。\n",
    "    \n",
    "    结果和参数相互对应的时候，似然和概率在数值上是相等的，如果用 θ 表示环境对应的参数，x 表示结果，那么概率可以表示为：P(x|θ) \n",
    "    \n",
    "    相对应的似然可以表示为：L(θ|x) 理解为已知结果为 x ，参数为θ (似然函数里θ 是变量，这里## 标题 ##说的参数是相对与概率而言的)对应的概率，即： L(θ|x) = P(x|θ) 需要说明的是两者在数值上相等，但是意义并不相同，这里L(θ|x)  是关于 θ 的函数，而 P 则是关于 x 的函数，两者从不同的角度描述一件事情。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9e02b4c84972>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mtheta_gauss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m900\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtheta_est\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneg_log_likelihood\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtheta_gauss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "# https://blog.csdn.net/lanchunhui/article/details/50357417\n",
    "\n",
    "# 单参的情况\n",
    "def log_likelihood(theta, F, e):\n",
    "    return -.5*np.sum(np.log(2*np.pi*e**2)+(F-theta[0])**2/(e**2))\n",
    "\n",
    "# 双参的情况\n",
    "def log_likelihood(theta, F, e):\n",
    "    return -.5*np.sum(np.log(2*np.pi*(theta[1]**2+e**2))+(F-theta[0])**2/(theta[1]**2+e**2))\n",
    "\n",
    "#此时，我们可以使用scipy下的最优化函数，一般是最小化目标函数（objective function），所以\n",
    "from scipy import optimize\n",
    "\n",
    "def neg_log_likelihood(theta, F, e):\n",
    "    return -log_likelihood(theta, F, e)\n",
    "\n",
    "theta_gauss = [900, 5]\n",
    "theta_est = optimize.fmin(func=neg_log_likelihood, x0=theta_gauss, args=(F, e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    https://blog.csdn.net/continueoo/article/details/77893587\n",
    "    \n",
    "    https://blog.csdn.net/tostq/article/details/70846702 深度剖析HMM（附Python代码） 用HMM解决三个实际问题\n",
    "    \n",
    "    https://blog.csdn.net/quicmous/article/details/52208302 隐马尔可夫模型求解三大问题实例剖析\n",
    "    \n",
    "    一、什么场景下需要HMM模型\n",
    "    X是一个时间和状态都是离散随机过程，Xn是n时刻下的状态，如果Xn + 1对于过去状态的条件概率分布仅是Xn的一个函数，即我们通常说的状态仅与上一个状态有关，则该过程是一个一阶马尔科夫过程。公式如下：P(Xn+1=x∣X0,X1,X2,…，Xn)=P(Xn+1=x∣Xn)\n",
    "    我们现实生活中有很多这种类似的例子，上述例子中天气是我们不能直接观测的（被关小黑屋的情况）称为隐藏状态，苔藓的干湿被称为观察状态，观察到的状态序列与隐藏过程有一定的概率关系，这时我们使用隐马尔科夫模型对这样的过程建模，这个模型包含了一个底层隐藏的随时间改变的马尔科夫过程，以及一个与隐藏状态某种程度相关的可观察到的状态集合。我们生活中有很多这种无法被直接观测的例子，比如语音设备中的某些内部产出，或者是投骰子中某些骰子的状态选择（4,6,8面骰子的案例，网上很多这里不赘述），或者是词性标注中的词语词性等等。整个HMM模型都是围绕观测状态和隐藏状态建模和处理的。\n",
    "\n",
    "    二、 HMM模型的相关概念定义\n",
    "    隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。以上描述来自百度百科。HMM模型大概长像如下，偷的52NLP的图，这个模型包含了一个底层隐藏的随时间改变的马尔科夫过程，通常是一阶的，对应下图就是天气之间的状态转移概率关系。以及一个与隐藏状态某种程度相关的可观察到的状态集合，对应就是天气如何影响苔藓，即我们的术语叫做发射概率或者混淆概率。\n",
    "\n",
    "    三、HMM模型的5元组\n",
    "    每个模型都有自己相关的概念，弄清算法之前我们先来看看这个模型的基本概念，5元组。\n",
    "    （S,K,π,A,B）：以上分别对应了HMM中的5个重要概念\n",
    "    S：隐藏状态的集合（Sun Cloud Rain），N为隐状态个数 N=3\n",
    "    K : 输出状态或者说观测状态的集合（Soggy Damp Dryish Dry），M为观测状态个数M=4\n",
    "    π : 对应隐藏状态的初始化概率 （sun : 0.5 cloud : 0.3 rain : 0.2）\n",
    "    A : 隐藏状态的状态转移概率，是一个NN的概率矩阵\n",
    "    B : 隐藏状态到观测状态的混淆矩阵，是一个NM的发射概率矩阵\n",
    "        另外符号体系中会有对于每个序列的状态序列和观测序列，如一周的观测和状态，定义下\n",
    "    O : 某特定观测序列\n",
    "    X : 某特定隐藏状态序列   \n",
    "    \n",
    "    四、HMM中的3个经典问题\n",
    "    关于HMM这个模型建立起来之后，其实有很多问题可以去讨论，但是一般讲学中我们讨论3个问题，即评估，预测，学习，下面我们来一起看一波吧~\n",
    "    这里为了解释方便，请大家无条件的接受以下两个基础条件：\n",
    "    1.已知对应问题的观测状态集合K\n",
    "    2.已知对应问题的隐藏状态集合S\n",
    "    如果没有以上两个问题，可能就不太属于今天要讨论的范畴了，可能属于聚类后者其他研究，总之这里我们不予考虑。\n",
    "    \n",
    "    3个经典问题如下：\n",
    "        ·观察序列概率的计算 评估（前向算法）\n",
    "        ·隐藏状态序列最大概率解 预测（维特比算法）\n",
    "        ·马尔科夫参数求解（π,A,B） 学习（EM算法 前后向算法）\n",
    "        \n",
    "    五、 HMM模型中的3个经典问题\n",
    "    \n",
    "    5.1、评估描述\n",
    "        \n",
    "        给定观测序列O（o1,o2,…,oT）和模型u = (π,A,B),求出P（O | u）,即给定模型下观测序列的概率是多少？对应于之前的例子，就是给定天气的转移矩阵，天气和苔藓的发射矩阵，以及天气的初始化列表（这些都是已知的，以前统计好的，具体方法这里不用纠结）。然后求出给定一周苔藓的状态，你判断这个状态存在的概率有多大（这个评估这里只是介绍方法，想想感觉这个案例这里没有什么特别大的实际意义）。\n",
    "\n",
    "        解决该问题有个很直观的想法就是把所有的隐藏状态都走一遍，然后看对应观测状态概率有多大，一起加起来就是这个状态的可能性。我们用数学式子表示如下：自己写公式还是很费力的，第二个公式中bxtxt+1ot这种写法是因为有些HMM模型的发射概率是在发射弧上面，即和该状态与下状态有关，所以写成这种样子，有时候如果只与当前状态有关可以写成btot的形式。\n",
    "    第一个公式：利用全概率公式求解所有可能隐状态序列下的概率之和。\n",
    "    第二个公式：已知状态下序列的概率。\n",
    "    第三个公式：任意隐藏序列的概率。\n",
    "    第四个公式：利用每个概率表示公式1，这里bxtot表示了发射只与当前状态有关，与2略不同，只是多了一个假设条件便于表示。另外此处说明下由于序列长度不同，该公式可能与其他某些书中公式有点差异，但基本思想一致，只不过具体表现上针对不同情况略有不同。最后一个bXTOT是在连乘之后的，不在求积符号里面！\n",
    "    \n",
    "    仔细看看上面公式，计算一下时间复杂度。一共N^T次方的可能序列，好了打住，不用往后看了，这已经指数级别了。我们可以看到计算公式里面实际是由大量冗余乘法计算的，现在给大家介绍动态规划的前向算法来巧妙的解决实际计算问题。\n",
    "    \n",
    "    评估实际算法：前向计算\n",
    "    \n",
    "    评估出这个观测序列存在的概率。\n",
    "    \n",
    "    #计算公式中的alpha二维数组\n",
    "    def _forward(self,observationsSeq):\n",
    "        T = len(observationsSeq)\n",
    "        # self.pi 各隐藏状态的概率\n",
    "        N = len(self.pi)  \n",
    "        alpha = np.zeros((T,N),dtype=float)\n",
    "        # self.B 隐藏状态到观测状态的混淆矩阵\n",
    "        alpha[0,:] = self.pi * self.B[:,observationsSeq[0]]  #numpy可以简化循环\n",
    "        for t in range(1,T):\n",
    "            for n in range(0,N):\n",
    "                alpha[t,n] = np.dot(alpha[t-1,:],self.A[:,n]) * self.B[n,observationsSeq[t]] #使用内积简化代码\n",
    "        return alpha\n",
    "    \n",
    "    5.2、预测描述\n",
    "    给定观测序列O（o1,o2,…,oT）和模型u = (π,A,B),求出最大概率的隐藏序列X（X1,X2…,XT），那么解法思路和上述一样，只不过将求和变成求最大值，并且相同的思路我们可以利用动态规划来解决这个问题，这里该方法有一个较为有名的算法“维特比算法”。下面引用了数学之美的一段话。\n",
    "\n",
    "    维特比算法是一个特殊但应用最广的动态规划算法，利用动态规划，可以解决任何一个图中的最短路径问题。而维特比算法是针对一个特殊的图——篱笆网络的有向图（Lattice )的最短路径问题而提出的。 它之所以重要，是因为凡是使用隐含马尔可夫模型描述的问题都可以用它来解码，包括今天的数字通信、语音识别、机器翻译、拼音转汉字、分词等。——《数学之美》\n",
    "\n",
    "    维特比算法：\n",
    "    argmax(P（X|O,u）),我们想求出最有可能的X序列（X1，X2,…,XT），我们依葫芦画瓢写出对应的维特比中间变量θt（j） = max(P(X1,X2,…Xt=Sj,O1,O2,…,Ot|u)) （这个公式确实有点难写出来，不过是想还是很容易理解）。这里我们根据上一时刻的概率，根据转移概率求出此刻最可能的情况，以此递归，最终能找到最优解。下面是具体推导过程，该过程中引入了一个变量来存储该节点的前一个路过节点，代码中用argmax(θ（j-1）)表示。\n",
    "    \n",
    "    推导公式如下，解释下：\n",
    "    \n",
    "    第一个公式：状态1时刻概率都是初始概率，这里注意有人用1有人用0，完全看是不是写代码方便了\n",
    "    第二个公式：状态t时刻为t-1时刻θ变量*转移概率的最大值\n",
    "    第三个公式：状态t时刻回溯上一个最大概率路径。    \n",
    "\n",
    "    θ1(j) = πj \n",
    "    θt(j) = max( θt-1(i) * aij * bj ot)   i∈[1,N]\n",
    "    φt(j) = argmax( θt-1(i) * aij )   i∈[1,N]\n",
    "    \n",
    "    #维特比算法进行预测，即解码，返回最大路径与该路径概率\n",
    "    \n",
    "    def viterbi(self,observationsSeq):\n",
    "        T = len(observationsSeq)\n",
    "        N = len(self.pi)\n",
    "        prePath = np.zeros((T,N),dtype=int)\n",
    "        dpMatrix = np.zeros((T,N),dtype=float)\n",
    "        dpMatrix[0,:] = self.pi * self.B[:,observationsSeq[0]]\n",
    "\n",
    "        for t in range(1,T):\n",
    "            for n in range(N):\n",
    "                probs = dpMatrix[t-1,:] * self.A[:,n] * self.B[n,observationsSeq[t]]\n",
    "                prePath[t,n] = np.argmax(probs)\n",
    "                dpMatrix[t,n] = np.max(probs)\n",
    "\n",
    "        maxProb = np.max(dpMatrix[T-1,:])\n",
    "        maxIndex = np.argmax(dpMatrix[T-1,:])\n",
    "        path = [maxIndex]\n",
    "\n",
    "        for t in reversed(range(1,T)):\n",
    "            path.append(prePath[t,path[-1]])\n",
    "\n",
    "        path.reverse()\n",
    "        return maxProb,path\n",
    "    \n",
    "\n",
    "    5.3、学习\n",
    "    学习是HMM经典问题里面比较难的问题，要求是给定一个观察序列，求出模型的参数（π,A,B）。前后向算法了。这个算法又叫baum-welch算法，是在EM算法提出之前就已经存在了，之后被证明为一种特殊的EM算法。一般的博客会从后向变量开始往后讲，但是我觉得从理解的角度出发，我们还是先来看看这个EM算法。\n",
    "    EM算法实例理解：\n",
    "    首先我们想一下学习参数我们需要做些什么，有哪些难点。首先我们不知道隐藏状态序列是什么样子的，再一个我们要求转移矩阵之类。我们想从概率最大入手或者说极大似然，但是只有知道u=(π,A,B)我们才能求出最可能的序列，有了最可能的序列我们才有可能估算最可能的参数集。这仿佛是一个循环的“鸡生蛋，蛋生鸡”的故事。此问题就是EM算法的应用场景，那么我们举个简单的例子，看一下EM算法的思想。\n",
    "    假设现在有N1个男生，N2个女生。并且男女身高集X1,X2均服从高斯分布X1N（μ1，б1^2），X2N（μ2，б2^2），其中均值和标准差μi,бi(i∈1，2)均是未知参数。我们现在随机抽取了n个人，假设抽取人身高序列为Oi(i∈1,…,n)。那么我们希望在u=(π,A,B)参数下使得该序列出现可能性最大。用数学公式表示如下：\n",
    "\n",
    "    现在就是想办法对求出上式u,最直接的思路就是对（π,A,B）分别求导，在分布已知（隐藏状态确认）的情况下还是比较好求，前提是对似然函数L（u）求lg,这样可以将上面的式子编程∑，方便计算。下面给出极大似然的一般步骤：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping    0.35\n",
      "eating      0.35\n",
      "pooping     0.30\n",
      "Name: states, dtype: float64\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# http://www.blackarbs.com/blog/introduction-hidden-markov-models-python-networkx-sklearn/2/9/2017\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# create state space and initial state probabilities\n",
    "\n",
    "states = ['sleeping', 'eating', 'pooping']\n",
    "pi = [0.35, 0.35, 0.3]\n",
    "state_space = pd.Series(pi, index=states, name='states')\n",
    "print(state_space)\n",
    "print(state_space.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sleeping eating pooping\n",
      "sleeping      0.4    0.2     0.4\n",
      "eating       0.45   0.45     0.1\n",
      "pooping      0.45   0.25     0.3\n",
      "\n",
      " [[0.4 0.2 0.4]\n",
      " [0.45 0.45 0.1]\n",
      " [0.45 0.25 0.3]] (3, 3) \n",
      "\n",
      "sleeping    1.0\n",
      "eating      1.0\n",
      "pooping     1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# The next step is to define the transition probabilities. They are simply the probabilities of \n",
    "# staying in the same state or moving to a different state given the current state. \n",
    "# create transition matrix\n",
    "# equals transition probability matrix of changing states given a state\n",
    "# matrix is size (M x M) where M is number of states\n",
    "\n",
    "q_df = pd.DataFrame(columns=states, index=states)\n",
    "q_df.loc[states[0]] = [0.4, 0.2, 0.4]\n",
    "q_df.loc[states[1]] = [0.45, 0.45, 0.1]\n",
    "q_df.loc[states[2]] = [0.45, 0.25, .3]\n",
    "\n",
    "print(q_df)\n",
    "\n",
    "q = q_df.values\n",
    "print('\\n', q, q.shape, '\\n')\n",
    "print(q_df.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('eating', 'eating'): 0.45,\n",
      " ('eating', 'pooping'): 0.1,\n",
      " ('eating', 'sleeping'): 0.45,\n",
      " ('pooping', 'eating'): 0.25,\n",
      " ('pooping', 'pooping'): 0.3,\n",
      " ('pooping', 'sleeping'): 0.45,\n",
      " ('sleeping', 'eating'): 0.2,\n",
      " ('sleeping', 'pooping'): 0.4,\n",
      " ('sleeping', 'sleeping'): 0.4}\n"
     ]
    }
   ],
   "source": [
    "# Now that we have the initial and transition probabilities setup we can create a Markov diagram using the Networkx package.\n",
    "from pprint import pprint \n",
    "\n",
    "# create a function that maps transition probability dataframe \n",
    "# to markov edges and weights\n",
    "\n",
    "def _get_markov_edges(Q):\n",
    "    edges = {}\n",
    "    for col in Q.columns:\n",
    "        for idx in Q.index:\n",
    "            edges[(idx,col)] = Q.loc[idx,col]\n",
    "    return edges\n",
    "\n",
    "edges_wts = _get_markov_edges(q_df)\n",
    "pprint(edges_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:\n",
      "['sleeping', 'eating', 'pooping']\n",
      "\n",
      "Edges:\n",
      "OutMultiEdgeDataView([('sleeping', 'sleeping', {'weight': 0.4, 'label': 0.4}), ('sleeping', 'eating', {'weight': 0.2, 'label': 0.2}), ('sleeping', 'pooping', {'weight': 0.4, 'label': 0.4}), ('eating', 'sleeping', {'weight': 0.45, 'label': 0.45}), ('eating', 'eating', {'weight': 0.45, 'label': 0.45}), ('eating', 'pooping', {'weight': 0.1, 'label': 0.1}), ('pooping', 'sleeping', {'weight': 0.45, 'label': 0.45}), ('pooping', 'eating', {'weight': 0.25, 'label': 0.25}), ('pooping', 'pooping', {'weight': 0.3, 'label': 0.3})])\n"
     ]
    }
   ],
   "source": [
    "# create graph object\n",
    "G = nx.MultiDiGraph()\n",
    "\n",
    "# nodes correspond to states\n",
    "G.add_nodes_from(states)\n",
    "print(f'Nodes:\\n{G.nodes()}\\n')\n",
    "\n",
    "# edges represent transition probabilities\n",
    "for k, v in edges_wts.items():\n",
    "    tmp_origin, tmp_destination = k[0], k[1]\n",
    "    G.add_edge(tmp_origin, tmp_destination, weight=v, label=v)\n",
    "print(f'Edges:')\n",
    "pprint(G.edges(data=True))    \n",
    "\n",
    "# pos = nx.drawing.nx_pydot.graphviz_layout(G, prog='dot')\n",
    "# nx.draw_networkx(G, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "healthy    0.5\n",
      "sick       0.5\n",
      "Name: states, dtype: float64\n",
      "\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "# Consider a situation where your dog is acting strangely and you wanted to model the probability that your dog's \n",
    "# behavior is due to sickness or simply quirky behavior when otherwise healthy.\n",
    "\n",
    "# In this situation the true state of the dog is unknown, thus hidden from you. One way to model this is to assume \n",
    "# that the dog has observable behaviors that represent the true, hidden state. Let's walk through an example.\n",
    "# First we create our state space - healthy or sick. We assume they are equiprobable.  \n",
    "# create state space and initial state probabilities\n",
    "\n",
    "hidden_states = ['healthy', 'sick']\n",
    "pi = [0.5, 0.5]\n",
    "state_space = pd.Series(pi, index=hidden_states, name='states')\n",
    "print(state_space)\n",
    "print('\\n', state_space.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        healthy sick\n",
      "healthy     0.7  0.3\n",
      "sick        0.4  0.6\n",
      "\n",
      " [[0.7 0.3]\n",
      " [0.4 0.6]] (2, 2) \n",
      "\n",
      "healthy    1.0\n",
      "sick       1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# create hidden transition matrix\n",
    "# a or alpha \n",
    "#   = transition probability matrix of changing states given a state\n",
    "# matrix is size (M x M) where M is number of states\n",
    "\n",
    "a_df = pd.DataFrame(columns=hidden_states, index=hidden_states)\n",
    "a_df.loc[hidden_states[0]] = [0.7, 0.3]\n",
    "a_df.loc[hidden_states[1]] = [0.4, 0.6]\n",
    "\n",
    "print(a_df)\n",
    "\n",
    "a = a_df.values\n",
    "print('\\n', a, a.shape, '\\n')\n",
    "print(a_df.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sleeping eating pooping\n",
      "healthy      0.2    0.6     0.2\n",
      "sick         0.4    0.1     0.5\n",
      "\n",
      " [[0.2 0.6 0.2]\n",
      " [0.4 0.1 0.5]] (2, 3) \n",
      "\n",
      "healthy    1.0\n",
      "sick       1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# create matrix of observation (emission) probabilities\n",
    "# b or beta = observation probabilities given state\n",
    "# matrix is size (M x O) where M is number of states \n",
    "# and O is number of different possible observations\n",
    "\n",
    "observable_states = states\n",
    "\n",
    "b_df = pd.DataFrame(columns=observable_states, index=hidden_states)\n",
    "b_df.loc[hidden_states[0]] = [0.2, 0.6, 0.2]\n",
    "b_df.loc[hidden_states[1]] = [0.4, 0.1, 0.5]\n",
    "\n",
    "print(b_df)\n",
    "\n",
    "b = b_df.values\n",
    "print('\\n', b, b.shape, '\\n')\n",
    "print(b_df.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('healthy', 'healthy'): 0.7,\n",
      " ('healthy', 'sick'): 0.3,\n",
      " ('sick', 'healthy'): 0.4,\n",
      " ('sick', 'sick'): 0.6}\n",
      "{('healthy', 'eating'): 0.6,\n",
      " ('healthy', 'pooping'): 0.2,\n",
      " ('healthy', 'sleeping'): 0.2,\n",
      " ('sick', 'eating'): 0.1,\n",
      " ('sick', 'pooping'): 0.5,\n",
      " ('sick', 'sleeping'): 0.4}\n"
     ]
    }
   ],
   "source": [
    "# create graph edges and weights\n",
    "\n",
    "hide_edges_wts = _get_markov_edges(a_df)\n",
    "pprint(hide_edges_wts)\n",
    "\n",
    "emit_edges_wts = _get_markov_edges(b_df)\n",
    "pprint(emit_edges_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:\n",
      "['healthy', 'sick']\n",
      "\n",
      "Edges:\n",
      "OutMultiEdgeDataView([('healthy', 'healthy', {'weight': 0.7, 'label': 0.7}), ('healthy', 'sick', {'weight': 0.3, 'label': 0.3}), ('healthy', 'sleeping', {'weight': 0.2, 'label': 0.2}), ('healthy', 'eating', {'weight': 0.6, 'label': 0.6}), ('healthy', 'pooping', {'weight': 0.2, 'label': 0.2}), ('sick', 'healthy', {'weight': 0.4, 'label': 0.4}), ('sick', 'sick', {'weight': 0.6, 'label': 0.6}), ('sick', 'sleeping', {'weight': 0.4, 'label': 0.4}), ('sick', 'eating', {'weight': 0.1, 'label': 0.1}), ('sick', 'pooping', {'weight': 0.5, 'label': 0.5})])\n"
     ]
    }
   ],
   "source": [
    "# create graph object\n",
    "G = nx.MultiDiGraph()\n",
    "\n",
    "# nodes correspond to states\n",
    "G.add_nodes_from(hidden_states)\n",
    "print(f'Nodes:\\n{G.nodes()}\\n')\n",
    "\n",
    "# edges represent hidden probabilities\n",
    "for k, v in hide_edges_wts.items():\n",
    "    tmp_origin, tmp_destination = k[0], k[1]\n",
    "    G.add_edge(tmp_origin, tmp_destination, weight=v, label=v)\n",
    "\n",
    "# edges represent emission probabilities\n",
    "for k, v in emit_edges_wts.items():\n",
    "    tmp_origin, tmp_destination = k[0], k[1]\n",
    "    G.add_edge(tmp_origin, tmp_destination, weight=v, label=v)\n",
    "    \n",
    "print(f'Edges:')\n",
    "pprint(G.edges(data=True))    \n",
    "\n",
    "# pos = nx.drawing.nx_pydot.graphviz_layout(G, prog='neato')\n",
    "# nx.draw_networkx(G, pos)\n",
    "\n",
    "# # create edge labels for jupyter plot but is not necessary\n",
    "# emit_edge_labels = {(n1,n2):d['label'] for n1,n2,d in G.edges(data=True)}\n",
    "# nx.draw_networkx_edge_labels(G , pos, edge_labels=emit_edge_labels)\n",
    "# nx.drawing.nx_pydot.write_dot(G, 'pet_dog_hidden_markov.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Obs_code   Obs_seq\n",
      "0         1    eating\n",
      "1         1    eating\n",
      "2         2   pooping\n",
      "3         1    eating\n",
      "4         0  sleeping\n",
      "5         1    eating\n",
      "6         2   pooping\n",
      "7         1    eating\n",
      "8         0  sleeping\n",
      "9         2   pooping\n",
      "10        2   pooping\n",
      "11        0  sleeping\n",
      "12        1    eating\n",
      "13        0  sleeping\n",
      "14        1    eating\n"
     ]
    }
   ],
   "source": [
    "# observation sequence of dog's behaviors\n",
    "# observations are encoded numerically\n",
    "\n",
    "obs_map = {'sleeping':0, 'eating':1, 'pooping':2}\n",
    "obs = np.array([1,1,2,1,0,1,2,1,0,2,2,0,1,0,1])\n",
    "\n",
    "inv_obs_map = dict((v,k) for k, v in obs_map.items())\n",
    "obs_seq = [inv_obs_map[v] for v in list(obs)]\n",
    "\n",
    "print( pd.DataFrame(np.column_stack([obs, obs_seq]), \n",
    "                columns=['Obs_code', 'Obs_seq']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Viterbi algorithm for shortest path\n",
    "# code adapted from Stephen Marsland's, Machine Learning An Algorthmic Perspective, Vol. 2\n",
    "# https://github.com/alexsosn/MarslandMLAlgo/blob/master/Ch16/HMM.py\n",
    "\n",
    "def viterbi(pi, a, b, obs):\n",
    "    \n",
    "    nStates = np.shape(b)[0]\n",
    "    T = np.shape(obs)[0]\n",
    "    \n",
    "    # init blank path\n",
    "    path = np.zeros(T)\n",
    "    # delta --> highest probability of any path that reaches state i\n",
    "    delta = np.zeros((nStates, T))\n",
    "    # phi --> argmax by time step for each state\n",
    "    phi = np.zeros((nStates, T))\n",
    "    \n",
    "    # init delta and phi \n",
    "    delta[:, 0] = pi * b[:, obs[0]]\n",
    "    phi[:, 0] = 0\n",
    "\n",
    "    print('\\nStart Walk Forward\\n')    \n",
    "    # the forward algorithm extension\n",
    "    for t in range(1, T):\n",
    "        for s in range(nStates):\n",
    "            delta[s, t] = np.max(delta[:, t-1] * a[:, s]) * b[s, obs[t]] \n",
    "            phi[s, t] = np.argmax(delta[:, t-1] * a[:, s])\n",
    "            print('s={s} and t={t}: phi[{s}, {t}] = {phi}'.format(s=s, t=t, phi=phi[s, t]))\n",
    "    \n",
    "    # find optimal path\n",
    "    print('-'*50)\n",
    "    print('Start Backtrace\\n')\n",
    "    path[T-1] = np.argmax(delta[:, T-1])\n",
    "    #p('init path\\n    t={} path[{}-1]={}\\n'.format(T-1, T, path[T-1]))\n",
    "    for t in range(T-2, -1, -1):\n",
    "        path[t] = phi[path[t+1], [t+1]]\n",
    "        #p(' '*4 + 't={t}, path[{t}+1]={path}, [{t}+1]={i}'.format(t=t, path=path[t+1], i=[t+1]))\n",
    "        print('path[{}] = {}'.format(t, path[t]))\n",
    "        \n",
    "    return path, delta, phi\n",
    "\n",
    "path, delta, phi = viterbi(pi, a, b, obs)\n",
    "print('\\nsingle best state path: \\n', path)\n",
    "print('delta:\\n', delta)\n",
    "print('phi:\\n', phi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
