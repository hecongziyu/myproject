{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 线性模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[ 0.30951    -0.23076957]] [1.0957512] 0.48129517\n",
      "20 [[0.0330434  0.01941632]] [0.4378943] 0.0029906894\n",
      "40 [[0.06454597 0.14533748]] [0.35025376] 0.00033976193\n",
      "60 [[0.08496391 0.18211187]] [0.31837168] 4.352153e-05\n",
      "80 [[0.09410608 0.19384262]] [0.30672675] 5.7778366e-06\n",
      "100 [[0.09776759 0.19781701]] [0.30246496] 7.744898e-07\n",
      "120 [[0.09916805 0.19921347]] [0.30090362] 1.040735e-07\n",
      "140 [[0.09969241 0.19971418]] [0.30033132] 1.39945975e-08\n",
      "160 [[0.09988674 0.19989567]] [0.30012152] 1.882621e-09\n",
      "180 [[0.09995839 0.19996184]] [0.30004454] 2.5288652e-10\n",
      "200 [[0.09998471 0.19998604]] [0.3000163] 3.399883e-11\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 使用 NumPy 生成假数据(phony data), 总共 100 个点.\n",
    "x_data = np.float32(np.random.rand(2, 100)) # 随机输入\n",
    "y_data = np.dot([0.100, 0.200], x_data) + 0.300\n",
    "\n",
    "# 构造一个线性模型\n",
    "# \n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))\n",
    "y = tf.matmul(W, x_data) + b\n",
    "\n",
    "\n",
    "# 最小化方差\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 启动图 (graph)\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "# 拟合平面\n",
    "for step in range(0, 201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(W), sess.run(b), sess.run(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42652742 0.42345572 0.44583921 0.5271843  0.37269287 0.40453453\n",
      " 0.46982448 0.36144103 0.55413746 0.44155353 0.56481228 0.52643828\n",
      " 0.55807167 0.51325505 0.47489496 0.48198068 0.53236455 0.36329095\n",
      " 0.57076461 0.35559385 0.41086792 0.47681058 0.34504356 0.42093779\n",
      " 0.44895968 0.4921881  0.47643313 0.45867934 0.47160521 0.46235535\n",
      " 0.48541165 0.54165832 0.4155582  0.44276405 0.44587754 0.5000065\n",
      " 0.41143187 0.55661783 0.53950392 0.5162824  0.37891455 0.43957152\n",
      " 0.49578797 0.51525523 0.41386802 0.48646227 0.4004611  0.41227659\n",
      " 0.34392934 0.46297992 0.42689615 0.51192555 0.36117764 0.56575961\n",
      " 0.40109718 0.40907412 0.48773237 0.52012944 0.42903121 0.48956345\n",
      " 0.46271163 0.50426997 0.39911664 0.45237131 0.49717382 0.45414295\n",
      " 0.47192753 0.58114473 0.49521731 0.49287806 0.34015675 0.3836381\n",
      " 0.37570606 0.39477831 0.50218042 0.50596267 0.31240795 0.4892804\n",
      " 0.35732876 0.54349182 0.53625322 0.47501245 0.43674573 0.57909262\n",
      " 0.51392237 0.52440284 0.35603051 0.42983195 0.45226334 0.40671456\n",
      " 0.4397404  0.56311923 0.38117051 0.45078294 0.39964559 0.39171968\n",
      " 0.48819105 0.45491083 0.47323589 0.48860993]\n"
     ]
    }
   ],
   "source": [
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# x不是一个特定的值，而是一个占位符placeholder，我们在TensorFlow运行计算时输入这个值。我们希望能够输入任\n",
    "# 意数量的MNIST图像，每一张图展平成784维的向量。\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "# 我们赋予tf.Variable不同的初值来创建不同的Variable：在这里，我们都用全为零的张量来初始化W和b。因为我们要学\n",
    "# 习W和b的值，它们的初值可以随意设置\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# 模型 \n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "# 添加一个新的占位符用于输入正确值\n",
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 启动图 (graph)\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "# 评估模型\n",
    "# tf.argmax(y,1)返回的是模型对于任一输入x预测到的标签值，而 tf.argmax(y_,1) 代表正确的标签\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))   \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MNIST进阶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#  Tensorflow依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。一般而言，使用TensorFlow程序\n",
    "# 的流程是先创建一个图，然后在session中启动它。\n",
    "\n",
    "# 这里，我们使用更加方便的InteractiveSession类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，\n",
    "# 插入一些计算图，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使\n",
    "# 用IPython。如果你没有使用InteractiveSession，那么你需要在启动session之前构建整个计算图，然后启动该计算图。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积和池化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？在这个实例里，我们会一直使用vanilla版本。\n",
    "#我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。我们的池化用简单传统\n",
    "#的2x2大小的模板做max pooling。为了代码更简洁，我们把这部分抽象成一个函数。\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# 卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "# 为了用这一层，我们把x变成一个4d向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的\n",
    "# 通道数为1，如果是rgb彩色图，则为3)\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\n",
    "#第一层\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "#第二层\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# 密集连接层\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# 输出层\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(20000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print \"step %d, training accuracy %g\"%(i, train_accuracy)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print \"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运作方式"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "一、每一层都创建于一个唯一的tf.name_scope之下，创建于该作用域之下的所有元素都将带有其前缀\n",
    "with tf.name_scope('hidden1') as scope:\n",
    "\n",
    "在定义的作用域中，每一层所使用的权重和偏差都在tf.Variable实例中生成，并且包含了各自期望的shape。\n",
    "\n",
    "weights = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, hidden1_units],stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))), name='weights')\n",
    "biases = tf.Variable(tf.zeros([hidden1_units]), name='biases')\n",
    "例如，当这些层是在hidden1作用域下生成时，赋予权重变量的独特名称将会是\"hidden1/weights\"\n",
    "\n",
    "二、图表\n",
    "在run_training()这个函数的一开始，是一个Python语言中的with命令，这个命令表明所有已经构建的操作都要与默认的tf.Graph全局实例关联起来。\n",
    "with tf.Graph().as_default():\n",
    "tf.Graph实例是一系列可以作为整体执行的操作。TensorFlow的大部分场景只需要依赖默认图表一个实例即可。\n",
    "\n",
    "以占位符为哈希键，创建一个Python字典对象，键值则是其代表的反馈Tensor。\n",
    "\n",
    "feed_dict = {\n",
    "    images_placeholder: images_feed,\n",
    "    labels_placeholder: labels_feed,\n",
    "}\n",
    "\n",
    "\n",
    "三、检查状态\n",
    "在运行sess.run函数时，要在代码中明确其需要获取的两个值：[train_op, loss]。\n",
    "\n",
    "for step in xrange(FLAGS.max_steps):\n",
    "    feed_dict = fill_feed_dict(data_sets.train,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    _, loss_value = sess.run([train_op, loss],\n",
    "                             feed_dict=feed_dict)\n",
    "                             \n",
    "四、状态可视化\n",
    "\n",
    "\n",
    "为了释放TensorBoard所使用的事件文件（events file），所有的即时数据（在这里只有一个）都要在图表构建阶段合并至一个操作（op）中。\n",
    "\n",
    "summary_op = tf.merge_all_summaries()\n",
    "在创建好会话（session）之后，可以实例化一个tf.train.SummaryWriter，用于写入包含了图表本身和即时数据具体值的事件文件。\n",
    "\n",
    "summary_writer = tf.train.SummaryWriter(FLAGS.train_dir,\n",
    "                                        graph_def=sess.graph_def)\n",
    "最后，每次运行summary_op时，都会往事件文件中写入最新的即时数据，函数的输出会传入事件文件读写器（writer）的add_summary()函数。。\n",
    "\n",
    "summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "summary_writer.add_summary(summary_str, step)\n",
    "事件文件写入完毕之后，可以就训练文件夹打开一个TensorBoard，查看即时数据的情况。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
