{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 分词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T08:12:10.876116Z",
     "start_time": "2020-06-22T08:12:07.457082Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['第Ⅰ', '卷', '（', '选择题', '共', '60', '分', '）', ',', '一、', '(A)', '仍', '选择题', '：', '1.', '本', '大题', '共', '8', '小题', '，', '每', '小', '题', '5分', '，', '二、', 'A.']\n",
      "use time: 0.0019969940185546875\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import pkuseg\n",
    "import time\n",
    "'''\n",
    "第Ⅰ卷（选择题 共60分）, 一、选择题：本大题共8小题，每小题5分，满分40分．在每小题给出的四个选项中，只有一项是符合题目要求的．\n",
    "1.已知集合则,  6、已知某地区中小学生人数和近视情况分别如图1和图2所示，为了解该地区中小学生的近视形成原因，用分层抽样的方法抽取的学生进行调查\n",
    "11.从0,1,2,3,4,5,6,7,8,9中任取七个不同的数，则这七个数的中位数是6的概率为.\n",
    "'''\n",
    "line  = '第Ⅰ卷（选择题 共60分）,一、 (A) 仍选择题：1.本大题共8小题，每小题5分，二、 A.' \n",
    "# jieba.add_word('(A)')\n",
    "# jieba.add_word('（A）')\n",
    "# ws = jieba.lcut(line, cut_all=False)\n",
    "# print('words:', ','.join(ws))\n",
    "lexicon = ['(A)', '1.', '一、','二、', 'A.']\n",
    "seg = pkuseg.pkuseg(user_dict=lexicon)\n",
    "start_time = time.time()\n",
    "text = seg.cut(line)\n",
    "print('text:', text)\n",
    "print('use time:', (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T07:54:53.574968Z",
     "start_time": "2020-06-22T07:54:53.551963Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/text/ torchtext\n",
    "# https://towardsdatascience.com/deep-learning-for-nlp-with-pytorch-and-torchtext-4f92d69052f\n",
    "\n",
    "from torchtext import data, datasets\n",
    "from torchtext.data import Iterator, BucketIterator,Iterator\n",
    "import jieba\n",
    "import torch\n",
    "import numpy as np\n",
    "# 定义分词工具\n",
    "def tokenizer(text):    \n",
    "    return [wd for wd in jieba.cut(text, cut_all=False)]\n",
    "def post_process(batch,vocab):\n",
    "    print('post process:',vocab)\n",
    "#     x = '（1)'.split() + x\n",
    "#     newdata = []\n",
    "    print('post process batch:', batch)\n",
    "    for idx in range(len(batch)):\n",
    "        if np.random.randint(2):\n",
    "            batch[idx] = '（ 1 )'.split() + batch[idx]\n",
    "        else:\n",
    "            batch[idx] = '（ 2 )'.split() + batch[idx]\n",
    "#         newdata.append(x)\n",
    "    print('post process batch:', batch)\n",
    "    return batch\n",
    "\n",
    "#  spacy 英文分词\n",
    "text = data.Field(tokenize=tokenizer,lower=True, batch_first=True, postprocessing=None)\n",
    "label = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train, val, test = data.TabularDataset.splits(path='D:/PROJECT_TW/git/data/testpaper', train='train.csv',\n",
    "                                              validation='valid.csv',test='test.csv',format='csv', \n",
    "                                              fields=[('text', text), ('labels', label)], skip_header=True)\n",
    "\n",
    "# sort_key就是告诉BucketIterator使用哪个key值去进行组合，很明显，在这里是comment_text\n",
    "# repeat设定为False是因为之后要打包这个迭代层\n",
    "# train_iter, val_iter=Iterator.splits(\n",
    "#     (train,val),\n",
    "#     batch_sizes=(4,4),\n",
    "#     device=torch.device('cpu'),\n",
    "# #     sort_key=lambda x:len(x.comment_text),\n",
    "#     sort_within_batch=False,\n",
    "#     repeat=False\n",
    "# )\n",
    "# iter_data = iter(train)\n",
    "# examp = next(iter_data)\n",
    "# print('e type:', examp)\n",
    "# print(examp.text)\n",
    "text.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T02:42:07.976556Z",
     "start_time": "2020-06-22T02:42:07.970539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.example.Example'>\n",
      "{'text': [\"'\", '一', '.', '选择题', '（', '共', '8', '小题', '）', \"'\"], 'labels': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(type(train[0]))\n",
    "print(train[0].__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T02:41:50.393708Z",
     "start_time": "2020-06-22T02:41:50.163200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "'\n",
      "[(\"'\", 6), ('一', 3), ('.', 3), ('选择题', 3), ('（', 3), ('共', 3), ('8', 3), ('小题', 3), ('）', 3)]\n",
      "dict_keys(['text', 'labels'])\n",
      "[\"'\", '一', '.', '选择题', '（', '共', '8', '小题', '）', \"'\"]\n",
      "tensor([[ 2,  5,  3,  8,  9,  6,  4,  7, 10,  2],\n",
      "        [ 2,  5,  3,  8,  9,  6,  4,  7, 10,  2],\n",
      "        [ 2,  5,  3,  8,  9,  6,  4,  7, 10,  2]])\n",
      "------\n",
      "\n",
      "tensor([1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(len(text.vocab))\n",
    "print(text.vocab.itos[2])\n",
    "print(text.vocab.freqs.most_common(20))\n",
    "print(train[0].__dict__.keys())\n",
    "print(train[0].text)\n",
    "\n",
    "batch=next(train_iter.__iter__())\n",
    "txt = batch.text\n",
    "tgt = batch.labels\n",
    "print(txt)\n",
    "print('------\\n')\n",
    "print(tgt)\n",
    "# print(lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T03:45:39.786291Z",
     "start_time": "2020-06-22T03:45:39.776297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['有力', '的', '文本', '预处理', '库']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.example import Example\n",
    "from torchtext import data, datasets\n",
    "lines = [['有力的文本预处理库'],['有力的文本预处理库'],['有力的文本预处理库']]\n",
    "TEXT = data.Field(tokenize=tokenizer,lower=True, batch_first=True, postprocessing=None)\n",
    "fields = [('text', TEXT),]\n",
    "examples = [Example.fromlist(line,fields) for line in lines]\n",
    "print(examples[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libreoffice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://medium.com/analytics-vidhya/macro-programming-in-openoffice-libreoffice-with-using-python-en-a37465e9bfa5\n",
    "# https://github.com/holloway/docvert-python3/ \n",
    "# https://wiki.openoffice.org/wiki/Python\n",
    "# https://www.jianshu.com/p/a6df4c177d62\n",
    "# \"C:\\\\Program Files (x86)\\LibreOffice 5\\program\\soffice.exe\" --calc --accept=\"socket,host=localhost,port=2002;urp;\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextEmbedding(nn.Module):\n",
    "    '''\n",
    "    对输入的文字串进行编码\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        pass\n",
    "    \n",
    "class TestPaperModel(nn.Module):\n",
    "    '''\n",
    "    试卷解析\n",
    "    '''\n",
    "    def __init__(self, num_class, vocab_size, embed_dim, hidden_dim):\n",
    "        self.embedding = TextEmbedding(vocab_size, embed_dim, hidden_dim)\n",
    "        self.rnn_cell = nn.LSTMCell(embed_dim, hidden_dim)\n",
    "        self.liner = nn.Liner(hidden_dim, num_class)\n",
    "        \n",
    "    def forward(self, input_data, targets):\n",
    "        '''\n",
    "        input data format: batch size * seq_no * char ids (max 10)\n",
    "        '''\n",
    "        output = self.embedding(input_data)\n",
    "        #  out put size: batch size * seq_no * embed dim\n",
    "\n",
    "        context_states, o_t = self.init_decoder()\n",
    "        \n",
    "        logits = []\n",
    "        max_len = output.size(1)\n",
    "        for t in range(max_len):\n",
    "            # max len\n",
    "            context_states, o_t , logit = self.forward_step(output[:,t,:], context_states, o_t, tgt)\n",
    "            logits.append(logit)\n",
    "        logits = torch.stack(logits, dim=1)  # [B, MAX_LEN, out_size]\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def forward_step(self, data, context_states,o_t, tgt):\n",
    "        prey_v = self.embeding_tgt(tgt)\n",
    "        indata = torch.cat([data, o_t, prey_v],dim=1)  # B * (txt_embed_size + tgt_embed_size + rnn_out_size)\n",
    "        h_t, c_t = self.rnn_cell(indata,context_states)\n",
    "        o_t = self.liner(torch.cat([h_t, c_t], dim=1)).tanh()\n",
    "        logit = F.softmax(o_t,dim=1)\n",
    "        return (h_t,c_t), o_t,logit\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
