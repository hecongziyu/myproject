{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project_tw\\anly\\venv\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np;\n",
    "import os\n",
    "import jieba\n",
    "import gensim.models.word2vec as w2v\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(uchar):\n",
    "    \"\"\"判断一个unicode是否是汉字\"\"\"\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def is_number(uchar):\n",
    "    \"\"\"判断一个unicode是否是数字\"\"\"\n",
    "    if uchar >= u'\\u0030' and uchar <= u'\\u0039':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def is_alphabet(uchar):\n",
    "    \"\"\"判断一个unicode是否是英文字母\"\"\"\n",
    "    if (uchar >= u'\\u0041' and uchar <= u'\\u005a') or (uchar >= u'\\u0061' and uchar <= u'\\u007a'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def is_legal(uchar):\n",
    "    \"\"\"判断是否非汉字，数字和英文字符\"\"\"\n",
    "    if not (is_chinese(uchar) or is_number(uchar) or is_alphabet(uchar)):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def extract_chinese(line):\n",
    "    res = \"\"\n",
    "    for word in line:\n",
    "        if is_legal(word):\n",
    "            res = res + word\n",
    "    return res;\n",
    "def words2line(words):\n",
    "    line = \"\"\n",
    "    for word in words:\n",
    "        line = line + \" \" + word\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     51
    ]
   },
   "outputs": [],
   "source": [
    "#数据预处理函数，在dir文件夹下每个子文件是一类内容\n",
    "def datahelper(dir):\n",
    "#返回为文本，文本对应标签\n",
    "    labels_index={}\n",
    "    index_lables={}\n",
    "    num_recs=0\n",
    "    fs = os.listdir(dir)\n",
    "    MAX_SEQUENCE_LENGTH = 200\n",
    "    MAX_NB_WORDS = 50000\n",
    "    EMBEDDING_DIM = 20\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    i = 0;\n",
    "    for f in fs:\n",
    "        labels_index[f] = i;\n",
    "        index_lables[i] = f\n",
    "        i = i + 1;\n",
    "    print(labels_index)\n",
    "    texts = []\n",
    "    labels = []  # list of label ids\n",
    "    for la in labels_index.keys():\n",
    "        print(la + \" \" + index_lables[labels_index[la]])\n",
    "        la_dir = dir + \"/\" + la;\n",
    "        fs = os.listdir(la_dir)\n",
    "        for f in fs:\n",
    "            file = open(la_dir + \"/\" + f, encoding='utf-8')\n",
    "            lines = file.readlines();\n",
    "            text = ''\n",
    "            for line in lines:\n",
    "                if len(line) > 5:\n",
    "                    line = extract_chinese(line)\n",
    "                    words = jieba.lcut(line, cut_all=False, HMM=True)\n",
    "                    text = words\n",
    "                    texts.append(text)\n",
    "                    labels.append(labels_index[la])\n",
    "                    num_recs = num_recs + 1\n",
    "    return texts,labels,labels_index,index_lables\n",
    "\n",
    "#load word 2 vetc，加载词向量，可以事先预训练\n",
    "def getw2v():\n",
    "    model_file_name = 'D:/PROJECT_TW/git/data/nlp/w2v/new_model_big.txt'\n",
    "    \n",
    "    # 模型训练，生成词向量\n",
    "    '''\n",
    "    sentences = w2v.LineSentence('trainword.txt')\n",
    "    model = w2v.Word2Vec(sentences, size=20, window=5, min_count=5, workers=4)\n",
    "    model.save(model_file_name)\n",
    "    '''\n",
    "    model = w2v.Word2Vec.load(model_file_name)\n",
    "    return model;\n",
    "\n",
    "\n",
    "def trainw2v():\n",
    "    model_file_name = 'D:/PROJECT_TW/git/data/nlp/w2v/new_model_big.txt'\n",
    "    sentences = w2v.LineSentence('D:/PROJECT_TW/git/data/nlp/THUCNews/trainword.txt')\n",
    "    model = w2v.Word2Vec(sentences, size=20, window=5, min_count=5, workers=1)\n",
    "    model.save(model_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'家居': 0, '彩票': 1, '房产': 2, '教育': 3, '股票': 4, '财经': 5}\n",
      "家居 家居\n",
      "彩票 彩票\n",
      "房产 房产\n",
      "教育 教育\n",
      "股票 股票\n",
      "财经 财经\n"
     ]
    }
   ],
   "source": [
    "# 生成词向量训练数据\n",
    "train_dir = 'D:/PROJECT_TW/git/data/nlp/THUCNews'\n",
    "texts,labels,labels_index,index_lables=datahelper(train_dir)\n",
    "big_txt = ''\n",
    "for item in texts:\n",
    "    big_txt = '{}\\n{}'.format(big_txt,' '.join(item))\n",
    "big_txt = big_txt.encode('utf-8')\n",
    "\n",
    "with open('D:/PROJECT_TW/git/data/nlp/THUCNews/trainword.txt','wb') as f:\n",
    "    f.write(big_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练词向量\n",
    "trainw2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23999122  0.04855075 -0.92600787  0.10056638 -0.24005225  0.22864884\n",
      " -1.2042845  -0.18332459  0.80257744  0.6747311   0.13293691 -0.15797134\n",
      "  0.0390049   0.6759941  -0.8183674  -0.3172709   0.7973181   0.26182362\n",
      " -0.0844366   0.67150074]\n"
     ]
    }
   ],
   "source": [
    "model = getw2v()\n",
    "print(model.wv.get_vector('上海'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "#textCNN模型\n",
    "class textCNN(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(textCNN, self).__init__()\n",
    "        if DEBUG:\n",
    "            print('args --> {}'.format(args))\n",
    "        vocb_size = args['vocb_size']\n",
    "        dim = args['dim']\n",
    "        n_class = args['n_class']\n",
    "        max_len = args['max_len']\n",
    "        embedding_matrix=args['embedding_matrix']\n",
    "        #需要将事先训练好的词向量载入  dim  20\n",
    "        self.embeding = nn.Embedding(vocb_size, dim,_weight=embedding_matrix)\n",
    "        # h 表示每句话分隔后的单词， w 表示该单词的向量\n",
    "        # conv1 input size: batch number , channel,  h , w  . (1000, 1, 64, 20) --> (1000, 16, 32, 10)\n",
    "        self.conv1 = nn.Sequential(\n",
    "                     nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5,\n",
    "                               stride=1, padding=2),\n",
    "\n",
    "                     nn.ReLU(),\n",
    "                     nn.MaxPool2d(kernel_size=2) # \n",
    "                     )\n",
    "        # conv1 input size: batch number , channel,  h , w  . (1000, 16, 32, 10) --> (1000, 32, 16, 5)\n",
    "        self.conv2 = nn.Sequential(\n",
    "                     nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "                     nn.ReLU(),\n",
    "                     nn.MaxPool2d(2)\n",
    "                     )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # conv4 input size: batch number , channel,  h , w  . (1000, 64, 8, 2) --> (1000, 128, 4, 1)\n",
    "        self.conv4 = nn.Sequential(  # (16,64,64)\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.out = nn.Linear(512, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeding(x)\n",
    "        x=x.view(x.size(0),1,max_len,word_dim)\n",
    "        if DEBUG:\n",
    "            print('input size --> {}'.format(x.size()))\n",
    "        x = self.conv1(x)\n",
    "        if DEBUG:\n",
    "            print('conv1 size --> {}'.format(x.size()))\n",
    "        x = self.conv2(x)\n",
    "        if DEBUG:\n",
    "            print('conv2 size --> {}'.format(x.size()))        \n",
    "        x = self.conv3(x)\n",
    "        if DEBUG:\n",
    "            print('conv3 size --> {}'.format(x.size()))        \n",
    "        x = self.conv4(x)\n",
    "        if DEBUG:\n",
    "            print('conv4 size --> {}'.format(x.size()))        \n",
    "        x = x.view(x.size(0), -1) # 将（batch，outchanel,w,h）展平为（batch，outchanel*w*h）\n",
    "        #print(x.size())\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "\n",
    "\n",
    "# train_dir = 'D:/PROJECT_TW/git/data/nlp/THUCNews'\n",
    "# texts,labels,labels_index,index_lables=datahelper(train_dir)\n",
    "#词表\n",
    "word_vocb=[]\n",
    "word_vocb.append('')\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        word_vocb.append(word)\n",
    "word_vocb=set(word_vocb)\n",
    "vocb_size=len(word_vocb)\n",
    "#设置词表大小\n",
    "nb_words=40000\n",
    "max_len=64\n",
    "word_dim=20\n",
    "n_class=len(index_lables)\n",
    "\n",
    "args={}\n",
    "if nb_words<vocb_size:\n",
    "    nb_words=vocb_size;\n",
    "\n",
    "#textCNN调用的参数\n",
    "args['vocb_size']=nb_words\n",
    "args['max_len']=max_len\n",
    "args['n_class']=n_class\n",
    "args['dim']=word_dim\n",
    "\n",
    "\n",
    "\n",
    "texts_with_id=np.zeros([len(texts),max_len])\n",
    "\n",
    "#词表与索引的map\n",
    "word_to_idx={word:i for i,word in enumerate(word_vocb)}\n",
    "idx_to_word={word_to_idx[word]:word for word in word_to_idx}\n",
    "#每个单词的对应的词向量\n",
    "embeddings_index = getw2v()\n",
    "#预先处理好的词向量\n",
    "embedding_matrix = np.zeros((nb_words, word_dim))\n",
    "\n",
    "for word, i in word_to_idx.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    if  embeddings_index.wv.__contains__(word):\n",
    "        embedding_vector = embeddings_index.wv.get_vector(word)\n",
    "        if embedding_vector is not None:\n",
    "#             print('--> {}'.format(i))\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "args['embedding_matrix']=torch.Tensor(embedding_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1322, -0.2534, -0.4950, -0.0790, -0.2268, -0.6080,  0.7234,\n",
      "        -1.6187, -0.5441,  0.7554, -0.2016, -0.0341,  0.4113,  0.4508,\n",
      "         0.2651,  0.0046,  0.5257,  0.6460,  0.0834, -0.0562])\n"
     ]
    }
   ],
   "source": [
    "print(args['embedding_matrix'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2509\n",
      "acc 0.57\n",
      "0 loss -->  0.24468156695365906 \n",
      "epoch 0 step 199 acc 0.34063745019920316\n",
      "acc 0.6233333333333333\n",
      "1 loss -->  0.12069165706634521 \n",
      "epoch 1 step 199 acc 0.37250996015936255\n",
      "acc 0.6433333333333333\n",
      "2 loss -->  0.017282620072364807 \n",
      "epoch 2 step 199 acc 0.3844621513944223\n",
      "acc 0.6533333333333333\n",
      "3 loss -->  0.006407138891518116 \n",
      "epoch 3 step 199 acc 0.3904382470119522\n",
      "acc 0.6533333333333333\n",
      "4 loss -->  0.0002040023246081546 \n",
      "epoch 4 step 199 acc 0.3904382470119522\n",
      "acc 0.65\n",
      "5 loss -->  0.19347572326660156 \n",
      "epoch 5 step 199 acc 0.3884462151394422\n",
      "acc 0.6366666666666667\n",
      "6 loss -->  0.0001253441150765866 \n",
      "epoch 6 step 199 acc 0.3804780876494024\n",
      "acc 0.6366666666666667\n",
      "7 loss -->  0.01363480556756258 \n",
      "epoch 7 step 199 acc 0.3804780876494024\n",
      "acc 0.6033333333333334\n",
      "8 loss -->  0.6101404428482056 \n",
      "epoch 8 step 199 acc 0.3605577689243028\n",
      "acc 0.6566666666666666\n",
      "9 loss -->  8.731277745255284e-08 \n",
      "epoch 9 step 199 acc 0.39243027888446214\n",
      "acc 0.6533333333333333\n",
      "10 loss -->  9.639929032800865e-08 \n",
      "epoch 10 step 199 acc 0.3904382470119522\n",
      "acc 0.67\n",
      "11 loss -->  8.239219084771321e-08 \n",
      "epoch 11 step 199 acc 0.40039840637450197\n",
      "acc 0.6366666666666667\n",
      "12 loss -->  5.2026944530325636e-08 \n",
      "epoch 12 step 199 acc 0.3804780876494024\n",
      "acc 0.6566666666666666\n",
      "13 loss -->  1.7698297938295582e-08 \n",
      "epoch 13 step 199 acc 0.39243027888446214\n",
      "acc 0.6133333333333333\n",
      "14 loss -->  1.542282035416065e-08 \n",
      "epoch 14 step 199 acc 0.3665338645418327\n",
      "acc 0.64\n",
      "15 loss -->  1.7142976105333219e-07 \n",
      "epoch 15 step 199 acc 0.38247011952191234\n",
      "acc 0.6033333333333334\n",
      "16 loss -->  2.6074667758280157e-08 \n",
      "epoch 16 step 199 acc 0.3605577689243028\n",
      "acc 0.6233333333333333\n",
      "17 loss -->  4.1773176917558885e-08 \n",
      "epoch 17 step 199 acc 0.37250996015936255\n",
      "acc 0.65\n",
      "18 loss -->  2.63531746895751e-05 \n",
      "epoch 18 step 199 acc 0.3884462151394422\n",
      "acc 0.6466666666666666\n",
      "19 loss -->  0.00017575494712218642 \n",
      "epoch 19 step 199 acc 0.38645418326693226\n",
      "acc 0.6633333333333333\n",
      "20 loss -->  0.002172480570152402 \n",
      "epoch 20 step 199 acc 0.39641434262948205\n",
      "acc 0.6966666666666667\n",
      "21 loss -->  0.008540737442672253 \n",
      "epoch 21 step 199 acc 0.4163346613545817\n",
      "acc 0.68\n",
      "22 loss -->  0.0012631078716367483 \n",
      "epoch 22 step 199 acc 0.4063745019920319\n",
      "acc 0.6733333333333333\n",
      "23 loss -->  0.0006145449005998671 \n",
      "epoch 23 step 199 acc 0.40239043824701193\n",
      "acc 0.6633333333333333\n",
      "24 loss -->  0.009431423619389534 \n",
      "epoch 24 step 199 acc 0.39641434262948205\n",
      "acc 0.7233333333333334\n",
      "25 loss -->  0.00038856250466778874 \n",
      "epoch 25 step 199 acc 0.43227091633466136\n",
      "acc 0.68\n",
      "26 loss -->  8.554355008527637e-05 \n",
      "epoch 26 step 199 acc 0.4063745019920319\n",
      "acc 0.6733333333333333\n",
      "27 loss -->  3.85863495466765e-05 \n",
      "epoch 27 step 199 acc 0.40239043824701193\n",
      "acc 0.6766666666666666\n",
      "28 loss -->  2.619339466036763e-05 \n",
      "epoch 28 step 199 acc 0.4043824701195219\n",
      "acc 0.6333333333333333\n",
      "29 loss -->  1.4770672351005487e-05 \n",
      "epoch 29 step 199 acc 0.3784860557768924\n",
      "acc 0.68\n",
      "30 loss -->  0.0003585804661270231 \n",
      "epoch 30 step 199 acc 0.4063745019920319\n",
      "acc 0.6533333333333333\n",
      "31 loss -->  8.455767783743795e-07 \n",
      "epoch 31 step 199 acc 0.3904382470119522\n",
      "acc 0.6566666666666666\n",
      "32 loss -->  0.00030105136102065444 \n",
      "epoch 32 step 199 acc 0.39243027888446214\n",
      "acc 0.6266666666666667\n",
      "33 loss -->  8.633176662442565e-07 \n",
      "epoch 33 step 199 acc 0.3745019920318725\n",
      "acc 0.6466666666666666\n",
      "34 loss -->  0.00022317464754451066 \n",
      "epoch 34 step 199 acc 0.38645418326693226\n",
      "acc 0.6366666666666667\n",
      "35 loss -->  1.625349455025571e-06 \n",
      "epoch 35 step 199 acc 0.3804780876494024\n",
      "acc 0.65\n",
      "36 loss -->  0.0001987349969567731 \n",
      "epoch 36 step 199 acc 0.3884462151394422\n",
      "acc 0.6233333333333333\n",
      "37 loss -->  9.6286453299399e-07 \n",
      "epoch 37 step 199 acc 0.37250996015936255\n",
      "acc 0.6466666666666666\n",
      "38 loss -->  0.000521525158546865 \n",
      "epoch 38 step 199 acc 0.38645418326693226\n",
      "acc 0.62\n",
      "39 loss -->  2.217181418018299e-06 \n",
      "epoch 39 step 199 acc 0.3705179282868526\n",
      "acc 0.6466666666666666\n",
      "40 loss -->  8.688472007634118e-05 \n",
      "epoch 40 step 199 acc 0.38645418326693226\n",
      "acc 0.6266666666666667\n",
      "41 loss -->  2.36018809118832e-06 \n",
      "epoch 41 step 199 acc 0.3745019920318725\n",
      "acc 0.67\n",
      "42 loss -->  0.0003173541044816375 \n",
      "epoch 42 step 199 acc 0.40039840637450197\n",
      "acc 0.58\n",
      "43 loss -->  0.0005219031008891761 \n",
      "epoch 43 step 199 acc 0.3466135458167331\n",
      "acc 0.6233333333333333\n",
      "44 loss -->  1.658428300288506e-05 \n",
      "epoch 44 step 199 acc 0.37250996015936255\n",
      "acc 0.6833333333333333\n",
      "45 loss -->  1.6230969777097926e-05 \n",
      "epoch 45 step 199 acc 0.40836653386454186\n",
      "acc 0.6866666666666666\n",
      "46 loss -->  1.9123667982512416e-07 \n",
      "epoch 46 step 199 acc 0.4103585657370518\n",
      "acc 0.6466666666666666\n",
      "47 loss -->  0.00010037261381512508 \n",
      "epoch 47 step 199 acc 0.38645418326693226\n",
      "acc 0.68\n",
      "48 loss -->  6.8709283368662e-05 \n",
      "epoch 48 step 199 acc 0.4063745019920319\n",
      "acc 0.6966666666666667\n",
      "49 loss -->  1.7002646472974448e-06 \n",
      "epoch 49 step 199 acc 0.4163346613545817\n",
      "acc 0.6766666666666666\n",
      "50 loss -->  0.00014070233737584203 \n",
      "epoch 50 step 199 acc 0.4043824701195219\n",
      "acc 0.71\n",
      "51 loss -->  1.3829553608957212e-06 \n",
      "epoch 51 step 199 acc 0.4243027888446215\n",
      "acc 0.68\n",
      "52 loss -->  3.913810724043287e-05 \n",
      "epoch 52 step 199 acc 0.4063745019920319\n",
      "acc 0.71\n",
      "53 loss -->  4.7661518465247354e-07 \n",
      "epoch 53 step 199 acc 0.4243027888446215\n",
      "acc 0.6766666666666666\n",
      "54 loss -->  7.616168295498937e-05 \n",
      "epoch 54 step 199 acc 0.4043824701195219\n",
      "acc 0.71\n",
      "55 loss -->  4.5971222562002367e-07 \n",
      "epoch 55 step 199 acc 0.4243027888446215\n",
      "acc 0.6833333333333333\n",
      "56 loss -->  1.9745322788367048e-05 \n",
      "epoch 56 step 199 acc 0.40836653386454186\n",
      "acc 0.7166666666666667\n",
      "57 loss -->  1.3661821185451117e-06 \n",
      "epoch 57 step 199 acc 0.42828685258964144\n",
      "acc 0.69\n",
      "58 loss -->  6.84125188854523e-06 \n",
      "epoch 58 step 199 acc 0.4123505976095618\n",
      "acc 0.7133333333333334\n",
      "59 loss -->  6.443009397116839e-07 \n",
      "epoch 59 step 199 acc 0.4262948207171315\n",
      "acc 0.7033333333333334\n",
      "60 loss -->  5.4699991203222e-09 \n",
      "epoch 60 step 199 acc 0.4203187250996016\n",
      "acc 0.7033333333333334\n",
      "61 loss -->  9.710721826650115e-08 \n",
      "epoch 61 step 199 acc 0.4203187250996016\n",
      "acc 0.7\n",
      "62 loss -->  3.4892611466830203e-08 \n",
      "epoch 62 step 199 acc 0.41832669322709165\n",
      "acc 0.6966666666666667\n",
      "63 loss -->  5.603271180376623e-09 \n",
      "epoch 63 step 199 acc 0.4163346613545817\n",
      "acc 0.7\n",
      "64 loss -->  9.84822334793023e-10 \n",
      "epoch 64 step 199 acc 0.41832669322709165\n",
      "acc 0.69\n",
      "65 loss -->  8.438029226454091e-07 \n",
      "epoch 65 step 199 acc 0.4123505976095618\n",
      "acc 0.67\n",
      "66 loss -->  4.886779869650582e-10 \n",
      "epoch 66 step 199 acc 0.40039840637450197\n",
      "acc 0.5133333333333333\n",
      "67 loss -->  0.04228377342224121 \n",
      "epoch 67 step 199 acc 0.30677290836653387\n",
      "acc 0.6266666666666667\n",
      "68 loss -->  0.012317555025219917 \n",
      "epoch 68 step 199 acc 0.3745019920318725\n",
      "acc 0.6733333333333333\n",
      "69 loss -->  9.489336427748185e-09 \n",
      "epoch 69 step 199 acc 0.40239043824701193\n",
      "acc 0.6666666666666666\n",
      "70 loss -->  2.4951983590426607e-08 \n",
      "epoch 70 step 199 acc 0.398406374501992\n",
      "acc 0.6866666666666666\n",
      "71 loss -->  8.801333273300571e-11 \n",
      "epoch 71 step 199 acc 0.4103585657370518\n",
      "acc 0.68\n",
      "72 loss -->  1.1341612093929143e-09 \n",
      "epoch 72 step 199 acc 0.4063745019920319\n",
      "acc 0.69\n",
      "73 loss -->  2.432132828467104e-10 \n",
      "epoch 73 step 199 acc 0.4123505976095618\n",
      "acc 0.6866666666666666\n",
      "74 loss -->  5.507871381205121e-10 \n",
      "epoch 74 step 199 acc 0.4103585657370518\n",
      "acc 0.69\n",
      "75 loss -->  2.348185534906122e-10 \n",
      "epoch 75 step 199 acc 0.4123505976095618\n",
      "acc 0.6833333333333333\n",
      "76 loss -->  4.574801093504277e-10 \n",
      "epoch 76 step 199 acc 0.40836653386454186\n",
      "acc 0.6866666666666666\n",
      "77 loss -->  3.3439989666206316e-10 \n",
      "epoch 77 step 199 acc 0.4103585657370518\n",
      "acc 0.6866666666666666\n",
      "78 loss -->  2.5628743571814994e-10 \n",
      "epoch 78 step 199 acc 0.4103585657370518\n",
      "acc 0.69\n",
      "79 loss -->  2.0473631123785196e-10 \n",
      "epoch 79 step 199 acc 0.4123505976095618\n",
      "acc 0.6833333333333333\n",
      "80 loss -->  6.771987415277181e-10 \n",
      "epoch 80 step 199 acc 0.40836653386454186\n",
      "acc 0.6833333333333333\n",
      "81 loss -->  1.0194272093144718e-09 \n",
      "epoch 81 step 199 acc 0.40836653386454186\n",
      "acc 0.68\n",
      "82 loss -->  4.890417848457673e-09 \n",
      "epoch 82 step 199 acc 0.4063745019920319\n",
      "acc 0.6966666666666667\n",
      "83 loss -->  5.012396053771795e-10 \n",
      "epoch 83 step 199 acc 0.4163346613545817\n",
      "acc 0.6766666666666666\n",
      "84 loss -->  4.048359980579619e-10 \n",
      "epoch 84 step 199 acc 0.4043824701195219\n",
      "acc 0.6866666666666666\n",
      "85 loss -->  1.3598758230592978e-10 \n",
      "epoch 85 step 199 acc 0.4103585657370518\n",
      "acc 0.68\n",
      "86 loss -->  1.8189680317526324e-10 \n",
      "epoch 86 step 199 acc 0.4063745019920319\n",
      "acc 0.6866666666666666\n",
      "87 loss -->  1.2372805557880895e-10 \n",
      "epoch 87 step 199 acc 0.4103585657370518\n",
      "acc 0.67\n",
      "88 loss -->  2.1184654031003447e-10 \n",
      "epoch 88 step 199 acc 0.40039840637450197\n",
      "acc 0.6833333333333333\n",
      "89 loss -->  5.856824469852029e-10 \n",
      "epoch 89 step 199 acc 0.40836653386454186\n",
      "acc 0.6766666666666666\n",
      "90 loss -->  1.194541567883789e-07 \n",
      "epoch 90 step 199 acc 0.4043824701195219\n",
      "acc 0.6866666666666666\n",
      "91 loss -->  1.2207118649243398e-09 \n",
      "epoch 91 step 199 acc 0.4103585657370518\n",
      "acc 0.68\n",
      "92 loss -->  5.867292207639707e-10 \n",
      "epoch 92 step 199 acc 0.4063745019920319\n",
      "acc 0.6766666666666666\n",
      "93 loss -->  6.473227509573576e-10 \n",
      "epoch 93 step 199 acc 0.4043824701195219\n",
      "acc 0.6633333333333333\n",
      "94 loss -->  5.384211299030994e-09 \n",
      "epoch 94 step 199 acc 0.39641434262948205\n",
      "acc 0.6833333333333333\n",
      "95 loss -->  4.430642519537287e-09 \n",
      "epoch 95 step 199 acc 0.40836653386454186\n",
      "acc 0.6466666666666666\n",
      "96 loss -->  1.0810943251726712e-09 \n",
      "epoch 96 step 199 acc 0.38645418326693226\n",
      "acc 0.6666666666666666\n",
      "97 loss -->  8.879899482394649e-09 \n",
      "epoch 97 step 199 acc 0.398406374501992\n",
      "acc 0.6733333333333333\n",
      "98 loss -->  5.418006487900584e-09 \n",
      "epoch 98 step 199 acc 0.40239043824701193\n",
      "acc 0.6666666666666666\n",
      "99 loss -->  3.813913629358012e-09 \n",
      "epoch 99 step 199 acc 0.398406374501992\n",
      "acc 0.6666666666666666\n",
      "100 loss -->  4.094245831254284e-09 \n",
      "epoch 100 step 199 acc 0.398406374501992\n",
      "acc 0.6666666666666666\n",
      "101 loss -->  2.941853871618605e-09 \n",
      "epoch 101 step 199 acc 0.398406374501992\n",
      "acc 0.6666666666666666\n",
      "102 loss -->  3.5073042248257025e-09 \n",
      "epoch 102 step 199 acc 0.398406374501992\n",
      "acc 0.6733333333333333\n",
      "103 loss -->  1.9940342710356163e-09 \n",
      "epoch 103 step 199 acc 0.40239043824701193\n",
      "acc 0.6733333333333333\n",
      "104 loss -->  2.9755702346534463e-09 \n",
      "epoch 104 step 199 acc 0.40239043824701193\n",
      "acc 0.6666666666666666\n",
      "105 loss -->  1.0858363097554502e-09 \n",
      "epoch 105 step 199 acc 0.398406374501992\n",
      "acc 0.6766666666666666\n",
      "106 loss -->  4.124149022288748e-09 \n",
      "epoch 106 step 199 acc 0.4043824701195219\n",
      "acc 0.67\n",
      "107 loss -->  5.401347147326874e-10 \n",
      "epoch 107 step 199 acc 0.40039840637450197\n",
      "acc 0.6766666666666666\n",
      "108 loss -->  7.309856275838911e-09 \n",
      "epoch 108 step 199 acc 0.4043824701195219\n",
      "acc 0.6766666666666666\n",
      "109 loss -->  2.765611628596787e-10 \n",
      "epoch 109 step 199 acc 0.4043824701195219\n",
      "acc 0.6833333333333333\n",
      "110 loss -->  1.6249380729860263e-09 \n",
      "epoch 110 step 199 acc 0.40836653386454186\n",
      "acc 0.68\n",
      "111 loss -->  2.7117721956848584e-10 \n",
      "epoch 111 step 199 acc 0.4063745019920319\n",
      "acc 0.6833333333333333\n",
      "112 loss -->  1.1490142171055595e-09 \n",
      "epoch 112 step 199 acc 0.40836653386454186\n",
      "acc 0.6833333333333333\n",
      "113 loss -->  2.7565363880377447e-10 \n",
      "epoch 113 step 199 acc 0.40836653386454186\n",
      "acc 0.6766666666666666\n",
      "114 loss -->  3.0132731865251117e-09 \n",
      "epoch 114 step 199 acc 0.4043824701195219\n",
      "acc 0.68\n",
      "115 loss -->  1.2434497875801753e-12 \n",
      "epoch 115 step 199 acc 0.4063745019920319\n",
      "acc 0.6966666666666667\n",
      "116 loss -->  6.460709189859415e-10 \n",
      "epoch 116 step 199 acc 0.4163346613545817\n",
      "acc 0.6833333333333333\n",
      "117 loss -->  3.175937846044974e-10 \n",
      "epoch 117 step 199 acc 0.40836653386454186\n",
      "acc 0.69\n",
      "118 loss -->  1.7498033022533832e-09 \n",
      "epoch 118 step 199 acc 0.4123505976095618\n",
      "acc 0.7066666666666667\n",
      "119 loss -->  2.3240361024523537e-10 \n",
      "epoch 119 step 199 acc 0.42231075697211157\n",
      "acc 0.71\n",
      "120 loss -->  7.474909363355819e-13 \n",
      "epoch 120 step 199 acc 0.4243027888446215\n",
      "acc 0.7033333333333334\n",
      "121 loss -->  4.008882201894659e-12 \n",
      "epoch 121 step 199 acc 0.4203187250996016\n",
      "acc 0.7133333333333334\n",
      "122 loss -->  4.1140425484711973e-13 \n",
      "epoch 122 step 199 acc 0.4262948207171315\n",
      "acc 0.7033333333333334\n",
      "123 loss -->  1.4399148106497561e-12 \n",
      "epoch 123 step 199 acc 0.4203187250996016\n",
      "acc 0.7033333333333334\n",
      "124 loss -->  4.2064130499099017e-13 \n",
      "epoch 124 step 199 acc 0.4203187250996016\n",
      "acc 0.69\n",
      "125 loss -->  3.609468279819339e-11 \n",
      "epoch 125 step 199 acc 0.4123505976095618\n",
      "acc 0.7033333333333334\n",
      "126 loss -->  1.4768631196454551e-12 \n",
      "epoch 126 step 199 acc 0.4203187250996016\n",
      "acc 0.7033333333333334\n",
      "127 loss -->  7.680966865146466e-13 \n",
      "epoch 127 step 199 acc 0.4203187250996016\n",
      "acc 0.7033333333333334\n",
      "128 loss -->  1.8118839423069376e-14 \n",
      "epoch 128 step 199 acc 0.4203187250996016\n",
      "acc 0.6433333333333333\n",
      "129 loss -->  2.0833113879847875e-12 \n",
      "epoch 129 step 199 acc 0.3844621513944223\n",
      "acc 0.6533333333333333\n",
      "130 loss -->  3.057717185583897e-05 \n",
      "epoch 130 step 199 acc 0.3904382470119522\n",
      "acc 0.6333333333333333\n",
      "131 loss -->  2.8146191652922425e-06 \n",
      "epoch 131 step 199 acc 0.3784860557768924\n",
      "acc 0.7066666666666667\n",
      "132 loss -->  4.800796773452021e-07 \n",
      "epoch 132 step 199 acc 0.42231075697211157\n",
      "acc 0.6733333333333333\n",
      "133 loss -->  1.5418733312344557e-07 \n",
      "epoch 133 step 199 acc 0.40239043824701193\n",
      "acc 0.68\n",
      "134 loss -->  3.185943455719098e-07 \n",
      "epoch 134 step 199 acc 0.4063745019920319\n",
      "acc 0.6933333333333334\n",
      "135 loss -->  1.7492600818513893e-05 \n",
      "epoch 135 step 199 acc 0.41434262948207173\n",
      "acc 0.6966666666666667\n",
      "136 loss -->  1.5166193634286174e-06 \n",
      "epoch 136 step 199 acc 0.4163346613545817\n",
      "acc 0.69\n",
      "137 loss -->  4.359893068794918e-07 \n",
      "epoch 137 step 199 acc 0.4123505976095618\n",
      "acc 0.7\n",
      "138 loss -->  7.121986982383532e-07 \n",
      "epoch 138 step 199 acc 0.41832669322709165\n",
      "acc 0.6966666666666667\n",
      "139 loss -->  4.318335982134158e-07 \n",
      "epoch 139 step 199 acc 0.4163346613545817\n",
      "acc 0.6966666666666667\n",
      "140 loss -->  3.498312821648142e-07 \n",
      "epoch 140 step 199 acc 0.4163346613545817\n",
      "acc 0.6966666666666667\n",
      "141 loss -->  3.276443294453202e-07 \n",
      "epoch 141 step 199 acc 0.4163346613545817\n",
      "acc 0.69\n",
      "142 loss -->  2.816736355271132e-07 \n",
      "epoch 142 step 199 acc 0.4123505976095618\n",
      "acc 0.6933333333333334\n",
      "143 loss -->  1.9759875158342766e-07 \n",
      "epoch 143 step 199 acc 0.41434262948207173\n",
      "acc 0.6866666666666666\n",
      "144 loss -->  1.9842995868657454e-07 \n",
      "epoch 144 step 199 acc 0.4103585657370518\n",
      "acc 0.7\n",
      "145 loss -->  1.569785013089131e-07 \n",
      "epoch 145 step 199 acc 0.41832669322709165\n",
      "acc 0.6866666666666666\n",
      "146 loss -->  2.0425970603810129e-07 \n",
      "epoch 146 step 199 acc 0.4103585657370518\n",
      "acc 0.69\n",
      "147 loss -->  1.7861928824913775e-07 \n",
      "epoch 147 step 199 acc 0.4123505976095618\n",
      "acc 0.69\n",
      "148 loss -->  8.483674207582226e-08 \n",
      "epoch 148 step 199 acc 0.4123505976095618\n",
      "acc 0.6866666666666666\n",
      "149 loss -->  7.992603912043705e-08 \n",
      "epoch 149 step 199 acc 0.4103585657370518\n",
      "acc 0.69\n",
      "150 loss -->  1.8313075145215407e-07 \n",
      "epoch 150 step 199 acc 0.4123505976095618\n",
      "acc 0.6866666666666666\n",
      "151 loss -->  6.479060488118193e-08 \n",
      "epoch 151 step 199 acc 0.4103585657370518\n",
      "acc 0.6866666666666666\n",
      "152 loss -->  1.0729439736678614e-07 \n",
      "epoch 152 step 199 acc 0.4103585657370518\n",
      "acc 0.6966666666666667\n",
      "153 loss -->  1.1798523757988733e-07 \n",
      "epoch 153 step 199 acc 0.4163346613545817\n",
      "acc 0.6866666666666666\n",
      "154 loss -->  1.0749620571459673e-07 \n",
      "epoch 154 step 199 acc 0.4103585657370518\n",
      "acc 0.6966666666666667\n",
      "155 loss -->  6.913231942462517e-08 \n",
      "epoch 155 step 199 acc 0.4163346613545817\n",
      "acc 0.6933333333333334\n",
      "156 loss -->  7.322704220769083e-08 \n",
      "epoch 156 step 199 acc 0.41434262948207173\n",
      "acc 0.6933333333333334\n",
      "157 loss -->  7.254864442529652e-08 \n",
      "epoch 157 step 199 acc 0.41434262948207173\n",
      "acc 0.6833333333333333\n",
      "158 loss -->  5.5849575630873005e-08 \n",
      "epoch 158 step 199 acc 0.40836653386454186\n",
      "acc 0.7\n",
      "159 loss -->  9.394561573117244e-08 \n",
      "epoch 159 step 199 acc 0.41832669322709165\n",
      "acc 0.7033333333333334\n",
      "160 loss -->  9.164488687929406e-07 \n",
      "epoch 160 step 199 acc 0.4203187250996016\n",
      "acc 0.7\n",
      "161 loss -->  8.786214777956047e-08 \n",
      "epoch 161 step 199 acc 0.41832669322709165\n",
      "acc 0.7066666666666667\n",
      "162 loss -->  8.481433155793638e-08 \n",
      "epoch 162 step 199 acc 0.42231075697211157\n",
      "acc 0.7\n",
      "163 loss -->  5.2968072594694604e-08 \n",
      "epoch 163 step 199 acc 0.41832669322709165\n",
      "acc 0.7133333333333334\n",
      "164 loss -->  4.7427047178416615e-08 \n",
      "epoch 164 step 199 acc 0.4262948207171315\n",
      "acc 0.7066666666666667\n",
      "165 loss -->  3.6578803985776176e-08 \n",
      "epoch 165 step 199 acc 0.42231075697211157\n",
      "acc 0.7\n",
      "166 loss -->  2.6348576653845157e-08 \n",
      "epoch 166 step 199 acc 0.41832669322709165\n",
      "acc 0.7033333333333334\n",
      "167 loss -->  1.4971265116514587e-08 \n",
      "epoch 167 step 199 acc 0.4203187250996016\n",
      "acc 0.71\n",
      "168 loss -->  4.1758813296155495e-08 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 168 step 199 acc 0.4243027888446215\n",
      "acc 0.7166666666666667\n",
      "169 loss -->  2.2043824543516166e-08 \n",
      "epoch 169 step 199 acc 0.42828685258964144\n",
      "acc 0.7033333333333334\n",
      "170 loss -->  4.380496074381235e-13 \n",
      "epoch 170 step 199 acc 0.4203187250996016\n",
      "acc 0.6833333333333333\n",
      "171 loss -->  1.1586820045095259e-11 \n",
      "epoch 171 step 199 acc 0.40836653386454186\n",
      "acc 0.67\n",
      "172 loss -->  1.287731365984257e-09 \n",
      "epoch 172 step 199 acc 0.40039840637450197\n",
      "acc 0.68\n",
      "173 loss -->  5.9531334528628577e-08 \n",
      "epoch 173 step 199 acc 0.4063745019920319\n",
      "acc 0.66\n",
      "174 loss -->  1.93508906676243e-08 \n",
      "epoch 174 step 199 acc 0.3944223107569721\n",
      "acc 0.6766666666666666\n",
      "175 loss -->  7.669941126664526e-09 \n",
      "epoch 175 step 199 acc 0.4043824701195219\n",
      "acc 0.6733333333333333\n",
      "176 loss -->  7.292720649587636e-09 \n",
      "epoch 176 step 199 acc 0.40239043824701193\n",
      "acc 0.6733333333333333\n",
      "177 loss -->  7.142554547812097e-09 \n",
      "epoch 177 step 199 acc 0.40239043824701193\n",
      "acc 0.6766666666666666\n",
      "178 loss -->  6.880153335941941e-09 \n",
      "epoch 178 step 199 acc 0.4043824701195219\n",
      "acc 0.6766666666666666\n",
      "179 loss -->  6.66506849711368e-09 \n",
      "epoch 179 step 199 acc 0.4043824701195219\n",
      "acc 0.6733333333333333\n",
      "180 loss -->  6.409377029115149e-09 \n",
      "epoch 180 step 199 acc 0.40239043824701193\n",
      "acc 0.6733333333333333\n",
      "181 loss -->  6.207541591862764e-09 \n",
      "epoch 181 step 199 acc 0.40239043824701193\n",
      "acc 0.6733333333333333\n",
      "182 loss -->  6.219087911318866e-09 \n",
      "epoch 182 step 199 acc 0.40239043824701193\n",
      "acc 0.6733333333333333\n",
      "183 loss -->  6.005281161236553e-09 \n",
      "epoch 183 step 199 acc 0.40239043824701193\n",
      "acc 0.6733333333333333\n",
      "184 loss -->  5.734907215781959e-09 \n",
      "epoch 184 step 199 acc 0.40239043824701193\n",
      "acc 0.6733333333333333\n",
      "185 loss -->  5.619662069022979e-09 \n",
      "epoch 185 step 199 acc 0.40239043824701193\n",
      "acc 0.67\n",
      "186 loss -->  5.384659829132943e-09 \n",
      "epoch 186 step 199 acc 0.40039840637450197\n",
      "acc 0.6733333333333333\n",
      "187 loss -->  5.290141213976085e-09 \n",
      "epoch 187 step 199 acc 0.40239043824701193\n",
      "acc 0.67\n",
      "188 loss -->  5.07992625742304e-09 \n",
      "epoch 188 step 199 acc 0.40039840637450197\n",
      "acc 0.67\n",
      "189 loss -->  5.1405466550136225e-09 \n",
      "epoch 189 step 199 acc 0.40039840637450197\n",
      "acc 0.67\n",
      "190 loss -->  5.0079633773236765e-09 \n",
      "epoch 190 step 199 acc 0.40039840637450197\n",
      "acc 0.67\n",
      "191 loss -->  4.9417661074357966e-09 \n",
      "epoch 191 step 199 acc 0.40039840637450197\n",
      "acc 0.6666666666666666\n",
      "192 loss -->  4.957640520331097e-09 \n",
      "epoch 192 step 199 acc 0.398406374501992\n",
      "acc 0.6666666666666666\n",
      "193 loss -->  5.054121121617072e-09 \n",
      "epoch 193 step 199 acc 0.398406374501992\n",
      "acc 0.67\n",
      "194 loss -->  5.224145560589477e-09 \n",
      "epoch 194 step 199 acc 0.40039840637450197\n",
      "acc 0.6666666666666666\n",
      "195 loss -->  5.374139799840805e-09 \n",
      "epoch 195 step 199 acc 0.398406374501992\n",
      "acc 0.6666666666666666\n",
      "196 loss -->  4.921886009867649e-09 \n",
      "epoch 196 step 199 acc 0.398406374501992\n",
      "acc 0.6666666666666666\n",
      "197 loss -->  3.834512707356907e-09 \n",
      "epoch 197 step 199 acc 0.398406374501992\n",
      "acc 0.6666666666666666\n",
      "198 loss -->  3.302030204821449e-09 \n",
      "epoch 198 step 199 acc 0.398406374501992\n",
      "acc 0.6666666666666666\n",
      "199 loss -->  3.18313864156039e-09 \n",
      "epoch 199 step 199 acc 0.398406374501992\n",
      "acc 0.6633333333333333\n",
      "200 loss -->  3.0222679914260198e-09 \n",
      "epoch 200 step 199 acc 0.39641434262948205\n",
      "acc 0.66\n",
      "201 loss -->  2.0746671047788823e-09 \n",
      "epoch 201 step 199 acc 0.3944223107569721\n",
      "acc 0.66\n",
      "202 loss -->  2.3733854881413663e-09 \n",
      "epoch 202 step 199 acc 0.3944223107569721\n",
      "acc 0.66\n",
      "203 loss -->  1.944219452099105e-09 \n",
      "epoch 203 step 199 acc 0.3944223107569721\n",
      "acc 0.6566666666666666\n",
      "204 loss -->  1.7476070590660697e-09 \n",
      "epoch 204 step 199 acc 0.39243027888446214\n",
      "acc 0.6733333333333333\n",
      "205 loss -->  1.429612317416229e-09 \n",
      "epoch 205 step 199 acc 0.40239043824701193\n",
      "acc 0.66\n",
      "206 loss -->  6.914003813918157e-10 \n",
      "epoch 206 step 199 acc 0.3944223107569721\n",
      "acc 0.66\n",
      "207 loss -->  7.934864987291235e-10 \n",
      "epoch 207 step 199 acc 0.3944223107569721\n",
      "acc 0.6666666666666666\n",
      "208 loss -->  5.114365597691517e-10 \n",
      "epoch 208 step 199 acc 0.398406374501992\n",
      "acc 0.6633333333333333\n",
      "209 loss -->  4.510862794404602e-10 \n",
      "epoch 209 step 199 acc 0.39641434262948205\n",
      "acc 0.6633333333333333\n",
      "210 loss -->  3.8544989422462095e-10 \n",
      "epoch 210 step 199 acc 0.39641434262948205\n",
      "acc 0.67\n",
      "211 loss -->  2.767302775819047e-10 \n",
      "epoch 211 step 199 acc 0.40039840637450197\n",
      "acc 0.6666666666666666\n",
      "212 loss -->  2.3097665446947246e-10 \n",
      "epoch 212 step 199 acc 0.398406374501992\n",
      "acc 0.6666666666666666\n",
      "213 loss -->  1.4479847876280871e-10 \n",
      "epoch 213 step 199 acc 0.398406374501992\n",
      "acc 0.67\n",
      "214 loss -->  1.1177743453050226e-10 \n",
      "epoch 214 step 199 acc 0.40039840637450197\n",
      "acc 0.6733333333333333\n",
      "215 loss -->  1.028158877480756e-10 \n",
      "epoch 215 step 199 acc 0.40239043824701193\n",
      "acc 0.6733333333333333\n",
      "216 loss -->  7.211955338481957e-11 \n",
      "epoch 216 step 199 acc 0.40239043824701193\n",
      "acc 0.6733333333333333\n",
      "217 loss -->  5.841442607401603e-11 \n",
      "epoch 217 step 199 acc 0.40239043824701193\n",
      "acc 0.63\n",
      "218 loss -->  2.7219471468442613e-11 \n",
      "epoch 218 step 199 acc 0.37649402390438247\n",
      "acc 0.6833333333333333\n",
      "219 loss -->  9.613309259748348e-08 \n",
      "epoch 219 step 199 acc 0.40836653386454186\n",
      "acc 0.6266666666666667\n",
      "220 loss -->  5.612832154611169e-08 \n",
      "epoch 220 step 199 acc 0.3745019920318725\n",
      "acc 0.6833333333333333\n",
      "221 loss -->  0.0 \n",
      "epoch 221 step 199 acc 0.40836653386454186\n",
      "acc 0.6966666666666667\n",
      "222 loss -->  1.7763568394002505e-15 \n",
      "epoch 222 step 199 acc 0.4163346613545817\n",
      "acc 0.6966666666666667\n",
      "223 loss -->  2.4868995328087033e-15 \n",
      "epoch 223 step 199 acc 0.4163346613545817\n",
      "acc 0.7\n",
      "224 loss -->  2.1316282919835953e-15 \n",
      "epoch 224 step 199 acc 0.41832669322709165\n",
      "acc 0.6966666666666667\n",
      "225 loss -->  2.1316282919835953e-15 \n",
      "epoch 225 step 199 acc 0.4163346613545817\n",
      "acc 0.7\n",
      "226 loss -->  1.7763568394002505e-15 \n",
      "epoch 226 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "227 loss -->  1.421085492696024e-15 \n",
      "epoch 227 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "228 loss -->  1.421085492696024e-15 \n",
      "epoch 228 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "229 loss -->  1.421085492696024e-15 \n",
      "epoch 229 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "230 loss -->  1.421085492696024e-15 \n",
      "epoch 230 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "231 loss -->  1.421085492696024e-15 \n",
      "epoch 231 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "232 loss -->  1.421085492696024e-15 \n",
      "epoch 232 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "233 loss -->  1.421085492696024e-15 \n",
      "epoch 233 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "234 loss -->  1.0658141459917976e-15 \n",
      "epoch 234 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "235 loss -->  1.0658141459917976e-15 \n",
      "epoch 235 step 199 acc 0.41832669322709165\n",
      "acc 0.7033333333333334\n",
      "236 loss -->  1.0658141459917976e-15 \n",
      "epoch 236 step 199 acc 0.4203187250996016\n",
      "acc 0.7033333333333334\n",
      "237 loss -->  1.0658141459917976e-15 \n",
      "epoch 237 step 199 acc 0.4203187250996016\n",
      "acc 0.7033333333333334\n",
      "238 loss -->  1.0658141459917976e-15 \n",
      "epoch 238 step 199 acc 0.4203187250996016\n",
      "acc 0.7066666666666667\n",
      "239 loss -->  7.10542746348012e-16 \n",
      "epoch 239 step 199 acc 0.42231075697211157\n",
      "acc 0.6966666666666667\n",
      "240 loss -->  7.10542746348012e-16 \n",
      "epoch 240 step 199 acc 0.4163346613545817\n",
      "acc 0.71\n",
      "241 loss -->  3.55271373174006e-16 \n",
      "epoch 241 step 199 acc 0.4243027888446215\n",
      "acc 0.7066666666666667\n",
      "242 loss -->  3.55271373174006e-16 \n",
      "epoch 242 step 199 acc 0.42231075697211157\n",
      "acc 0.71\n",
      "243 loss -->  3.55271373174006e-16 \n",
      "epoch 243 step 199 acc 0.4243027888446215\n",
      "acc 0.71\n",
      "244 loss -->  3.55271373174006e-16 \n",
      "epoch 244 step 199 acc 0.4243027888446215\n",
      "acc 0.71\n",
      "245 loss -->  3.55271373174006e-16 \n",
      "epoch 245 step 199 acc 0.4243027888446215\n",
      "acc 0.71\n",
      "246 loss -->  3.55271373174006e-16 \n",
      "epoch 246 step 199 acc 0.4243027888446215\n",
      "acc 0.71\n",
      "247 loss -->  3.55271373174006e-16 \n",
      "epoch 247 step 199 acc 0.4243027888446215\n",
      "acc 0.71\n",
      "248 loss -->  3.55271373174006e-16 \n",
      "epoch 248 step 199 acc 0.4243027888446215\n",
      "acc 0.71\n",
      "249 loss -->  0.0 \n",
      "epoch 249 step 199 acc 0.4243027888446215\n",
      "acc 0.71\n",
      "250 loss -->  3.55271373174006e-16 \n",
      "epoch 250 step 199 acc 0.4243027888446215\n",
      "acc 0.7133333333333334\n",
      "251 loss -->  0.0 \n",
      "epoch 251 step 199 acc 0.4262948207171315\n",
      "acc 0.7133333333333334\n",
      "252 loss -->  0.0 \n",
      "epoch 252 step 199 acc 0.4262948207171315\n",
      "acc 0.71\n",
      "253 loss -->  0.0 \n",
      "epoch 253 step 199 acc 0.4243027888446215\n",
      "acc 0.71\n",
      "254 loss -->  0.0 \n",
      "epoch 254 step 199 acc 0.4243027888446215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.71\n",
      "255 loss -->  0.0 \n",
      "epoch 255 step 199 acc 0.4243027888446215\n",
      "acc 0.7133333333333334\n",
      "256 loss -->  0.0 \n",
      "epoch 256 step 199 acc 0.4262948207171315\n",
      "acc 0.7066666666666667\n",
      "257 loss -->  2.2843950038889393e-13 \n",
      "epoch 257 step 199 acc 0.42231075697211157\n",
      "acc 0.6933333333333334\n",
      "258 loss -->  1.9184654204335884e-14 \n",
      "epoch 258 step 199 acc 0.41434262948207173\n",
      "acc 0.6933333333333334\n",
      "259 loss -->  3.907985131383846e-15 \n",
      "epoch 259 step 199 acc 0.41434262948207173\n",
      "acc 0.6966666666666667\n",
      "260 loss -->  2.4868995328087033e-15 \n",
      "epoch 260 step 199 acc 0.4163346613545817\n",
      "acc 0.7\n",
      "261 loss -->  1.7763568394002505e-15 \n",
      "epoch 261 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "262 loss -->  1.421085492696024e-15 \n",
      "epoch 262 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "263 loss -->  1.0658141459917976e-15 \n",
      "epoch 263 step 199 acc 0.41832669322709165\n",
      "acc 0.7066666666666667\n",
      "264 loss -->  2.1316282919835953e-15 \n",
      "epoch 264 step 199 acc 0.42231075697211157\n",
      "acc 0.71\n",
      "265 loss -->  1.0658141459917976e-15 \n",
      "epoch 265 step 199 acc 0.4243027888446215\n",
      "acc 0.7133333333333334\n",
      "266 loss -->  1.421085492696024e-15 \n",
      "epoch 266 step 199 acc 0.4262948207171315\n",
      "acc 0.71\n",
      "267 loss -->  1.0658141459917976e-15 \n",
      "epoch 267 step 199 acc 0.4243027888446215\n",
      "acc 0.7066666666666667\n",
      "268 loss -->  7.10542746348012e-16 \n",
      "epoch 268 step 199 acc 0.42231075697211157\n",
      "acc 0.71\n",
      "269 loss -->  3.55271373174006e-16 \n",
      "epoch 269 step 199 acc 0.4243027888446215\n",
      "acc 0.7033333333333334\n",
      "270 loss -->  3.55271373174006e-16 \n",
      "epoch 270 step 199 acc 0.4203187250996016\n",
      "acc 0.7166666666666667\n",
      "271 loss -->  0.0 \n",
      "epoch 271 step 199 acc 0.42828685258964144\n",
      "acc 0.72\n",
      "272 loss -->  0.0 \n",
      "epoch 272 step 199 acc 0.4302788844621514\n",
      "acc 0.72\n",
      "273 loss -->  0.0 \n",
      "epoch 273 step 199 acc 0.4302788844621514\n",
      "acc 0.7133333333333334\n",
      "274 loss -->  0.0 \n",
      "epoch 274 step 199 acc 0.4262948207171315\n",
      "acc 0.7133333333333334\n",
      "275 loss -->  0.0 \n",
      "epoch 275 step 199 acc 0.4262948207171315\n",
      "acc 0.7133333333333334\n",
      "276 loss -->  0.0 \n",
      "epoch 276 step 199 acc 0.4262948207171315\n",
      "acc 0.7166666666666667\n",
      "277 loss -->  0.0 \n",
      "epoch 277 step 199 acc 0.42828685258964144\n",
      "acc 0.7033333333333334\n",
      "278 loss -->  8.43769498715119e-15 \n",
      "epoch 278 step 199 acc 0.4203187250996016\n",
      "acc 0.66\n",
      "279 loss -->  0.0 \n",
      "epoch 279 step 199 acc 0.3944223107569721\n",
      "acc 0.7133333333333334\n",
      "280 loss -->  2.0570212742355987e-13 \n",
      "epoch 280 step 199 acc 0.4262948207171315\n",
      "acc 0.6966666666666667\n",
      "281 loss -->  2.0961011043736134e-14 \n",
      "epoch 281 step 199 acc 0.4163346613545817\n",
      "acc 0.7033333333333334\n",
      "282 loss -->  2.1316282919835953e-15 \n",
      "epoch 282 step 199 acc 0.4203187250996016\n",
      "acc 0.7\n",
      "283 loss -->  2.1316282919835953e-15 \n",
      "epoch 283 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "284 loss -->  2.1316282919835953e-15 \n",
      "epoch 284 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "285 loss -->  2.1316282919835953e-15 \n",
      "epoch 285 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "286 loss -->  2.1316282919835953e-15 \n",
      "epoch 286 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "287 loss -->  2.1316282919835953e-15 \n",
      "epoch 287 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "288 loss -->  2.4868995328087033e-15 \n",
      "epoch 288 step 199 acc 0.41832669322709165\n",
      "acc 0.7\n",
      "289 loss -->  2.1316282919835953e-15 \n",
      "epoch 289 step 199 acc 0.41832669322709165\n",
      "acc 0.7033333333333334\n",
      "290 loss -->  2.4868995328087033e-15 \n",
      "epoch 290 step 199 acc 0.4203187250996016\n",
      "acc 0.7066666666666667\n",
      "291 loss -->  2.4868995328087033e-15 \n",
      "epoch 291 step 199 acc 0.42231075697211157\n",
      "acc 0.7066666666666667\n",
      "292 loss -->  2.4868995328087033e-15 \n",
      "epoch 292 step 199 acc 0.42231075697211157\n",
      "acc 0.7066666666666667\n",
      "293 loss -->  2.4868995328087033e-15 \n",
      "epoch 293 step 199 acc 0.42231075697211157\n",
      "acc 0.7066666666666667\n",
      "294 loss -->  2.4868995328087033e-15 \n",
      "epoch 294 step 199 acc 0.42231075697211157\n",
      "acc 0.7066666666666667\n",
      "295 loss -->  2.1316282919835953e-15 \n",
      "epoch 295 step 199 acc 0.42231075697211157\n",
      "acc 0.7066666666666667\n",
      "296 loss -->  2.1316282919835953e-15 \n",
      "epoch 296 step 199 acc 0.42231075697211157\n",
      "acc 0.7066666666666667\n",
      "297 loss -->  2.1316282919835953e-15 \n",
      "epoch 297 step 199 acc 0.42231075697211157\n",
      "acc 0.7066666666666667\n",
      "298 loss -->  2.1316282919835953e-15 \n",
      "epoch 298 step 199 acc 0.42231075697211157\n",
      "acc 0.7133333333333334\n",
      "299 loss -->  2.4868995328087033e-15 \n",
      "epoch 299 step 199 acc 0.4262948207171315\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-bd85f674e3d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project_tw\\anly\\venv\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project_tw\\anly\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "model_path = 'D:/PROJECT_TW/git/data/nlp/w2v/simple_text_cnn.pkl'\n",
    "EPOCH=500;\n",
    "#构建textCNN模型\n",
    "cnn=textCNN(args)\n",
    "if os.path.exists(model_path):\n",
    "    print('load saved model ... ')\n",
    "    cnn.load_state_dict(torch.load(model_path))\n",
    "\n",
    "max_len=64\n",
    "#生成训练数据，需要将训练数据的Word转换为word的索引, 每段话最多只取64个词\n",
    "for i in range(0,len(texts)):\n",
    "    if len(texts[i])<max_len:\n",
    "        for j in range(0,len(texts[i])):\n",
    "            texts_with_id[i][j]=word_to_idx[texts[i][j]]\n",
    "        for j in range(len(texts[i]),max_len):\n",
    "            texts_with_id[i][j] = word_to_idx['']\n",
    "    else:\n",
    "        for j in range(0,max_len):\n",
    "            texts_with_id[i][j]=word_to_idx[texts[i][j]]\n",
    "\n",
    "LR = 0.001\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "#损失函数\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#训练批次大小\n",
    "epoch_size=10;\n",
    "texts_len=len(texts_with_id)\n",
    "print(texts_len)\n",
    "#划分训练数据和测试数据  Sklearn-train_test_split随机划分训练集和测试集\n",
    "x_train, x_test, y_train, y_test = train_test_split(texts_with_id, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "test_x=torch.LongTensor(x_test)\n",
    "test_y=torch.LongTensor(y_test)\n",
    "train_x=x_train\n",
    "train_y=y_train\n",
    "\n",
    "test_epoch_size=300;\n",
    "for epoch in range(EPOCH):\n",
    "    cnn.train()\n",
    "    for i in range(0,(int)(len(train_x)/epoch_size)):\n",
    "#     for i in range(0,1):\n",
    "        b_x = Variable(torch.LongTensor(train_x[i*epoch_size:i*epoch_size+epoch_size]))\n",
    "        \n",
    "        b_y = Variable(torch.LongTensor((train_y[i*epoch_size:i*epoch_size+epoch_size])))\n",
    "        output = cnn(b_x)\n",
    "        loss = loss_function(output, b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(loss)\n",
    "        pred_y = torch.max(output, 1)[1].data.squeeze()\n",
    "        acc = (b_y == pred_y)\n",
    "        acc = acc.numpy().sum()\n",
    "        accuracy = acc / (b_y.size(0))\n",
    "\n",
    "    acc_all = 0\n",
    "    cnn.eval()\n",
    "    for j in range(0, (int)(len(test_x) / test_epoch_size)):\n",
    "#     for j in range(0, 1):\n",
    "        b_x = Variable(torch.LongTensor(test_x[j * test_epoch_size:j * test_epoch_size + test_epoch_size]))\n",
    "        b_y = Variable(torch.LongTensor((test_y[j * test_epoch_size:j * test_epoch_size + test_epoch_size])))\n",
    "        test_output = cnn(b_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "#         print(pred_y)\n",
    "#         print(test_y)\n",
    "        acc = (pred_y == b_y)\n",
    "        acc = acc.numpy().sum()\n",
    "        print(\"acc \" + str(acc / b_y.size(0)))\n",
    "        acc_all = acc_all + acc\n",
    "\n",
    "    accuracy = acc_all / (test_y.size(0))\n",
    "    print('{} loss -->  {} '.format(epoch, loss))\n",
    "    print(\"epoch \" + str(epoch) + \" step \" + str(i) + \" \" + \"acc \" + str(accuracy))\n",
    "    torch.save(cnn.state_dict(),model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -55.1157,  -55.8341,  -26.0998,    4.1878, -118.9738,  -45.4318]])\n",
      "{0: '家居', 1: '彩票', 2: '房产', 3: '教育', 4: '股票', 5: '财经'}\n",
      "idx 3 labels  --> 教育\n"
     ]
    }
   ],
   "source": [
    "cnn=textCNN(args)\n",
    "if os.path.exists(model_path):\n",
    "    cnn.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "testDoc = '''\n",
    "专家提醒：留学忌盲目做好职业规划\n",
    "　　人民网·天津视窗1月3日电：记者在采访中了解到，在今年严峻的就业形势下，一部分大学毕业生选择考研、出国深造，以此应对金融危机带来的就业压力。\n",
    "　　在采访中，很多大学毕业生表示，他们希望选择读研或出国留学深造来规避就业压力。南开大学商学院市场营销专业的应届大学生陈凯告诉记者：“现在我正准备考研，提高自己各方面的理论知识和实践技能，换一个环境来增加自己的阅历。”此外，现在出国留学费用比以前便宜不少，出国留学也成了不少毕业生的选择。\n",
    "'''    \n",
    "# 提取中文, 分词， 得到词向量\n",
    "line = extract_chinese(testDoc)\n",
    "words = jieba.lcut(line, cut_all=False, HMM=True)\n",
    "words_model = getw2v()\n",
    "words_ids = [word_to_idx[x] for x in words if x in word_to_idx]\n",
    "words_ids = words_ids[0:64]\n",
    "words_ids = torch.LongTensor(words_ids)\n",
    "words_ids = words_ids.unsqueeze(0)\n",
    "pred_probs =  cnn(words_ids)\n",
    "print(pred_probs)\n",
    "pred_label = index_lables[torch.max(pred_probs,1)[1].data.item()]\n",
    "print(index_lables)\n",
    "print('idx {} labels  --> {}'.format(torch.max(pred_probs,1)[1].data.item(), pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: '家居', 1: '彩票', 2: '房产', 3: '教育', 4: '股票', 5: '财经'}\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(pred_probs,1)[1].data.item())\n",
    "print(index_lables)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
