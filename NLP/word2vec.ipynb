{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    http://www.dataguru.cn/article-13488-1.html\n",
    "    \n",
    "    https://github.com/Adoni/word2vec_pytorch\n",
    "    \n",
    "    https://cloud.tencent.com/developer/news/157676\n",
    "    \n",
    "    https://blog.csdn.net/BBZZ2/article/details/71123150?utm_source=blogxgwz1 高级词向量表达\n",
    "    \n",
    "    Word2vec，是为一群用来产生词向量的相关模型。这些模型为浅而双层的神经网络，用来训练以重新建构语言学之词文本。网络以词表现，并且需猜测相邻位置的输入词，在word2vec中词袋模型假设下，词的顺序是不重要的。训练完成之后，word2vec模型可用来映射每个词到一个向量，可用来表示词对词之间的关系，该向量为神经网络之隐藏层。Word2vec 可以根据给定的语料库，通过优化后的训练模型快速有效地将一个词语表达成向量形式，为自然语言处理领域的应用研究提供了新的工具。Word2vec依赖skip-grams或连续词袋（CBOW）来建立神经词嵌入。Word2vec为托马斯·米科洛夫（Tomas Mikolov）在Google带领的研究团队创造。该算法渐渐被其他人所分析和解释。\n",
    "    依赖\n",
    "    1) 词袋模型\n",
    "        词袋模型（Bag-of-words model）是个在自然语言处理和信息检索(IR)下被简化的表达模型。此模型下，像是句子或是文件这样的文字可以用一个袋子装着这些词的方式表现，这种表现方式不考虑文法以及词的顺序。最近词袋模型也被应用在计算机视觉领域。词袋模型被广泛应用在文件分类，词出现的频率可以用来当作训练分类器的特征。关于\"词袋\"这个用字的由来可追溯到泽里格·哈里斯于1954年在Distributional Structure的文章。\n",
    "        \n",
    "    2) Skip-gram 模型\n",
    "        Skip-gram 模型是一个简单但却非常实用的模型。在自然语言处理中，语料的选取是一个相当重要的问题: 第一，语料必须充分。一方面词典的词量要足够大，另一方面要尽可能多地包含反映词语之间关系的句子，例如，只有“鱼在水中游”这种句式在语料中尽可能地多，模型才能够学习到该句中的语义和语法关系，这和人类学习自然语言一个道理，重复的次数多了，也就会模仿了； 第二，语料必须准确。 也就是说所选取的语料能够正确反映该语言的语义和语法关系，这一点似乎不难做到，例如中文里，《人民日报》的语料比较准确。 但是，更多的时候，并不是语料的选取引发了对准确性问题的担忧，而是处理的方法。 n元模型中，因为窗口大小的限制，导致超出窗口范围的词语与当前词之间的关系不能被正确地反映到模型之中，如果单纯扩大窗口大小又会增加训练的复杂度。Skip-gram 模型的提出很好地解决了这些问题。顾名思义，Skip-gram 就是“跳过某些符号”，例如，句子“中国足球踢得真是太烂了”有4个3元词组，分别是“中国足球踢得”、“足球踢得真是”、“踢得真是太烂”、“真是太烂了”，可是我们发现，这个句子的本意就是“中国足球太烂”可是上述 4个3元词组并不能反映出这个信息。Skip-gram 模型却允许某些词被跳过，因此可以组成“中国足球太烂”这个3元词组。 如果允许跳过2个词，即 2-Skip-gram\n",
    "        \n",
    "        \n",
    "    word2vec和word embedding的区别 简言之，word embedding 是一个将词向量化的概念，中文译名为\"词嵌入\"。 word2vec是谷歌提出的一种word embedding的具体手段，采用了两种模型(CBOW与skip-gram模型)与两种方法(负采样与层次softmax方法)的组合，比较常见的组合为 skip-gram+负采样方法。\n",
    "    \n",
    "    典型代表：word2vec和auto-encoder （这里解释一下AutoEncoder，AutoEncoder也可以用于训练词向量，先将one hot映射成一个hidden state，再映射回原来的维度，令输入等于输出，取中间的hidden vector作为词向量，在不损耗原表达能力的前提下压缩向量维度，得到一个压缩的向量表达形式。） \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from collections import deque\n",
    "\n",
    "# deque 是双边队列（double-ended queue），具有队列和栈的性质，在 list 的基础上增加了移动、旋转和增删等\n",
    "numpy.random.seed(12345)\n",
    "class InputData:\n",
    "    \"\"\"Store data for word2vec, such as word map, sampling table and so on.\n",
    "    Attributes:\n",
    "        word_frequency: Count of each word, used for filtering low-frequency words and sampling table\n",
    "        word2id: Map from word to word id, without low-frequency words.\n",
    "        id2word: Map from word id to word, without low-frequency words.\n",
    "        sentence_count: Sentence count in files.\n",
    "        word_count: Word count in files, without low-frequency words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_name, min_count):\n",
    "        self.input_file_name = file_name\n",
    "        self.get_words(min_count)\n",
    "        self.word_pair_catch = deque()\n",
    "        self.init_sample_table()\n",
    "        print('Word Count: %d' % len(self.word2id))\n",
    "        print('Sentence Length: %d' % (self.sentence_length))\n",
    "\n",
    "    def get_words(self, min_count):\n",
    "        self.input_file = open(self.input_file_name)\n",
    "        self.sentence_length = 0\n",
    "        self.sentence_count = 0\n",
    "        word_frequency = dict()\n",
    "        for line in self.input_file:\n",
    "            self.sentence_count += 1\n",
    "            line = line.strip().split(' ')\n",
    "            self.sentence_length += len(line)\n",
    "            for w in line:\n",
    "                try:\n",
    "                    word_frequency[w] += 1\n",
    "                except:\n",
    "                    word_frequency[w] = 1\n",
    "        self.word2id = dict()\n",
    "        self.id2word = dict()\n",
    "        wid = 0\n",
    "        self.word_frequency = dict()\n",
    "        for w, c in word_frequency.items():\n",
    "            if c < min_count:\n",
    "                self.sentence_length -= c\n",
    "                continue\n",
    "            self.word2id[w] = wid\n",
    "            self.id2word[wid] = w\n",
    "            self.word_frequency[wid] = c\n",
    "            wid += 1\n",
    "        self.word_count = len(self.word2id)\n",
    "\n",
    "    def init_sample_table(self):\n",
    "        self.sample_table = []\n",
    "        sample_table_size = 1e8\n",
    "        pow_frequency = numpy.array(list(self.word_frequency.values()))**0.75   # ???? 为什么要 **0.75\n",
    "        words_pow = sum(pow_frequency)\n",
    "        ratio = pow_frequency / words_pow\n",
    "        count = numpy.round(ratio * sample_table_size)\n",
    "        for wid, c in enumerate(count):\n",
    "            self.sample_table += [wid] * int(c)\n",
    "        self.sample_table = numpy.array(self.sample_table)\n",
    "\n",
    "    # @profile\n",
    "    def get_batch_pairs(self, batch_size, window_size):\n",
    "        while len(self.word_pair_catch) < batch_size:\n",
    "            sentence = self.input_file.readline()\n",
    "            if sentence is None or sentence == '':\n",
    "                self.input_file = open(self.input_file_name)\n",
    "                sentence = self.input_file.readline()\n",
    "            word_ids = []\n",
    "            for word in sentence.strip().split(' '):\n",
    "                try:\n",
    "                    word_ids.append(self.word2id[word])\n",
    "                except:\n",
    "                    continue\n",
    "            for i, u in enumerate(word_ids):\n",
    "                for j, v in enumerate(\n",
    "                        word_ids[max(i - window_size, 0):i + window_size]):\n",
    "                    assert u < self.word_count\n",
    "                    assert v < self.word_count\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    self.word_pair_catch.append((u, v))\n",
    "        batch_pairs = []\n",
    "        for _ in range(batch_size):\n",
    "            batch_pairs.append(self.word_pair_catch.popleft())\n",
    "        return batch_pairs\n",
    "\n",
    "    # @profile\n",
    "    def get_neg_v_neg_sampling(self, pos_word_pair, count):\n",
    "        neg_v = numpy.random.choice(\n",
    "            self.sample_table, size=(len(pos_word_pair), count)).tolist()\n",
    "        return neg_v\n",
    "\n",
    "    def evaluate_pair_count(self, window_size):\n",
    "        return self.sentence_length * (2 * window_size - 1) - (\n",
    "            self.sentence_count - 1) * (1 + window_size) * window_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "??deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     22
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://adoni.github.io/2017/11/08/word2vec-pytorch/\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    \"\"\"Skip gram model of word2vec.\n",
    "    Attributes:\n",
    "        emb_size: Embedding size.\n",
    "        emb_dimention: Embedding dimention, typically from 50 to 500.\n",
    "        u_embedding: Embedding for center word.\n",
    "        v_embedding: Embedding for neibor words.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)\n",
    "        self.init_emb()\n",
    "        \n",
    "    def init_emb(self):\n",
    "        \"\"\"Initialize embedding weight like word2vec.\n",
    "        The u_embedding is a uniform distribution in [-0.5/em_size, 0.5/emb_size], and the elements of v_embedding are zeroes.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)        \n",
    "        \n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "        \"\"\"Forward process.\n",
    "        As pytorch designed, all variables must be batch format, so all input of this method is a list of word id.\n",
    "        Args:\n",
    "            pos_u: list of center word ids for positive word pairs.\n",
    "            pos_v: list of neibor word ids for positive word pairs.\n",
    "            neg_u: list of center word ids for negative word pairs.   该参数没有用\n",
    "            neg_v: list of neibor word ids for negative word pairs.   \n",
    "        Returns:\n",
    "            Loss of this process, a pytorch variable.\n",
    "        \"\"\"\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        emb_v = self.v_embeddings(pos_v)\n",
    "        score = torch.mul(emb_u, emb_v).squeeze()\n",
    "        score = torch.sum(score, dim=1)\n",
    "        score = F.logsigmoid(score)\n",
    "        neg_emb_v = self.v_embeddings(neg_v)\n",
    "        neg_score = torch.bmm(neg_emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        neg_score = F.logsigmoid(-1 * neg_score)\n",
    "        return -1 * (torch.sum(score)+torch.sum(neg_score))    # ??? 为什么要变成负数\n",
    "    \n",
    "    def save_embedding(self, id2word, file_name, use_cuda=False):\n",
    "        \"\"\"Save all embeddings to file.\n",
    "        As this class only record word id, so the map from id to word has to be transfered from outside.\n",
    "        Args:\n",
    "            id2word: map from word id to word.\n",
    "            file_name: file name.\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        if use_cuda:\n",
    "            embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
    "        else:\n",
    "            embedding = self.u_embeddings.weight.data.numpy()\n",
    "        fout = open(file_name, 'w')\n",
    "        fout.write('%d %d\\n' % (len(id2word), self.emb_dimension))\n",
    "        for wid, w in id2word.items():\n",
    "            e = embedding[wid]\n",
    "            e = ' '.join(map(lambda x: str(x), e))\n",
    "            fout.write('%s %s\\n' % (w, e))    \n",
    "\n",
    "def test():\n",
    "    model = SkipGramModel(100, 100)\n",
    "    id2word = dict()\n",
    "    for i in range(5):\n",
    "        id2word[i] = str(i)\n",
    "    model.save_embedding(id2word,'/home/hecong/word.txt')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4399, -0.7140])\n",
      "tensor([0.3918, 0.3287])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Sigmoid()\n",
    "input = torch.randn(2)\n",
    "print(input)\n",
    "output = m(input)\n",
    "print(output)\n",
    "# LogSimoid =  log(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self,\n",
    "                 input_file_name,\n",
    "                 output_file_name,\n",
    "                 emb_dimension=100,\n",
    "                 batch_size=50,\n",
    "                 window_size=5,\n",
    "                 iteration=1,\n",
    "                 initial_lr=0.025,\n",
    "                 min_count=5):\n",
    "        \"\"\"Initilize class parameters.\n",
    "        Args:\n",
    "            input_file_name: Name of a text data from file. Each line is a sentence splited with space.\n",
    "            output_file_name: Name of the final embedding file.\n",
    "            emb_dimention: Embedding dimention, typically from 50 to 500.\n",
    "            batch_size: The count of word pairs for one forward.\n",
    "            window_size: Max skip length between words.\n",
    "            iteration: Control the multiple training iterations.\n",
    "            initial_lr: Initial learning rate.\n",
    "            min_count: The minimal word frequency, words with lower frequency will be filtered.\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        self.data = InputData(input_file_name, min_count)\n",
    "        self.output_file_name = output_file_name\n",
    "        self.emb_size = len(self.data.word2id)\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.iteration = iteration\n",
    "        self.initial_lr = initial_lr\n",
    "        self.skip_gram_model = SkipGramModel(self.emb_size, self.emb_dimension)\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.skip_gram_model.cuda()\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.skip_gram_model.parameters(), lr=self.initial_lr)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Multiple training.\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        pair_count = self.data.evaluate_pair_count(self.window_size)\n",
    "        batch_count = self.iteration * pair_count / self.batch_size\n",
    "        process_bar = tqdm(range(int(batch_count)))\n",
    "        # self.skip_gram_model.save_embedding(\n",
    "        #     self.data.id2word, 'begin_embedding.txt', self.use_cuda)\n",
    "        for i in process_bar:\n",
    "            pos_pairs = self.data.get_batch_pairs(self.batch_size,\n",
    "                                                  self.window_size)\n",
    "            neg_v = self.data.get_neg_v_neg_sampling(pos_pairs, 5)\n",
    "            pos_u = [pair[0] for pair in pos_pairs]\n",
    "            pos_v = [pair[1] for pair in pos_pairs]\n",
    "\n",
    "            pos_u = Variable(torch.LongTensor(pos_u))\n",
    "            pos_v = Variable(torch.LongTensor(pos_v))\n",
    "            neg_v = Variable(torch.LongTensor(neg_v))\n",
    "            if self.use_cuda:\n",
    "                pos_u = pos_u.cuda()\n",
    "                pos_v = pos_v.cuda()\n",
    "                neg_v = neg_v.cuda()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.skip_gram_model.forward(pos_u, pos_v, neg_v)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            process_bar.set_description(\"Loss: %0.8f, lr: %0.6f\" %\n",
    "                                        (loss.data[0],\n",
    "                                         self.optimizer.param_groups[0]['lr']))\n",
    "            if i * self.batch_size % 100000 == 0:\n",
    "                lr = self.initial_lr * (1.0 - 1.0 * i / batch_count)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "        self.skip_gram_model.save_embedding(\n",
    "            self.data.id2word, self.output_file_name, self.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
