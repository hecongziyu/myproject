{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 深度语义匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.cnblogs.com/qniguoym/p/7772561.html\n",
    "    \n",
    "    https://blog.csdn.net/xiayto/article/details/81247461\n",
    "    \n",
    "    https://blog.csdn.net/mpk_no1/article/details/72836024  深度学习笔记——基于Word2vec和Doc2vec的句子对匹配方法    \n",
    "    \n",
    "    https://www.sohu.com/a/149089880_465975\n",
    "    \n",
    "    https://blog.csdn.net/yangliuy/article/details/78768643 深度文本匹配开源工具\n",
    "    \n",
    "    https://blog.csdn.net/hlang8160/article/details/78775435 用深度学习解决问答（QA）方法_语义匹配\n",
    "    \n",
    "    https://blog.csdn.net/melon0014/article/details/82466595  深度文本匹配总结\n",
    "    \n",
    "    深度文本匹配模型主要分为“基于单语义文档表达的深度学习模型”和“基于多语义文档表达的深度学习模型”两种。\n",
    "    \n",
    "    1、基于单语义文档表达的深度学习模型\n",
    "    \n",
    "        该类模型的特点是将两份待匹配的文档分别映射成两个向量，再将两个向量通过神经网络，最终输出一个结果，得出是否匹配的结论。典型的结构如下图所示： \n",
    "\n",
    "![Image of Yaktocat](https://img-blog.csdn.net/20180906191228136?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21lbG9uMDAxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "\n",
    "        该类别中经典的模型有：DSSM、CDSSM、ARC-I。\n",
    "    \n",
    "    1.1 经典模型\n",
    "    \n",
    "    首先介绍使用基础神经网络搭建的文本匹配模型。下图是keras中该模型的网络图，我们可以看到，网络分别为： \n",
    "      1）Embedding将句子变成300X25尺寸的张量（其中300是词向量维度） \n",
    "      2）通过激活函数是RELU的TimeDistributed(Dense)层过滤掉负数 \n",
    "      3）Lamda层是Maxpooling的作用 \n",
    "      4）将两个句子的向量拼接 \n",
    "      5、6）带dropout的全连接层 \n",
    "      7）BN层 \n",
    "      8）全连接层\n",
    "\n",
    "![Image of Yaktocat](https://img-blog.csdn.net/20180908104424827?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21lbG9uMDAxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)      \n",
    "      \n",
    "    1.2  ARC-I\n",
    "    \n",
    "    ARC-I的特点是在基础神经网络上将句子向量化的方法改为卷积操作。下图是keras中该模型的网络图，我们可以看到，网络分别为： \n",
    "      1）Embedding将句子变成300X20尺寸的张量（其中300是词向量维度） \n",
    "      2）一维卷积层 \n",
    "      3）Maxpooling \n",
    "      4、5）拼接向量 \n",
    "      \n",
    "![Image of Yaktocat](https://img-blog.csdn.net/20180908103338869?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21lbG9uMDAxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)   \n",
    "\n",
    "\n",
    "    2. 基于多语义文档表达的深度学习模型\n",
    "    \n",
    "    该类模型的特点是将两份待匹配的文档通过神经网络以词、短语、句子等不同粒度表达，并交叉等到相似度矩阵，再将矩阵输入神经网络，得到是否匹配的结论。这一类模型使句子表示更丰富，可以描述更加抽象的内容，在复述问题等任务上效果很好。 \n",
    "    \n",
    "![Image of Yaktocat](https://img-blog.csdn.net/20180906191405673?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21lbG9uMDAxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)   \n",
    "\n",
    "    上图很好地描述了多语义模型的特点，区别于单语义模型，该类模型会在两个带匹配向量组合之前先进行交叉融合，产生更大的信息量。\n",
    "    \n",
    "    2.1 MV-LSTM\n",
    "    \n",
    "    如下图所示。首先将两句话分别使用双向LSTM拼接起来，然后组成矩阵，矩阵的每个元素都是两句话中各自的词的LSTM向量运算结果（如乘积），将矩阵kmaxpooling之后再经过神经网络得到匹配结论。\n",
    "\n",
    "![Image of Yaktocat](https://img-blog.csdn.net/20180906191913712?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21lbG9uMDAxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)   \n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 深度学习解决大规模文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://blog.csdn.net/starzhou/article/details/70602328\n",
    "    \n",
    "    \n",
    "    https://blog.csdn.net/John_xyz/article/details/79602506\n",
    "    \n",
    "    https://blog.csdn.net/github_36326955/article/details/54891204\n",
    "    \n",
    "    https://blog.csdn.net/baidu_15113429/article/details/70670292\n",
    "    \n",
    "    https://blog.csdn.net/yangliuy/article/details/78768643 深度文本匹配开源工具（MatchZoo)\n",
    "    \n",
    "    https://blog.csdn.net/liuchonge/article/details/72614524 文本分类实战--从TFIDF到深度学习\n",
    "    \n",
    "    https://www.cnblogs.com/sxron/articles/5285770.html 使用深度RNN模型构建语义搜索引擎\n",
    "    \n",
    "    https://cs.stanford.edu/~quocle/paragraph_vector.pdf\n",
    "    \n",
    "    https://www.cnblogs.com/bamtercelboo/p/7181899.html 基于pytorch实现word2vec\n",
    "    \n",
    "    https://blog.csdn.net/u010626747/article/details/54095733 Python训练Word2Vec和Doc2Vec\n",
    "    \n",
    "    https://skymind.ai/wiki/word2vec  word2vec\n",
    "    \n",
    "    https://blog.csdn.net/itplus/article/details/37969519 word2vec 中的数学原理详解 一\n",
    "    \n",
    "    https://blog.csdn.net/itplus/article/details/37969817 word2vec 中的数学原理详解 三\n",
    "    \n",
    "    https://en.wikipedia.org/wiki/Word2vec\n",
    "    \n",
    "    https://www.cnblogs.com/pinard/p/7243513.html word2vec原理(二) 基于Hierarchical Softmax的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    https://baijiahao.baidu.com/s?id=1569634140033905&wfr=spider&for=pc 人工智能之妙用word2vec\n",
    "    https://www.cnblogs.com/juefan/p/3386991.html Word2Vec在中文的应用\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
