{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://mayhewsw.github.io/2019/01/16/can-bert-generate-text/ \n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/  transformer \n",
    "\n",
    "笔记：\n",
    "一、seq2seq encoder-decoder\n",
    "1、encoder model:\n",
    "   embed(x) --> lstm(x) return output, h\n",
    "2、decoder model:\n",
    "   embed(x) --> lstm(x, encode_h) return output ,h --> lin(output,num_words) --> argmax_softmax(output)\n",
    "3、train:\n",
    "   eng sentence -->  encoder([SOS_TOKEN] + sentence + [EOS_TOKEN])  -->\n",
    "   d_sentence =  [SOS_TOKEN] -->frg sentence = decoder(d_sen, encode_h) + ... + decoder(d_sen, h)  \n",
    "   \n",
    "二、seq2seq attention encoder-decoder\n",
    "1、encoder model\n",
    "2、attention decoder model:\n",
    "    embed(x), hidden, encode_outputs --> atten_weight = softmax(liner(x + hidden, max_len))  \n",
    "    --> atten_applied = torch.bmm(atten_weight, encode_outputs)\n",
    "    --> output, hidden = lstm(liner(2*hidden_size, hidden_size)  --> log_softmax(output) --> return output,hidden,atten_weight\n",
    "    \n",
    "    \n",
    "三、Transformer, self-attention\n",
    "    https://jalammar.github.io/illustrated-transformer/\n",
    "    https://arxiv.org/pdf/1706.03762.pdf\n",
    "    The transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
