{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch 自动求导说明 需要补充\n",
    "\n",
    "out = $  \\frac{1}{4} \\sum_{i}(Z_j) $\n",
    "$$ Z_j =  (W*X + 2)^2 *3 $$  $$ \\frac{d_z}{d_w} = $$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000016A7E57E908>\n",
      "tensor(27.)\n",
      "tensor([[ 4.5000,  4.5000],\n",
      "        [ 4.5000,  4.5000]])\n",
      "tensor([[ 0.5500,  0.5500],\n",
      "        [ 0.5500,  0.5500]])\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.ones(2,2),requires_grad=True)\n",
    "w = Variable(torch.randint(1, 2, (2,)),requires_grad=True)\n",
    "\n",
    "y = w*x + 2\n",
    "# print(x.creator)      # None,用户直接创建没有creater属性\n",
    "print(y.grad_fn)      # <torch.autograd._functions.basic_ops.AddConstant object at 0x7fb9b4d4b208>\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(out)\n",
    "out.backward()\n",
    "# 梯度更新\n",
    "print(x.grad.data)\n",
    "d_p = x.grad.data\n",
    "x.data.add_(-0.1,d_p) # 1 + (-0.1) * 4.5 = 0.55  根据导数及学习率，更新数据\n",
    "print(x.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch 损失函数学习\n",
    "1、cross_entropy : This criterion combines log_softmax and nll_loss in a single function. 可用于分类问题, torch.nn.CrossEntropyLoss，需\n",
    "   再仔细看一下其推导过程\n",
    "2、nll_loss :  The negative log likelihood loss. 负对数似然损失函数, 主要用于分类问题 https://www.bilibili.com/video/av15474329/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0405,  0.2065, -0.1137,  0.3112, -1.5401],\n",
      "        [-0.8510, -0.2648, -0.7867,  0.4211,  1.3341],\n",
      "        [ 1.4545,  0.8318,  0.2347,  1.6478,  0.9291]]) tensor([ 1,  0,  0])\n",
      "********************\n",
      "tensor([[ 0.1445, -0.2706,  0.0455,  0.0697,  0.0109],\n",
      "        [-0.3129,  0.0367,  0.0218,  0.0729,  0.1816],\n",
      "        [-0.2417,  0.0492,  0.0271,  0.1112,  0.0542]])\n",
      "Object `torch.nn.nll_loss` not found.\n"
     ]
    }
   ],
   "source": [
    "# cross entropy example\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randint(5, (3,), dtype=torch.int64)\n",
    "loss = F.cross_entropy(input, target)\n",
    "loss.backward()\n",
    "print(input,target)\n",
    "print(\"*\"*20)\n",
    "print(input.grad.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.optim是一个实现了各种优化算法的库。大部分常用的方法得到支持，并且接口具备足够的通用性，使得未来能够集成更加复杂的方法。\n",
    "Note: You can still pass options as keyword arguments. They will be used as defaults, in the groups that didn’t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups.\n",
    "For example, this is very useful when one wants to specify per-layer learning rates:\n",
    "\n",
    "optim.SGD([\n",
    "                {'params': model.base.parameters()},\n",
    "                {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "            ], lr=1e-2, momentum=0.9)\n",
    "\n",
    "This means that model.base’s parameters will use the default learning rate of 1e-2, model.classifier’s parameters will use a learning rate of 1e-3, and a momentum of 0.9 will be used for all parameters\n",
    "\n",
    "\n",
    "\n",
    "1、torch.optim.SGD 实现随机梯度下降算法, 参数 momentum 表势能  weight decay (L2 penalty)  https://byjiang.com/2017/09/10/A_Brief_Of_Optimization_Algorithms/ ； https://www.cnblogs.com/qniguoym/p/8058186.html. \n",
    "\n",
    "https://discuss.pytorch.org/t/how-does-pytorch-handle-the-mini-batch-training/9707/5  How does pytorch handle the mini-batch training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.optim.SGD  实现随机梯度下降算法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
