{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "\n",
    "# OpenCV Text Detection (EAST text detector)   https://www.pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.cnblogs.com/skyfsm/p/6806246.html 基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN\n",
    "    \n",
    "    https://blog.csdn.net/u013293750/article/details/64904681 CNN+LSTM深度学习文字检测\n",
    "    \n",
    "    https://blog.csdn.net/forest_world/article/details/78566737 主流ocr算法：CNN+BLSTM+CTC架构\n",
    "    \n",
    "    https://blog.csdn.net/slade_ruan/article/details/78301842?utm_source=blogxgwz1 场景文本检测，CTPN tensorflow版本\n",
    "    \n",
    "    https://blog.csdn.net/Quincuntial/article/details/79475339?utm_source=blogxgwz1 CTPN论文翻译——中英文对照\n",
    "    \n",
    "    http://lib.csdn.net/article/deeplearning/61632  通过代码理解faster-RCNN中的RPN\n",
    "    \n",
    "    https://slade-ruan.me/2017/10/22/text-detection-ctpn/  论文阅读与实现--CTPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.cnblogs.com/freeweb/p/6548208.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://deepsense.ai/region-of-interest-pooling-in-tensorflow-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    https://www.cnblogs.com/king-lps/p/9031568.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 生成训练、测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voc_2007_train\n",
      "voc_2007_val\n",
      "voc_2007_trainval\n",
      "voc_2007_test\n",
      "<bound method imdb.default_roidb of <lib.datasets.pascal_voc.pascal_voc object at 0x000001B922888550>>\n",
      "voc_2007_trainval gt roidb loaded from D:\\PROJECT_TW\\git\\data\\voc_2007_trainval_gt_roidb.pkl\n"
     ]
    }
   ],
   "source": [
    "from lib.datasets.factory import get_imdb\n",
    "from lib.datasets.pascal_voc import pascal_voc\n",
    "from lib.roi_data_layer.roidb import prepare_roidb\n",
    "from lib.roi_data_layer.layer import RoIDataLayer\n",
    "\n",
    "\n",
    "imdb = pascal_voc('trainval', '2007')\n",
    "# roidb ROI框的坐标位置信息, 信息来源于Annotations目录下对图片的XML定义\n",
    "prepare_roidb(imdb)   #  为方便训练，在原roidb信息基础上增加象image等等信息\n",
    "roidb = imdb.roidb \n",
    "data_layer = RoIDataLayer(roidb, imdb.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\3.jpg\n"
     ]
    }
   ],
   "source": [
    "blobs = data_layer.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     2,
     28
    ]
   },
   "outputs": [],
   "source": [
    "RPN_CHANNELS = 512\n",
    "TRUNCATED = False\n",
    "class DataMul(nn.Module):\n",
    "    def __init__(self, in_size, out_dim, bias= True):\n",
    "        super(DataMul, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_dim = out_dim\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(in_size, out_dim))\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.Tensor(out_dim))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        out = input.matmul(self.weight) + self.bias\n",
    "        return out\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return 'in_size={}, out_dim={}, bias={}'.format(\n",
    "            self.in_size, self.out_dim, self.bias)   \n",
    "    \n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self._predictions = {}\n",
    "        self._losses = {}\n",
    "        self._anchor_targets = {}\n",
    "        self._proposal_targets = {}\n",
    "        self._layers = {}\n",
    "        self._gt_image = None\n",
    "        self._act_summaries = {}\n",
    "        self._score_summaries = {}\n",
    "        self._event_summaries = {}\n",
    "        self._image_gt_summaries = {}\n",
    "        self._variables_to_fix = {}\n",
    "\n",
    "    def create_architecture(self, num_classes, tag=None,anchor_scales=(16,), anchor_ratios=(0.5, 1, 2)):\n",
    "        self._tag = tag\n",
    "        self._num_classes = num_classes\n",
    "        self._anchor_scales = anchor_scales\n",
    "        self._num_scales = len(anchor_scales)\n",
    "        self._anchor_ratios = anchor_ratios\n",
    "        self._num_ratios = len(anchor_ratios)\n",
    "        self._num_anchors = 10\n",
    "        assert tag != None\n",
    "        # Initialize layers\n",
    "        self._init_modules()\n",
    "        \n",
    "    def _init_modules(self):\n",
    "        self._init_head_tail()\n",
    "        # rpn\n",
    "        self.rpn_net = nn.Conv2d(self._net_conv_channels, RPN_CHANNELS, [3, 3], padding=1)\n",
    "        self.rpn_bi_net = nn.LSTM(RPN_CHANNELS, 256, batch_first=True, bidirectional=True)\n",
    "#         self.rpn_cls_score_net = nn.LSTM(RPN_CHANNELS, self._num_anchors * 2, batch_first=True, bidirectional=False)\n",
    "#         self.rpn_bbox_pred_net = nn.LSTM(RPN_CHANNELS, self._num_anchors * 4, batch_first=True, bidirectional=False)\n",
    "        self.rpn_cls_score_net = DataMul(RPN_CHANNELS, self._num_anchors * 2)\n",
    "        self.rpn_bbox_pred_net = DataMul(RPN_CHANNELS, self._num_anchors * 4)\n",
    "        self.init_weights()    \n",
    "        \n",
    "    # 对构建的网络参数（weight, bias）进行正则、初始化\n",
    "    def init_weights(self):\n",
    "        def normal_init(m, mean, stddev, truncated=False):\n",
    "            \"\"\"\n",
    "                weight initalizer: truncated normal and random normal.\n",
    "            \"\"\"\n",
    "            # x is a parameter\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                init.xavier_normal_(m.all_weights[0][0])\n",
    "                init.xavier_normal_(m.all_weights[0][1])\n",
    "                if len(m.all_weights) == 2:   # 双向  LSTM\n",
    "                    init.xavier_normal_(m.all_weights[1][0])\n",
    "                    init.xavier_normal_(m.all_weights[1][1])\n",
    "            else:\n",
    "                if truncated:\n",
    "                    m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n",
    "                else:\n",
    "                    m.weight.data.normal_(mean, stddev)\n",
    "                m.bias.data.zero_()\n",
    "        normal_init(self.rpn_net, 0, 0.01, TRUNCATED)\n",
    "#         normal_init(self.rpn_cls_score_net,0, 0.01, TRUNCATED)\n",
    "#         normal_init(self.rpn_bbox_pred_net,0, 0.01, TRUNCATED)\n",
    "        normal_init(self.rpn_bi_net,0, 0.01, TRUNCATED)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class vgg16(Network):\n",
    "    def __init__(self):\n",
    "        Network.__init__(self)\n",
    "        self._feat_stride = [16, ]\n",
    "        self._feat_compress = [1. / float(self._feat_stride[0]), ]\n",
    "        self._net_conv_channels = 512\n",
    "        self._fc7_channels = 4096\n",
    "\n",
    "    def _init_head_tail(self):\n",
    "        # 注意， 通过 models.vgg16() 加载的模型是基础模型，是还没有经过训练的模型， 所以需要load_pretrained_cnn从外部载入已训练好的权重信息\n",
    "        # 而通过 models.vgg16(pretrained=True)，则是已训练好的模型，无需再加载模型，本次实现采用models.vgg16(pretrained=True)，无需再加载了\n",
    "        # 注意预加载的是识别图像的（对于识字的需做更改）\n",
    "        self.vgg = models.vgg16_bn(pretrained=True)\n",
    "        # Remove fc8\n",
    "        self.vgg.classifier = nn.Sequential(*list(self.vgg.classifier._modules.values())[:-1])\n",
    "\n",
    "        # Fix the layers before conv3:\n",
    "        for layer in range(12):\n",
    "            for p in self.vgg.features[layer].parameters(): \n",
    "                p.requires_grad = False\n",
    "\n",
    "        # not using the last maxpool layer\n",
    "        self._layers['head'] = nn.Sequential(*list(self.vgg.features._modules.values())[:-1])\n",
    "#         print(self._layers['head'])\n",
    "\n",
    "\n",
    "    # 通过卷积网络VG16的feature层，抽取图片的特征\n",
    "    def _image_to_head(self):\n",
    "        net_conv = self._layers['head'](self._image)\n",
    "        self._act_summaries['conv'] = net_conv\n",
    "        return net_conv\n",
    "\n",
    "    def _head_to_tail(self, pool5):\n",
    "        pool5_flat = pool5.view(pool5.size(0), -1)\n",
    "        fc7 = self.vgg.classifier(pool5_flat)\n",
    "        return fc7\n",
    "\n",
    "\n",
    "    # 注意， 通过 models.vgg16() 加载的模型是基础模型，是还没有经过训练的模型， 所以需要该方法从外部载入权重信息\n",
    "    # 而通过 models.vgg16(pretrained=True)，则是已训练好的模型，无需再加载模型，本次实现采用models.vgg16(pretrained=True)，\n",
    "    # 无需再加载了\n",
    "    def load_pretrained_cnn(self, state_dict):\n",
    "        self.vgg.load_state_dict({k:v for k,v in state_dict.items() if k in self.vgg.state_dict()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     5,
     22,
     24,
     48,
     63
    ]
   },
   "outputs": [],
   "source": [
    "# https://blog.csdn.net/garfielder007/article/details/51378296  Python numpy函数hstack() vstack() stack() dstack() vsplit() concatenate()\n",
    "from lib.layutils.generate_anchors import generate_anchors\n",
    "import lib.layutils.anchor_target_layer as atl\n",
    "import numpy as np\n",
    "DEBUG_IN = False\n",
    "def generate_anchors_pre(height, width, feat_stride, anchor_scales=(8,16,32), anchor_ratios=(0.5,1,2)):\n",
    "    \"\"\" A wrapper function to generate anchors given different scales\n",
    "    Also return the number of anchors in variable 'length'\n",
    "    \"\"\"\n",
    "    anchors = generate_anchors(ratios=np.array(anchor_ratios), scales=np.array(anchor_scales))\n",
    "    A = anchors.shape[0]\n",
    "    shift_x = np.arange(0, width) * feat_stride\n",
    "    shift_y = np.arange(0, height) * feat_stride\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()\n",
    "    K = shifts.shape[0]\n",
    "    # width changes faster, so here it is H, W, C\n",
    "    anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\n",
    "    length = np.int32(anchors.shape[0])\n",
    "    return anchors, length\n",
    "\n",
    "def _anchor_target_layer(rpn_cls_score, gt_boxes, im_info, feat_stride, anchors, num_anchors):\n",
    "#     print('_anchor_target_layer begin .... 开始 。。。。')\n",
    "    rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights = atl.anchor_target_layer(\n",
    "        rpn_cls_score.data,\n",
    "        gt_boxes.data.numpy(),\n",
    "        im_info,\n",
    "        feat_stride,\n",
    "        anchors.data.numpy(),\n",
    "        num_anchors)\n",
    "\n",
    "    rpn_labels = torch.from_numpy(rpn_labels).float() #.set_shape([1, 1, None, None])\n",
    "    rpn_bbox_targets = torch.from_numpy(rpn_bbox_targets).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "    rpn_bbox_inside_weights = torch.from_numpy(rpn_bbox_inside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "    rpn_bbox_outside_weights = torch.from_numpy(rpn_bbox_outside_weights).float() #.set_shape([1, None, None, self._num_anchors * 4])\n",
    "    rpn_labels = rpn_labels.long()\n",
    "#     self._anchor_targets['rpn_labels'] = rpn_labels\n",
    "#     self._anchor_targets['rpn_bbox_targets'] = rpn_bbox_targets\n",
    "#     self._anchor_targets['rpn_bbox_inside_weights'] = rpn_bbox_inside_weights\n",
    "#     self._anchor_targets['rpn_bbox_outside_weights'] = rpn_bbox_outside_weights\n",
    "#     for k in self._anchor_targets.keys():\n",
    "#         self._score_summaries[k] = self._anchor_targets[k]\n",
    "    \n",
    "\n",
    "    return rpn_labels,rpn_bbox_targets,rpn_bbox_inside_weights,rpn_bbox_outside_weights\n",
    "\n",
    "\n",
    "def _smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=1.0, dim=[1]):\n",
    "    sigma_2 = sigma ** 2\n",
    "    box_diff = bbox_pred - bbox_targets\n",
    "    in_box_diff = bbox_inside_weights * box_diff\n",
    "    abs_in_box_diff = torch.abs(in_box_diff)\n",
    "    smoothL1_sign = (abs_in_box_diff < 1. / sigma_2).detach().float()\n",
    "    in_loss_box = torch.pow(in_box_diff, 2) * (sigma_2 / 2.) * smoothL1_sign \\\n",
    "                  + (abs_in_box_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\n",
    "    out_loss_box = bbox_outside_weights * in_loss_box\n",
    "    loss_box = out_loss_box\n",
    "    for i in sorted(dim, reverse=True):\n",
    "        loss_box = loss_box.sum(i)\n",
    "    loss_box = loss_box.mean()\n",
    "    return loss_box\n",
    "\n",
    "def _add_loss(rpn_cls_score_reshape,rpn_labels,\n",
    "              rpn_bbox_pred,\n",
    "              rpn_bbox_targets,\n",
    "              rpn_bbox_inside_weights,\n",
    "              rpn_bbox_outside_weights,\n",
    "             sigma_rpn = 3.0):\n",
    "#     loss_fun = nn.MSELoss() \n",
    "    rpn_cls_score = rpn_cls_score_reshape.view(-1,2)\n",
    "#     print('rpn cls score --> \\n {}'.format(rpn_cls_score))\n",
    "#     rpn_cls_score = rpn_cls_score_reshape\n",
    "    rpn_label = rpn_labels.view(-1)\n",
    "    rpn_select = (rpn_label.data != -1).nonzero().view(-1)\n",
    "    rpn_cls_score = rpn_cls_score.index_select(0, rpn_select).contiguous().view(-1, 2)\n",
    "    rpn_label = rpn_label.index_select(0, rpn_select).contiguous().view(-1)\n",
    "#     rpn_label = rpn_label.unsqueeze(1)\n",
    "#     print('rpn labels --> \\n {}'.format(rpn_label))\n",
    "    if DEBUG_IN:\n",
    "        print('rpn cls score --> \\n {}'.format(rpn_cls_score))\n",
    "        print('rpn label --> \\n {}'.format(rpn_label))\n",
    "    rpn_cross_entropy = F.cross_entropy(rpn_cls_score, rpn_label)\n",
    "    \n",
    "    rpn_loss_box = _smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights,\n",
    "                                          rpn_bbox_outside_weights, sigma=sigma_rpn, dim=[1, 2, 3])\n",
    "    \n",
    "    loss = rpn_cross_entropy + rpn_loss_box\n",
    "#     loss = rpn_cross_entropy\n",
    "    return loss,rpn_cross_entropy,rpn_loss_box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     37
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入模型\n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "0 total loss --> 0.2159 cls loss --> 0.1960 box loss --> 0.0200 \n",
      "save model \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\2.jpg\n",
      "1 total loss --> 0.4229 cls loss --> 0.3775 box loss --> 0.0455 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\7.jpg\n",
      "2 total loss --> 0.2874 cls loss --> 0.2612 box loss --> 0.0262 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "3 total loss --> 0.2105 cls loss --> 0.1905 box loss --> 0.0199 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\6.jpg\n",
      "4 total loss --> 0.3010 cls loss --> 0.2431 box loss --> 0.0579 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\2.jpg\n",
      "5 total loss --> 0.4938 cls loss --> 0.4444 box loss --> 0.0494 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "6 total loss --> 0.2412 cls loss --> 0.2213 box loss --> 0.0199 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\7.jpg\n",
      "7 total loss --> 0.3009 cls loss --> 0.2772 box loss --> 0.0237 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\2.jpg\n",
      "8 total loss --> 0.4722 cls loss --> 0.4287 box loss --> 0.0435 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\6.jpg\n",
      "9 total loss --> 0.2906 cls loss --> 0.2318 box loss --> 0.0588 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "10 total loss --> 0.2434 cls loss --> 0.2236 box loss --> 0.0198 \n",
      "save model \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\6.jpg\n",
      "11 total loss --> 0.2932 cls loss --> 0.2422 box loss --> 0.0510 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\2.jpg\n",
      "12 total loss --> 0.4829 cls loss --> 0.4297 box loss --> 0.0531 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "13 total loss --> 0.2259 cls loss --> 0.2061 box loss --> 0.0198 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\6.jpg\n",
      "14 total loss --> 0.2838 cls loss --> 0.2308 box loss --> 0.0530 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "15 total loss --> 0.2142 cls loss --> 0.1945 box loss --> 0.0197 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\7.jpg\n",
      "16 total loss --> 0.3072 cls loss --> 0.2806 box loss --> 0.0266 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\7.jpg\n",
      "17 total loss --> 0.2684 cls loss --> 0.2487 box loss --> 0.0197 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "18 total loss --> 0.2143 cls loss --> 0.1946 box loss --> 0.0197 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\2.jpg\n",
      "19 total loss --> 0.4273 cls loss --> 0.3882 box loss --> 0.0391 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\7.jpg\n",
      "20 total loss --> 0.2978 cls loss --> 0.2731 box loss --> 0.0247 \n",
      "save model \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\6.jpg\n",
      "21 total loss --> 0.3072 cls loss --> 0.2554 box loss --> 0.0518 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "22 total loss --> 0.2314 cls loss --> 0.2117 box loss --> 0.0197 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "23 total loss --> 0.2392 cls loss --> 0.2196 box loss --> 0.0197 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\7.jpg\n",
      "24 total loss --> 0.2846 cls loss --> 0.2583 box loss --> 0.0263 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\6.jpg\n",
      "25 total loss --> 0.3110 cls loss --> 0.2620 box loss --> 0.0490 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\2.jpg\n",
      "26 total loss --> 0.4699 cls loss --> 0.4249 box loss --> 0.0450 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "27 total loss --> 0.2197 cls loss --> 0.2001 box loss --> 0.0197 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\6.jpg\n",
      "28 total loss --> 0.2840 cls loss --> 0.2272 box loss --> 0.0568 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\7.jpg\n",
      "29 total loss --> 0.2822 cls loss --> 0.2605 box loss --> 0.0218 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "30 total loss --> 0.2402 cls loss --> 0.2205 box loss --> 0.0197 \n",
      "save model \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\2.jpg\n",
      "31 total loss --> 0.4352 cls loss --> 0.3956 box loss --> 0.0396 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "32 total loss --> 0.2369 cls loss --> 0.2172 box loss --> 0.0196 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\7.jpg\n",
      "33 total loss --> 0.3210 cls loss --> 0.3025 box loss --> 0.0185 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\2.jpg\n",
      "34 total loss --> 0.4240 cls loss --> 0.3787 box loss --> 0.0454 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n",
      "35 total loss --> 0.2088 cls loss --> 0.1892 box loss --> 0.0196 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\7.jpg\n",
      "36 total loss --> 0.3102 cls loss --> 0.2835 box loss --> 0.0267 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\6.jpg\n",
      "37 total loss --> 0.3072 cls loss --> 0.2531 box loss --> 0.0541 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\7.jpg\n",
      "38 total loss --> 0.2932 cls loss --> 0.2735 box loss --> 0.0197 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\2.jpg\n",
      "39 total loss --> 0.4460 cls loss --> 0.4001 box loss --> 0.0459 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\6.jpg\n",
      "40 total loss --> 0.2919 cls loss --> 0.2321 box loss --> 0.0598 \n",
      "save model \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\6.jpg\n",
      "41 total loss --> 0.3360 cls loss --> 0.2797 box loss --> 0.0563 \n",
      "train image name : num 0 : D:\\PROJECT_TW\\git\\data\\VOCdevkit2007\\VOC2007\\JPEGImages\\4.jpg\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "# feat_stride：[16]VGG中conv5_3相比于输入图像缩小了16倍，也就是相邻两个点之间的stride=16 \n",
    "feat_stride = [16, ]\n",
    "ANCHOR_SCALES = [16]\n",
    "ANCHOR_RATIOS = [0.5,1,2]\n",
    "MOMENTUM = 0.9\n",
    "lr = 0.00001\n",
    "DOUBLE_BIAS = True\n",
    "BIAS_DECAY = False\n",
    "WEIGHT_DECAY = 0.0001\n",
    "EPCHO = 5000\n",
    "num_anchors = 10\n",
    "RPN_CHANNELS = 512\n",
    "beta1=0.5\n",
    "\n",
    "net = vgg16()\n",
    "# print(imdb.num_classes)\n",
    "\n",
    "net.create_architecture(imdb.num_classes, tag='default',\n",
    "                                            anchor_scales=ANCHOR_SCALES,\n",
    "                                            anchor_ratios=ANCHOR_RATIOS)\n",
    "params = []\n",
    "\n",
    "for key, value in dict(net.named_parameters()).items():\n",
    "    if value.requires_grad:\n",
    "        if 'bias' in key:\n",
    "              params += [{'params':[value],'lr':lr*(DOUBLE_BIAS + 1), \n",
    "                  'weight_decay': BIAS_DECAY and WEIGHT_DECAY or 0}]\n",
    "        else:\n",
    "              params += [{'params':[value],'lr':lr, \n",
    "                  'weight_decay': WEIGHT_DECAY}]\n",
    "                \n",
    "if os.path.exists('D:\\\\PROJECT_TW\\\\git\\\\data\\\\example\\\\model\\\\text_detect_new.pkl'):\n",
    "    print('载入模型')\n",
    "    net.load_state_dict(torch.load('D:\\\\PROJECT_TW\\\\git\\\\data\\\\example\\\\model\\\\text_detect_new.pkl'))\n",
    "                \n",
    "\n",
    "optimizer = torch.optim.SGD(params,lr=lr, momentum=MOMENTUM)\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     params, lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "for step in range(EPCHO):\n",
    "    blobs = data_layer.forward()\n",
    "    image = torch.from_numpy(blobs['data'].transpose([0,3,1,2]))\n",
    "    im_info = blobs['im_info']\n",
    "    gt_boxes = torch.from_numpy(blobs['gt_boxes'])\n",
    "    net_conv = net._layers['head'](image)\n",
    "    anchors, length = generate_anchors_pre(net_conv.size(2), net_conv.size(3),feat_stride=feat_stride,anchor_scales=(16,))\n",
    "    \n",
    "    anchors = torch.from_numpy(anchors)\n",
    "    rpn = F.relu(net.rpn_net(net_conv))  # ( N , C, H, W）\n",
    "    # ( N , C, H, W）  --》 （N * H, W, C)\n",
    "    rpn_reshape = rpn.permute(0,2,3,1).squeeze(0)\n",
    "    \n",
    "    # 双向LSTM网络   -->  (N*H, W, C)\n",
    "    rpn_blstm,_ = net.rpn_bi_net(rpn_reshape)\n",
    "    rpn_blstm = F.relu(rpn_blstm)  # 注意另外可以考虑采用batch normal方法对数据进行整理\n",
    "    \n",
    "    # test detect 采用随机生成 偏移变量数组和得分初始化数组  \n",
    "    # 与rpn_blsm[N*H*W,C]矩阵相乘方式得到其偏移和分类得分 [N,H,W,4*num anchor或2]\n",
    "    # 这里暂时用lstm 来代替，后面需改成上述方案实现\n",
    "    rpn_blstm_reshape = rpn_blstm.view(-1, RPN_CHANNELS)\n",
    "    rpn_cls_score = net.rpn_cls_score_net(rpn_blstm)   # [W H, num_anchors*2], num_anchors = 10\n",
    "    rpn_cls_score = rpn_cls_score.view(rpn_blstm.size()[0], rpn_blstm.size()[1],-1)\n",
    "#     print('rpn cls score size {}'.format(rpn_cls_score.size()))\n",
    "\n",
    "    rpn_cls_score = rpn_cls_score.permute(2,0,1).unsqueeze(0)\n",
    "    rpn_cls_score = rpn_cls_score.permute(0,2,3,1)\n",
    "    # N , H, W, 2*10   --->  N, H ,W * 10, 2\n",
    "    rpn_cls_score_reshape = rpn_cls_score.contiguous().view(1,rpn_cls_score.size()[1],-1,2)  \n",
    "\n",
    "    if DEBUG:\n",
    "        print('rpn cls score shape --> {}'.format(rpn_cls_score.size()))\n",
    "        print('rpn cls score reshape shape --> {}'.format(rpn_cls_score_reshape.size()))\n",
    "    \n",
    "    \n",
    "    # 得到坐标点的10个分类概率（二分类方法)\n",
    "    rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape.view(-1,2),dim=1)           # N 2 H*NUM_ANCHORS W\n",
    "    if DEBUG:\n",
    "        print('rpn cls prob reshape --> \\n {}'.format(rpn_cls_prob_reshape))\n",
    "    \n",
    "    rpn_bbox_pred  = net.rpn_bbox_pred_net(rpn_blstm_reshape)   # [W H, num_anchors*4], num_anchors = 10\n",
    "    rpn_bbox_pred = rpn_bbox_pred.view(rpn_blstm.size()[0],rpn_blstm.size()[1],-1)\n",
    "    rpn_bbox_pred = rpn_bbox_pred.unsqueeze(0)\n",
    "\n",
    "        \n",
    "    rpn_labels,rpn_bbox_targets,rpn_bbox_inside_weights,rpn_bbox_outside_weights = _anchor_target_layer(rpn_cls_score,\n",
    "                                                                                                        gt_boxes,\n",
    "                                                                                                        im_info,\n",
    "                                                                                                        feat_stride,\n",
    "                                                                                                        anchors,\n",
    "                                                                                                        num_anchors)\n",
    "#     print('rpn bbox targets --> \\n {}'.format(rpn_bbox_targets))\n",
    "    if DEBUG:\n",
    "        print('rpn class score reshape size {}  --> \\n {}'.format(rpn_cls_score_reshape.size(),rpn_cls_score_reshape.view(-1,2)))\n",
    "        print('rpn_labels size {}'.format(rpn_labels.size()))\n",
    "        print('rpn labels {} --> \\n {}'.format(rpn_labels.size(), rpn_labels))\n",
    "        \n",
    "    loss,cls_loss,box_loss = _add_loss(rpn_cls_score_reshape,rpn_labels,rpn_bbox_pred,\n",
    "                     rpn_bbox_targets,rpn_bbox_inside_weights,rpn_bbox_outside_weights)\n",
    "    \n",
    "    print('{} total loss --> {:.4f} cls loss --> {:.4f} box loss --> {:.4f} '.format(step,loss,cls_loss,box_loss))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 10 == 0:\n",
    "        print('save model ')\n",
    "        torch.save(net.state_dict(), 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\example\\\\model\\\\text_detect_new.pkl')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  1,  0,  0,  1,\n",
      "         1,  0,  1,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,\n",
      "         0,  0,  0,  0,  0,  1,  1,  1,  0,  0,  1,  1,  1,  0,\n",
      "         1,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,\n",
      "         0,  1,  1,  1,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  1,  1,  1,  1,  1,  0,  0,  0,  0,  0,  1,  1,  1,\n",
      "         1,  1,  1,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  0,\n",
      "         1,  1,  1,  0,  1,  1,  1,  0,  0,  0,  0,  1,  1,  0,\n",
      "         1,  1,  1,  1,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,\n",
      "         0,  0,  1,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,\n",
      "         1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "         1,  1,  1,  0,  1,  1,  1,  0,  1,  1,  1,  1,  1,  1,\n",
      "         0,  1,  0,  0,  1,  1,  1,  0,  0,  1,  1,  1,  1,  0,\n",
      "         0,  0,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  0,  0,  1,\n",
      "         0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
      "         1,  0,  1,  0])\n",
      "tensor([[-1.4444,  1.4320],\n",
      "        [-1.4543,  1.4525],\n",
      "        [-1.5276,  1.6056],\n",
      "        [-1.3156,  1.2821],\n",
      "        [-1.3972,  1.4674],\n",
      "        [-1.5589,  1.6143],\n",
      "        [-1.5532,  1.5957],\n",
      "        [-1.5088,  1.5394],\n",
      "        [ 1.2752, -1.2732],\n",
      "        [ 1.2027, -1.2071],\n",
      "        [-1.5444,  1.5826],\n",
      "        [ 2.5430, -2.6551],\n",
      "        [ 2.3848, -2.4385],\n",
      "        [-1.5509,  1.5229],\n",
      "        [-1.2262,  1.1854],\n",
      "        [-1.2314,  1.1637],\n",
      "        [-1.4793,  1.4430],\n",
      "        [ 2.8236, -2.6980],\n",
      "        [-1.7577,  1.7594],\n",
      "        [-1.5934,  1.5847],\n",
      "        [-1.9576,  2.0169],\n",
      "        [-1.8243,  1.8733],\n",
      "        [-1.6972,  1.7344],\n",
      "        [-1.4720,  1.4575],\n",
      "        [-1.8268,  1.8623],\n",
      "        [-1.4030,  1.3909],\n",
      "        [-1.1697,  1.1321],\n",
      "        [ 0.6797, -0.6024],\n",
      "        [ 2.3548, -2.3748],\n",
      "        [ 2.7364, -2.6154],\n",
      "        [ 2.2861, -2.1690],\n",
      "        [ 2.6747, -2.7266],\n",
      "        [ 2.5421, -2.6574],\n",
      "        [-1.3372,  1.2938],\n",
      "        [-1.6603,  1.6291],\n",
      "        [-1.2931,  1.2960],\n",
      "        [ 0.6462, -0.5542],\n",
      "        [ 2.9239, -2.7929],\n",
      "        [-1.5600,  1.6040],\n",
      "        [-1.7992,  1.8370],\n",
      "        [-1.4703,  1.5083],\n",
      "        [ 1.0542, -1.0471],\n",
      "        [-1.4843,  1.5388],\n",
      "        [ 2.9316, -2.7951],\n",
      "        [ 2.7842, -2.7955],\n",
      "        [ 2.7825, -2.6555],\n",
      "        [ 2.6105, -2.5718],\n",
      "        [ 2.6946, -2.7010],\n",
      "        [ 2.7620, -2.7688],\n",
      "        [ 2.7384, -2.6061],\n",
      "        [-1.0249,  1.0657],\n",
      "        [-1.4929,  1.4556],\n",
      "        [ 1.7760, -1.7956],\n",
      "        [ 2.4800, -2.5976],\n",
      "        [ 2.5710, -2.5848],\n",
      "        [ 2.3181, -2.4211],\n",
      "        [ 2.2316, -2.2636],\n",
      "        [-1.2753,  1.2535],\n",
      "        [-1.4340,  1.4235],\n",
      "        [-1.1596,  1.1379],\n",
      "        [-1.2654,  1.2514],\n",
      "        [-1.3439,  1.3383],\n",
      "        [-1.4942,  1.4629],\n",
      "        [ 1.0865, -1.0576],\n",
      "        [-1.5364,  1.5204],\n",
      "        [-1.5188,  1.4903],\n",
      "        [-1.4708,  1.4519],\n",
      "        [-1.3555,  1.3315],\n",
      "        [-1.5582,  1.5302],\n",
      "        [-1.4020,  1.3884],\n",
      "        [-1.5468,  1.5068],\n",
      "        [-1.2894,  1.2916],\n",
      "        [-1.3533,  1.3397],\n",
      "        [-1.5231,  1.4890],\n",
      "        [ 0.9819, -1.0023],\n",
      "        [-1.1471,  1.1200],\n",
      "        [ 1.6845, -1.7923],\n",
      "        [ 2.4911, -2.3906],\n",
      "        [ 2.0303, -2.1208],\n",
      "        [ 2.4889, -2.5019],\n",
      "        [ 2.2417, -2.2862],\n",
      "        [ 2.2541, -2.1225],\n",
      "        [ 2.5518, -2.6598],\n",
      "        [ 1.9492, -1.9887],\n",
      "        [ 2.2877, -2.3351],\n",
      "        [-1.2489,  1.2291],\n",
      "        [-1.6257,  1.6096],\n",
      "        [-1.8005,  1.7930],\n",
      "        [-1.1831,  1.2188],\n",
      "        [-1.4084,  1.4099],\n",
      "        [ 2.8650, -2.9277],\n",
      "        [ 2.8377, -2.9045],\n",
      "        [ 2.2452, -2.2936],\n",
      "        [ 2.5556, -2.5661],\n",
      "        [ 2.5968, -2.7175],\n",
      "        [-1.4410,  1.3941],\n",
      "        [-1.7208,  1.6896],\n",
      "        [-1.3585,  1.3654],\n",
      "        [-1.4858,  1.4368],\n",
      "        [-0.7264,  0.6770],\n",
      "        [-1.0079,  0.9844],\n",
      "        [ 1.2168, -1.2369],\n",
      "        [ 2.3346, -2.3589],\n",
      "        [ 0.9131, -0.8812],\n",
      "        [-1.5349,  1.5717],\n",
      "        [-1.8042,  1.8284],\n",
      "        [-1.4322,  1.4793],\n",
      "        [-1.7710,  1.7769],\n",
      "        [-1.4555,  1.5037],\n",
      "        [-1.8186,  1.8325],\n",
      "        [-1.4918,  1.5385],\n",
      "        [ 1.1333, -1.1392],\n",
      "        [-1.7009,  1.6968],\n",
      "        [-1.7858,  1.8121],\n",
      "        [-1.4684,  1.4801],\n",
      "        [ 1.1629, -1.1518],\n",
      "        [-1.7725,  1.7905],\n",
      "        [-1.8029,  1.8296],\n",
      "        [-1.4299,  1.4518],\n",
      "        [ 1.1792, -1.2752],\n",
      "        [ 2.5260, -2.5459],\n",
      "        [ 2.4865, -2.4484],\n",
      "        [ 2.6349, -2.7484],\n",
      "        [-1.8402,  1.8334],\n",
      "        [-1.2394,  1.2055],\n",
      "        [-1.5434,  1.5184],\n",
      "        [-1.4450,  1.3974],\n",
      "        [-1.7049,  1.6847],\n",
      "        [-1.3783,  1.3752],\n",
      "        [-1.6260,  1.6232],\n",
      "        [ 2.3830, -2.2625],\n",
      "        [ 2.5135, -2.6320],\n",
      "        [ 2.2752, -2.3599],\n",
      "        [-1.3318,  1.3251],\n",
      "        [-1.7599,  1.7931],\n",
      "        [-1.3186,  1.3455],\n",
      "        [ 2.3899, -2.4404],\n",
      "        [ 2.2644, -2.2857],\n",
      "        [ 2.7388, -2.8703],\n",
      "        [ 2.7247, -2.8621],\n",
      "        [ 2.6555, -2.6203],\n",
      "        [ 2.8826, -2.8963],\n",
      "        [-1.1038,  1.1471],\n",
      "        [-1.2380,  1.2708],\n",
      "        [-1.2866,  1.3279],\n",
      "        [-1.7530,  1.7643],\n",
      "        [ 2.3636, -2.4650],\n",
      "        [ 2.9125, -2.9624],\n",
      "        [ 2.7189, -2.6809],\n",
      "        [ 2.9335, -2.8035],\n",
      "        [ 2.6968, -2.8141],\n",
      "        [ 2.5911, -2.6437],\n",
      "        [ 2.6668, -2.5513],\n",
      "        [ 2.7100, -2.6722],\n",
      "        [ 2.8333, -2.7050],\n",
      "        [ 2.3132, -2.3203],\n",
      "        [ 2.8060, -2.9589],\n",
      "        [ 2.6436, -2.7615],\n",
      "        [ 2.8064, -2.9585],\n",
      "        [ 2.5565, -2.6090],\n",
      "        [ 2.2415, -2.2212],\n",
      "        [ 2.7005, -2.8397],\n",
      "        [ 2.7044, -2.7462],\n",
      "        [-1.0503,  1.1447],\n",
      "        [-2.0054,  1.9992],\n",
      "        [-1.7953,  1.8097],\n",
      "        [-1.3277,  1.3515],\n",
      "        [-1.5389,  1.5301],\n",
      "        [-1.6516,  1.6541],\n",
      "        [-1.3770,  1.4159],\n",
      "        [ 1.0151, -0.9891],\n",
      "        [ 2.5259, -2.5657],\n",
      "        [ 2.2255, -2.2446],\n",
      "        [ 1.7504, -1.7959],\n",
      "        [ 1.9480, -1.9763],\n",
      "        [ 2.3510, -2.3555],\n",
      "        [ 2.3318, -2.3824],\n",
      "        [ 2.2464, -2.2500],\n",
      "        [ 1.8368, -1.9407],\n",
      "        [ 2.3020, -2.4071],\n",
      "        [-1.5223,  1.4847],\n",
      "        [-1.4418,  1.4083],\n",
      "        [-1.1057,  1.0990],\n",
      "        [-1.4780,  1.4427],\n",
      "        [-1.0720,  1.0640],\n",
      "        [ 1.3813, -1.3453],\n",
      "        [-1.1670,  1.1778],\n",
      "        [-1.4348,  1.3954],\n",
      "        [-1.2130,  1.1979],\n",
      "        [ 1.2855, -1.2706],\n",
      "        [-1.5481,  1.5166],\n",
      "        [-1.2632,  1.2466],\n",
      "        [-1.4058,  1.3716],\n",
      "        [-1.4617,  1.4343],\n",
      "        [-1.4810,  1.4545],\n",
      "        [-1.2025,  1.1921],\n",
      "        [ 2.5841, -2.5959],\n",
      "        [-1.2869,  1.3536],\n",
      "        [ 1.1967, -1.2063],\n",
      "        [-1.4148,  1.3872],\n",
      "        [-1.5359,  1.5855],\n",
      "        [-1.5059,  1.5550],\n",
      "        [-1.3667,  1.4111],\n",
      "        [ 0.9848, -0.9828],\n",
      "        [ 1.1128, -1.1042],\n",
      "        [-1.4300,  1.4794],\n",
      "        [-1.4712,  1.5169],\n",
      "        [-1.4669,  1.4972],\n",
      "        [-1.3841,  1.4343],\n",
      "        [ 1.0399, -1.0568],\n",
      "        [ 1.9773, -1.9925],\n",
      "        [ 1.9273, -1.8116],\n",
      "        [ 1.8055, -1.8251],\n",
      "        [ 2.2331, -2.1236],\n",
      "        [ 1.1414, -1.1249],\n",
      "        [-1.4179,  1.4109],\n",
      "        [-1.6649,  1.6162],\n",
      "        [-1.4044,  1.3901],\n",
      "        [ 2.6490, -2.6935],\n",
      "        [ 2.6161, -2.6289],\n",
      "        [ 2.4017, -2.5016],\n",
      "        [ 2.7255, -2.7376],\n",
      "        [ 2.8569, -2.7277],\n",
      "        [ 2.8881, -2.9352],\n",
      "        [ 2.4760, -2.3616],\n",
      "        [ 2.6465, -2.6695],\n",
      "        [ 2.5497, -2.5108],\n",
      "        [ 1.7974, -1.8204],\n",
      "        [ 2.7958, -2.6560],\n",
      "        [-1.0398,  0.9973],\n",
      "        [-1.3730,  1.3743],\n",
      "        [-1.6824,  1.6799],\n",
      "        [-1.5402,  1.5182],\n",
      "        [-1.4649,  1.4159],\n",
      "        [-1.7194,  1.6885],\n",
      "        [-1.3842,  1.3767],\n",
      "        [-1.2618,  1.3055],\n",
      "        [-1.8652,  1.8739],\n",
      "        [ 2.7310, -2.7449],\n",
      "        [ 2.6927, -2.6937],\n",
      "        [ 1.8093, -1.8330],\n",
      "        [-1.2718,  1.2661],\n",
      "        [-1.5245,  1.5156],\n",
      "        [ 1.1253, -1.1090],\n",
      "        [-0.8584,  0.8491],\n",
      "        [-0.0667,  0.0688],\n",
      "        [ 1.8944, -1.9772],\n",
      "        [ 1.8630, -1.9612],\n",
      "        [ 1.9591, -1.8515],\n",
      "        [ 1.6738, -1.5799],\n",
      "        [-1.4416,  1.3980],\n",
      "        [-1.7623,  1.7212],\n",
      "        [-1.5519,  1.5443],\n",
      "        [ 2.7097, -2.7565],\n",
      "        [-1.4697,  1.5144],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 1.7831, -1.6814]])\n"
     ]
    }
   ],
   "source": [
    "rpn_cls_score = rpn_cls_score_reshape.view(-1,2)\n",
    "rpn_label = rpn_labels.view(-1)\n",
    "rpn_select = (rpn_label.data != -1).nonzero().view(-1)\n",
    "rpn_cls_score = rpn_cls_score.index_select(0, rpn_select).contiguous().view(-1, 2)\n",
    "rpn_label = rpn_label.index_select(0, rpn_select).contiguous().view(-1)\n",
    "print(rpn_label)\n",
    "print(rpn_cls_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_anchor_target_layer begin .... 开始 。。。。\n",
      "torch.Size([1, 37, 54, 10])\n"
     ]
    }
   ],
   "source": [
    "# 重新加载某模块\n",
    "import lib.layutils.anchor_target_layer as atl\n",
    "import importlib\n",
    "importlib.reload(atl)\n",
    "\n",
    "import lib.utils.bbox as bbox\n",
    "importlib.reload(bbox)\n",
    "# print(rpn_cls_score.shape[1:3])\n",
    "rpn_labels = _anchor_target_layer(rpn_cls_score,\n",
    "                                  gt_boxes,\n",
    "                                  im_info,\n",
    "                                  feat_stride,\n",
    "                                  anchors,\n",
    "                                  num_anchors\n",
    "                                 )\n",
    "print(rpn_labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\example\\\\model\\\\text_detect.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     7,
     10
    ]
   },
   "outputs": [],
   "source": [
    "feat_stride = [16, ]\n",
    "ANCHOR_SCALES = [16]\n",
    "ANCHOR_RATIOS = [0.5,1,2]\n",
    "num_anchors = 10\n",
    "# model_path = '/home/hecong/temp/data/txtdect/text_detect_new.pkl'\n",
    "model_path = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\example\\\\model\\\\text_detect_new.pkl'\n",
    "net = vgg16()\n",
    "num_classes = 2\n",
    "net.create_architecture(num_classes, tag='default',\n",
    "                                            anchor_scales=ANCHOR_SCALES,\n",
    "                                            anchor_ratios=ANCHOR_RATIOS)    \n",
    "if os.path.exists(model_path):\n",
    "    net.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     14,
     20,
     24,
     58
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape --> torch.Size([1, 3, 469, 653])\n",
      "handle times 11.0070\n",
      "[[0.00000000e+00 2.66445435e+02 6.40000000e+02 2.66445435e+02\n",
      "  0.00000000e+00 3.15908936e+02 6.40000000e+02 3.15908936e+02\n",
      "  6.19373202e-01]]\n"
     ]
    }
   ],
   "source": [
    "import lib.text_connector.text_proposal_connector as tpc\n",
    "import lib.utils.nms as nms\n",
    "import torch\n",
    "import lib.layutils.proposal_layer as psl\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import importlib\n",
    "importlib.reload(tpc)\n",
    "importlib.reload(psl)\n",
    "import time\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "def connect_proposal(text_proposals, scores, im_size):\n",
    "    cp = tpc.TextProposalConnector()\n",
    "    line = cp.get_text_lines(text_proposals, scores, im_size)\n",
    "    return line\n",
    "\n",
    "\n",
    "def save_results(image_name, image, line, thresh):\n",
    "    im = image.copy()\n",
    "    inds = np.where(line[:, -1] >= thresh)[0]\n",
    "#     print('inds --->{}'.format(len(inds)))\n",
    "    if len(inds) == 0:\n",
    "        return\n",
    "    \n",
    "    for i in inds:\n",
    "        bbox = line[i, :].astype(np.int)\n",
    "#         print('bbox {}--> \\n {}'.format(i,bbox))\n",
    "        score = line[i, -1]\n",
    "        cv2.rectangle(\n",
    "            im, (bbox[0], bbox[1]), (bbox[6], bbox[7]),\n",
    "            color=(0, 255, 255),\n",
    "            thickness=2)\n",
    "    plt.imshow(im,'brg')\n",
    "    plt.show()\n",
    "    cv2.imwrite('d:\\\\1.jpg',im)\n",
    "    \n",
    "def testConnectProposal():\n",
    "    plt.rcParams['figure.figsize'] = 15, 10\n",
    "    img = cv2.imread(roidb[0]['image'])\n",
    "    boxes = roidb[0]['boxes']\n",
    "    print(boxes)\n",
    "    scores = np.ones(boxes.shape[0])\n",
    "    CONF_THRESH = 0.9\n",
    "    NMS_THRESH = 0.3\n",
    "    print(boxes.shape)\n",
    "    print(scores.shape)\n",
    "    dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32)\n",
    "    keep = nms.nms(dets, NMS_THRESH)\n",
    "    dets = dets[keep,:]\n",
    "    print(dets)\n",
    "    line = connect_proposal(dets[:, 0:4], dets[:, 4], img.shape)\n",
    "    line = line.astype(int)\n",
    "    print(line)\n",
    "    save_results(\"newimg.jpg\", img, line, thresh=0.7)\n",
    "\n",
    "def validNet(image_s,im_info):\n",
    "    feat_stride = [16, ]\n",
    "    ANCHOR_SCALES = [16]\n",
    "    ANCHOR_RATIOS = [0.5,1,2]\n",
    "    num_anchors = 10\n",
    "\n",
    "#     blobs = data_layer.forward()\n",
    "#     image = blobs['data']\n",
    "#     image_src = image.copy()\n",
    "#     im_info = blobs['im_info']\n",
    "    image = image_s.copy()\n",
    "    image = torch.from_numpy(image.transpose([0,3,1,2]))\n",
    "    print('image shape --> {}'.format(image.size()))\n",
    "    \n",
    "    net.eval()\n",
    "    net_conv = net._layers['head'](image)\n",
    "    anchors, length = generate_anchors_pre(net_conv.size(2), net_conv.size(3),feat_stride=feat_stride,anchor_scales=(16,))\n",
    "    anchors = torch.from_numpy(anchors)\n",
    "    rpn = F.relu(net.rpn_net(net_conv))  # ( N , C, H, W）\n",
    "    rpn_reshape = rpn.permute(0,2,3,1).squeeze(0)\n",
    "    rpn_blstm,_ = net.rpn_bi_net(rpn_reshape)\n",
    "    rpn_blstm = F.relu(rpn_blstm)  # 注意另外可以考虑采用batch normal方法对数据进行整理\n",
    "    rpn_blstm_reshape = rpn_blstm.view(-1, RPN_CHANNELS)\n",
    "    rpn_cls_score = net.rpn_cls_score_net(rpn_blstm)   # [W H, num_anchors*2], num_anchors = 10\n",
    "    rpn_cls_score = rpn_cls_score.view(rpn_blstm.size()[0], rpn_blstm.size()[1],-1)\n",
    "    rpn_cls_score = rpn_cls_score.permute(2,0,1).unsqueeze(0)\n",
    "    rpn_cls_score = rpn_cls_score.permute(0,2,3,1)\n",
    "    rpn_cls_score_reshape = rpn_cls_score.contiguous().view(1,rpn_cls_score.size()[1],-1,2)  \n",
    "    # 得到坐标点的10个分类概率（二分类方法)\n",
    "    rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape.view(-1,2),dim=1)           # N 2 H*NUM_ANCHORS W\n",
    "    rpn_cls_prob = rpn_cls_prob_reshape.view(rpn_cls_score.size()[0],\n",
    "                                             rpn_cls_score.size()[1],\n",
    "                                             -1,\n",
    "                                             2 * num_anchors\n",
    "                                            )\n",
    "    \n",
    "    if DEBUG:\n",
    "        print('rpn_cls_score_reshape --> \\n {}'.format(rpn_cls_score_reshape.size()) )\n",
    "        print('rpn cls prob reshape --> \\n {}'.format(rpn_cls_prob_reshape.size()))\n",
    "        print('rpn_cls_score size {} --> \\n {}'.format(rpn_cls_score.size(),rpn_cls_score))\n",
    "        print('rpn_cls_prob size {} --> \\n {}'.format(rpn_cls_prob.size(),rpn_cls_prob))\n",
    "    \n",
    "    rpn_bbox_pred  = net.rpn_bbox_pred_net(rpn_blstm_reshape)   # [W H, num_anchors*4], num_anchors = 10\n",
    "    rpn_bbox_pred = rpn_bbox_pred.view(rpn_blstm.size()[0],rpn_blstm.size()[1],-1)\n",
    "    rpn_bbox_pred = rpn_bbox_pred.unsqueeze(0)\n",
    "\n",
    "    \n",
    "    blob, scores = psl.proposal_layer(rpn_cls_prob, \n",
    "                                      rpn_bbox_pred.contiguous(),\n",
    "                                      im_info,\n",
    "                                      'TEST',\n",
    "                                      feat_stride,\n",
    "                                      anchors,\n",
    "                                      num_anchors)\n",
    "    return image_s, blob, scores, rpn_cls_prob, rpn_bbox_pred, rpn_cls_score,rpn_cls_prob_reshape\n",
    "\n",
    "def showValidNet(image,im_info):\n",
    "    PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n",
    "    image_orig = image.copy()\n",
    "    image = image.astype(np.float32)\n",
    "    image -= PIXEL_MEANS\n",
    "    image = image.reshape(1,image.shape[0],image.shape[1],image.shape[2])\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        image_src, blob, scores , rpn_cls_prob, rpn_bbox_pred, rpn_cls_score,rpn_cls_prob_reshape = validNet(image,im_info)\n",
    "    print('handle times {:.4f}'.format((time.time() - start_time)))  \n",
    "    plt.rcParams['figure.figsize'] = 15, 10\n",
    "    boxes = blob.data.numpy()[:,1:5]\n",
    "    boxes = boxes.astype(np.int)\n",
    "    score = scores.data.numpy()\n",
    "    score = score.reshape(score.shape[0])\n",
    "    image_new = image_src.reshape(image_src.shape[1],image_src.shape[2], image_src.shape[3])\n",
    "\n",
    "    dets = np.hstack((boxes, score[:, np.newaxis])).astype(np.float32)\n",
    "#     print('dets --> \\n {}'.format(dets))\n",
    "    keep = nms.nms(dets, 0.3)\n",
    "    dets = dets[keep,:]\n",
    "    \n",
    "    line = connect_proposal(dets[:, 0:4], dets[:, 4], image_new.shape)\n",
    "#     line = line.astype(int)\n",
    "#     print(line)\n",
    "    # image_new = image_src.reshape(image_src.shape[1],image_src.shape[2], image_src.shape[3])\n",
    "    print(line[0:1])\n",
    "    save_results('newimg.jpg',image_orig, line, 0.7)    \n",
    "    return rpn_cls_prob, rpn_bbox_pred, rpn_cls_score,rpn_cls_prob_reshape,boxes,score, line,dets,image_new\n",
    "\n",
    "# img_path = '/home/hecong/temp/data/txtdect/img/2.jpg'\n",
    "img_path = 'd:\\\\6.jpg'\n",
    "image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "im_info = [image.shape[0],image.shape[1],1]\n",
    "\n",
    "rpn_cls_prob, rpn_bbox_pred, rpn_cls_score,rpn_cls_prob_reshape,boxes,score, line,dets,image_new  =  showValidNet(image,im_info)\n",
    "\n",
    "# testConnectProposal()\n",
    "# showValidNet(image,im_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 600, 878, 3)\n"
     ]
    }
   ],
   "source": [
    "PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n",
    "image = cv2.imread('d:\\\\2.jpg',cv2.IMREAD_COLOR)\n",
    "image = image.astype(np.float32)\n",
    "image -= PIXEL_MEANS\n",
    "image = image.reshape(1,image.shape[0],image.shape[1],image.shape[2])\n",
    "print(image.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
