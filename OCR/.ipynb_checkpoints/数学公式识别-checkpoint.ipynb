{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "goolge ocr mathematics formula\n",
    "2、https://blog.csdn.net/Jeremy_lf/article/details/102728778  https://blog.csdn.net/Jeremy_lf/article/details/102869629 数学公式识别论文一\n",
    "3、https://github.com/luopeixiang/im2latex ！！！\n",
    "4、https://guillaumegenthial.github.io/image-to-latex.html ！！！\n",
    "5、https://github.com/harvardnlp/im2markup\n",
    "5、https://www.ocr.org.uk/Images/73472-datasheet-examination-formulae-and-statistical-tables.pdf ！！\n",
    "6、http://lstm.seas.harvard.edu/latex/ Image-to-Markup Generation with Coarse-to-Fine Attention\n",
    "7、https://github.com/LinXueyuanStdio/LaTeX_OCR ！！！\n",
    "8、https://blog.csdn.net/wxplol/article/details/99941160 ！！！！  其中包括位置信息的解释\n",
    "9、https://www.ocr.org.uk/Images/73472-datasheet-examination-formulae-and-statistical-tables.pdf \n",
    "10、https://arxiv.org/ftp/arxiv/papers/1908/1908.11415.pdf Translating Math Formula Images to LaTeX Sequences Using Deep Neural Networks with Sequence-level Training \n",
    "11、https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=1599&context=etd_projects ！\n",
    "12、https://towardsdatascience.com/deep-learning-for-symbolic-mathematics-5830b22063d0\n",
    "13、http://lstm.seas.harvard.edu/latex/\n",
    "14、https://arxiv.org/ftp/arxiv/papers/2003/2003.00817.pdf 手写体数学公式识别\n",
    "15、https://blog.csdn.net/AckClinkz/article/details/78279074  BLEU1（bilingual evaluation understudy）最早由IBM提出，用来评价翻译质量\n",
    "16、https://blog.csdn.net/lnformat/article/details/88639607\n",
    "17、https://zhidao.baidu.com/question/1990959368206778587.html 数学中的Sin和Cos是什么意思 （在一个平面直角坐标系中，以原点为圆心，1 为半径画一个圆，这个圆交 x 轴于 A 点。以 O 为旋转中心，将 A 点逆时针旋转一定的角度α至 B 点，设此时 B 点的坐标是(x,y)，那么此时 y 的值就叫做α的正弦，记作 sinα；此时 x 的值就叫做α的余弦，）\n",
    "18、https://github.com/antonvladyka/neuralnetworksanddeeplearning.com.pdf\n",
    "19、https://github.com/harvardnlp/im2markup \n",
    "20、https://github.com/luopeixiang ！！！！！\n",
    "21、https://www.zhihu.com/question/347678607/answer/834903728 如何理解Transformer论文中的positional encoding，和三角函数有什么关系？\n",
    "22、https://blog.csdn.net/qq_16234613/article/details/83012046 NLP 自然语言处理 集束搜索beam search和贪心搜索greedy search\n",
    "23、https://github.com/jtyoui/Jtyoui/tree/master/jtyoui/statistics/maths \n",
    "24、https://github.com/rsmith-nl/texcalc 计算latex 结果\n",
    "25、https://github.com/roniemartinez/latex2mathml MathML H5数学表达式\n",
    "26、https://www.cnblogs.com/3daytears/p/9236175.html 生成latex格式公式\n",
    "27、https://pylatexenc.readthedocs.io/en/latest/ convert latex to regular mathematical\n",
    "28、https://www.cnblogs.com/3daytears/p/9236175.html ！！！\n",
    "29、https://pylatexenc.readthedocs.io/en/latest/latexwalker/ Simple Parser for LaTeX Code\n",
    "30、https://github.com/augustt198/latex2sympy\n",
    "31、https://github.com/jungomi/math-formula-recognition Multi-Scale Attention with Dense Encoder for Handwritten Mathematical Expression Recognition.\n",
    "32、https://blog.csdn.net/cj151525/article/details/95756847 python sympy\n",
    "33、https://cloud.tencent.com/act/event/ocrdemo 腾讯云识别\n",
    "34、https://blog.csdn.net/m_buddy/article/details/85178900 《SCA-CNN：Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning》论文笔记 ！！！！\n",
    "35、https://blog.csdn.net/qq_37014750/article/details/83989334 论文阅读：A2-Nets: Double Attention Networks\n",
    "36、 https://www.jianshu.com/p/a617d20162cf 详解编辑距离(Edit Distance)及其代码实现\n",
    "37、https://arxiv.org/abs/1805.09461 Deep Reinforcement Learning For Sequence to Sequence Models  ！！！\n",
    "38、https://www.techleer.com/articles/460-practical_rl-reinforcement-learning-for-seq2seq-pytorch-tensorflow-theano/ ！！！\n",
    "39、https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/cs224n-2017-lecture14-highlight.pdf ！\n",
    "40、https://blog.csdn.net/tudaodiaozhale/article/details/98511081 【自然语言处理】聊聊曝光误差（Exposure Bias）怎么被解决的 \n",
    "41、https://www.jianshu.com/p/9643cba47655  Pytorch中的学习率衰减方法\n",
    "42、https://www.jianshu.com/p/53576b2a7122  CTPN：自然图像文本检测\n",
    "convert LaTeX into a regular mathematical expression ????\n",
    "goolge seq2seq reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latex  数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:35:33.030519Z",
     "start_time": "2020-05-01T14:35:32.983649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import Latex\n",
    "from pytexit import py2tex\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "import io\n",
    "\n",
    "# ss = py2tex(r'sqrt(2/2 + 100)')\n",
    "# print('ss ->', ss)\n",
    "# Latex(ss)\n",
    "# 下载 anltr https://www.cnblogs.com/solvit/p/10097234.html,\n",
    "# 执行 antlr4 PS.g4 -o gen 生成解释代码  https://github.com/augustt198/latex2sympy\n",
    "# import sympy\n",
    "# sympy.simplify('sqrt(2/2-0.5)')\n",
    "#注意 sympy开根号不显示无理数，只会sqrt方式显示。比如8–√=22–√8 ​=22​\n",
    "\n",
    "# latex 转图稿保存\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "def latex_to_img(tex, family='serif'):\n",
    "    buf = io.BytesIO()\n",
    "    plt.axis('off')\n",
    "    plt.rc('text', usetex=False)\n",
    "# params = {'mathtext.fontset': 'stixsans'}\n",
    "# plt.rcParams.update(params)    \n",
    "    # ['dejavusans', 'dejavuserif', 'cm', 'stix', 'stixsans', 'custom']\n",
    "    plt.rc('mathtext',fontset='cm')\n",
    "#     plt.rc('mathtext',default='regular')\n",
    "    plt.rc('font', family=family)\n",
    "    plt.text(0.5, 0.5, r\"$%s$\" % tex,fontsize = 40, ha='center', va='center')\n",
    "    plt.savefig(buf,format='png',transparent=False,pad_inches=0,dpi=300)\n",
    "    plt.close()\n",
    "    image = Image.open(buf)\n",
    "    return image\n",
    "\n",
    "def image_ract(image_array):\n",
    "    x_array, y_array = np.where(image_array==1)\n",
    "    return x_array[np.argmin(x_array)] -2 , y_array[np.argmin(y_array)] -2 , x_array[np.argmax(x_array)] + 2, y_array[np.argmax(y_array)] + 2\n",
    "\n",
    "def get_latex_image(tex,family='serif'):\n",
    "    image = latex_to_img(tex,family)\n",
    "    image_array = np.array(image.copy().convert(\"L\"))\n",
    "    image_array_new = 1 - image_array/255 \n",
    "    x1,y1,x2,y2 = image_ract(image_array_new)\n",
    "    image = image.crop((y1,x1,y2,x2))\n",
    "    image = image.convert('RGB')\n",
    "    return image\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:35:54.174055Z",
     "start_time": "2020-05-01T14:35:53.611578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "latex ='\\\\int _ { - \\\\epsilon } ^ { \\\\infty } d l \\\\:  \\\\int _ { - \\\\epsilon }  \\\\sqrt { 4  } + \\\\frac { 99 } { 8 } '\n",
    "# latex = '\\\\sqrt { 4  } + \\\\frac { 9 } { 8 }'\n",
    "image = get_latex_image(latex)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T14:26:38.118698Z",
     "start_time": "2020-04-17T14:26:37.765861Z"
    }
   },
   "outputs": [],
   "source": [
    "# 生成训练，测试数据\n",
    "# https://haolaoshi.blog.csdn.net/article/details/89531570 Latex四则运算符号\n",
    "# -*- coding: UTF-8 -*-\n",
    "from pytexit import py2tex\n",
    "from random import randint\n",
    "\n",
    "formul_files = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\latex_formul_normal.txt'\n",
    "\n",
    "fms = ['\\\\frac {{ {0} }} {{ {1} }} + {{ {2} }}', \n",
    "        '{{ {0} }} \\\\times {{ {1} }} - {2} {3}', \n",
    "       '{0} - \\\\frac {{ {1} ^ {2} }} {{ {3} }} = {4}',\n",
    "       '\\\\sqrt {{ {0} {1} }} + \\\\frac {{ {3} }} {{ {4} }}',\n",
    "       '{0} \\\\div {1} \\\\times \\sqrt {{ {2} }} = {3}']\n",
    "\n",
    "size = 20000\n",
    "formul_lists = []\n",
    "for idx in range(len(fms),len(fms)+size):\n",
    "    fm = fms[idx % len(fms)]\n",
    "    fm = fm.format(randint(0,9),randint(0,9),randint(0,9),randint(0,9),randint(0,9),randint(0,9),randint(0,9),randint(0,9),randint(0,9))\n",
    "    fm = fm + '\\n'\n",
    "    formul_lists.append(fm)\n",
    "\n",
    "with open(formul_files, 'w', encoding='utf8') as f:\n",
    "    f.writelines(formul_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T15:14:27.545499Z",
     "start_time": "2020-04-17T14:55:43.057329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen image : 0\n",
      "gen image : 1000\n",
      "gen image : 2000\n",
      "gen image : 3000\n",
      "gen image : 4000\n",
      "gen image : 5000\n",
      "gen image : 6000\n",
      "gen image : 7000\n",
      "gen image : 8000\n",
      "gen image : 9000\n",
      "gen image : 10000\n",
      "gen image : 11000\n",
      "gen image : 12000\n",
      "gen image : 13000\n",
      "gen image : 14000\n",
      "gen image : 15000\n",
      "gen image : 16000\n",
      "gen image : 17000\n",
      "gen image : 18000\n",
      "gen image : 19000\n",
      "生成图片完成 \n"
     ]
    }
   ],
   "source": [
    "# 生成图片数据\n",
    "import os\n",
    "formul_files = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\latex_formul_normal.txt'\n",
    "formul_image_path = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\gen_images'\n",
    "\n",
    "formul_lists = None\n",
    "with open(formul_files,'r', encoding='utf8') as f:\n",
    "    formul_lists = f.readlines()\n",
    "# print(formul_lists)\n",
    "\n",
    "for idx in range(len(formul_lists)):\n",
    "    tex = formul_lists[idx]\n",
    "    tex = tex.replace('\\n','')\n",
    "#     print('tex:', tex)\n",
    "#     print('\\\\frac{8}{6}+{7}')\n",
    "#     tex = '\\\\frac{8}{6}+{7}'\n",
    "    if idx % 1000 == 0:\n",
    "        print('gen image :', idx)\n",
    "    image = get_latex_image(tex)\n",
    "    image.save(os.path.sep.join([formul_image_path,f'{idx}.png']))\n",
    "    \n",
    "print('生成图片完成 ')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T09:46:18.292487Z",
     "start_time": "2020-04-17T09:46:18.285489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RGB'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open('D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\gen_images\\\\0.png')\n",
    "# image = image.convert('RGB')\n",
    "image.mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:34:03.967593Z",
     "start_time": "2020-04-20T00:34:03.452886Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://github.com/luopeixiang/im2latex\n",
    "https://blog.csdn.net/SHU15121856/article/details/104448734 nn.LSTM和nn.LSTMCell的使用\n",
    "\n",
    "'''\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch.distributions.uniform import Uniform\n",
    "\n",
    "def add_positional_features(tensor: torch.Tensor,\n",
    "                            min_timescale: float = 1.0,\n",
    "                            max_timescale: float = 1.0e4):\n",
    "    \"\"\"\n",
    "    Implements the frequency-based positional encoding described\n",
    "    in `Attention is all you Need\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : ``torch.Tensor``\n",
    "        a Tensor with shape (batch_size, timesteps, hidden_dim).\n",
    "    min_timescale : ``float``, optional (default = 1.0)\n",
    "        The largest timescale to use.\n",
    "    Returns\n",
    "    -------\n",
    "    The input tensor augmented with the sinusoidal frequencies.\n",
    "    \"\"\"\n",
    "    _, timesteps, hidden_dim = tensor.size()\n",
    "    timestep_range = get_range_vector(timesteps, tensor.device).data.float()\n",
    "    # We're generating both cos and sin frequencies,\n",
    "    # so half for each.\n",
    "    num_timescales = hidden_dim // 2\n",
    "    timescale_range = get_range_vector(num_timescales, tensor.device).data.float()\n",
    "\n",
    "    log_timescale_increments = math.log(float(max_timescale) / float(min_timescale)) / float(num_timescales - 1)\n",
    "    inverse_timescales = min_timescale * \\\n",
    "        torch.exp(timescale_range * -log_timescale_increments)\n",
    "\n",
    "    # Broadcasted multiplication - shape (timesteps, num_timescales)\n",
    "    scaled_time = timestep_range.unsqueeze(1) * inverse_timescales.unsqueeze(0)\n",
    "    # shape (timesteps, 2 * num_timescales)\n",
    "    sinusoids = torch.randn(scaled_time.size(0), 2*scaled_time.size(1), device=tensor.device)\n",
    "    sinusoids[:, ::2] = torch.sin(scaled_time)\n",
    "    sinusoids[:, 1::2] = torch.cos(scaled_time)\n",
    "    if hidden_dim % 2 != 0:\n",
    "        # if the number of dimensions is odd, the cos and sin\n",
    "        # timescales had size (hidden_dim - 1) / 2, so we need\n",
    "        # to add a row of zeros to make up the difference.\n",
    "        sinusoids = torch.cat([sinusoids, sinusoids.new_zeros(timesteps, 1)], 1)\n",
    "    return tensor + sinusoids.unsqueeze(0)\n",
    "\n",
    "def get_range_vector(size: int, device) -> torch.Tensor:\n",
    "    return torch.arange(0, size, dtype=torch.long, device=device)\n",
    "\n",
    "INIT = 1e-2\n",
    "\n",
    "class Im2LatexModel(nn.Module):\n",
    "    def __init__(self, out_size, emb_size, dec_rnn_h,\n",
    "                 enc_out_dim=512,  n_layer=1,\n",
    "                 add_pos_feat=False, dropout=0.):\n",
    "        super(Im2LatexModel, self).__init__()\n",
    "\n",
    "        self.cnn_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1), 0),\n",
    "\n",
    "            nn.Conv2d(256, enc_out_dim, 3, 1, 0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.rnn_decoder = nn.LSTMCell(dec_rnn_h+emb_size, dec_rnn_h)\n",
    "        self.embedding = nn.Embedding(out_size, emb_size)\n",
    "\n",
    "        self.init_wh = nn.Linear(enc_out_dim, dec_rnn_h)\n",
    "        self.init_wc = nn.Linear(enc_out_dim, dec_rnn_h)\n",
    "        self.init_wo = nn.Linear(enc_out_dim, dec_rnn_h)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.beta = nn.Parameter(torch.Tensor(enc_out_dim))\n",
    "        init.uniform_(self.beta, -INIT, INIT)\n",
    "        self.W_1 = nn.Linear(enc_out_dim, enc_out_dim, bias=False)\n",
    "        self.W_2 = nn.Linear(dec_rnn_h, enc_out_dim, bias=False)\n",
    "\n",
    "        self.W_3 = nn.Linear(dec_rnn_h+enc_out_dim, dec_rnn_h, bias=False)\n",
    "        self.W_out = nn.Linear(dec_rnn_h, out_size, bias=False)\n",
    "\n",
    "        self.add_pos_feat = add_pos_feat\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.uniform = Uniform(0, 1)\n",
    "\n",
    "    def forward(self, imgs, formulas, epsilon=1.):\n",
    "        \"\"\"args:\n",
    "        imgs: [B, C, H, W]\n",
    "        formulas: [B, MAX_LEN]\n",
    "        epsilon: probability of the current time step to  use the true previous token\n",
    "        return:\n",
    "        logits: [B, MAX_LEN, VOCAB_SIZE]\n",
    "        \"\"\"\n",
    "        # encoding\n",
    "        encoded_imgs = self.encode(imgs)  # [B, H*W, 512]\n",
    "        # init decoder's states  ？？？？？？？？？？\n",
    "        dec_states, o_t = self.init_decoder(encoded_imgs)\n",
    "        max_len = formulas.size(1)\n",
    "        logits = []\n",
    "        for t in range(max_len):\n",
    "            tgt = formulas[:, t:t+1]\n",
    "            # schedule sampling\n",
    "            if logits and self.uniform.sample().item() > epsilon:\n",
    "                tgt = torch.argmax(torch.log(logits[-1]), dim=1, keepdim=True)\n",
    "            # ont step decoding\n",
    "            dec_states, O_t, logit = self.step_decoding(dec_states, o_t, encoded_imgs, tgt)\n",
    "            logits.append(logit)\n",
    "        logits = torch.stack(logits, dim=1)  # [B, MAX_LEN, out_size]\n",
    "        return logits\n",
    "\n",
    "    def encode(self, imgs):\n",
    "        encoded_imgs = self.cnn_encoder(imgs)  # [B, 512, H', W']\n",
    "        encoded_imgs = encoded_imgs.permute(0, 2, 3, 1)  # [B, H', W', 512]\n",
    "        B, H, W, _ = encoded_imgs.shape\n",
    "        encoded_imgs = encoded_imgs.contiguous().view(B, H*W, -1)\n",
    "        if self.add_pos_feat:\n",
    "            encoded_imgs = add_positional_features(encoded_imgs)\n",
    "        return encoded_imgs\n",
    "\n",
    "    \n",
    "    def step_decoding(self, dec_states, o_t, enc_out, tgt):\n",
    "        \"\"\"\n",
    "            Runing one step decoding\n",
    "            dec_states : (h_t, c_t)\n",
    "            o_t: atten scores\n",
    "            enc_out: pre logits\n",
    "            tgt: pre true target\n",
    "        \"\"\"\n",
    "\n",
    "        prev_y = self.embedding(tgt).squeeze(1)  # [B, emb_size]\n",
    "        inp = torch.cat([prev_y, o_t], dim=1)  # [B, emb_size+dec_rnn_h]\n",
    "        h_t, c_t = self.rnn_decoder(inp, dec_states)  # h_t:[B, dec_rnn_h]\n",
    "        h_t = self.dropout(h_t)\n",
    "        c_t = self.dropout(c_t)\n",
    "\n",
    "        # context_t : [B, C]\n",
    "        context_t, attn_scores = self._get_attn(enc_out, h_t)\n",
    "\n",
    "        # [B, dec_rnn_h]\n",
    "        o_t = self.W_3(torch.cat([h_t, context_t], dim=1)).tanh()\n",
    "        o_t = self.dropout(o_t)\n",
    "\n",
    "        # calculate logit\n",
    "        logit = F.softmax(self.W_out(o_t), dim=1)  # [B, out_size]\n",
    "\n",
    "        return (h_t, c_t), o_t, logit\n",
    "\n",
    "    def _get_attn(self, enc_out, h_t):\n",
    "        \"\"\"Attention mechanism\n",
    "        args:\n",
    "            enc_out: row encoder's output [B, L=H*W, C]\n",
    "            h_t: the current time step hidden state [B, dec_rnn_h]\n",
    "        return:\n",
    "            context: this time step context [B, C]\n",
    "            attn_scores: Attention scores\n",
    "        \"\"\"\n",
    "        # cal alpha\n",
    "        alpha = torch.tanh(self.W_1(enc_out)+self.W_2(h_t).unsqueeze(1))\n",
    "        alpha = torch.sum(self.beta*alpha, dim=-1)  # [B, L]\n",
    "        alpha = F.softmax(alpha, dim=-1)  # [B, L]\n",
    "\n",
    "        # cal context: [B, C]\n",
    "        context = torch.bmm(alpha.unsqueeze(1), enc_out)\n",
    "        context = context.squeeze(1)\n",
    "        return context, alpha\n",
    "\n",
    "    def init_decoder(self, enc_out):\n",
    "        \"\"\"args:\n",
    "            enc_out: the output of row encoder [B, H*W, C]\n",
    "          return:\n",
    "            h_0, c_0:  h_0 and c_0's shape: [B, dec_rnn_h]\n",
    "            init_O : the average of enc_out  [B, dec_rnn_h]\n",
    "            for decoder\n",
    "        \"\"\"\n",
    "        mean_enc_out = enc_out.mean(dim=1)\n",
    "        h = self._init_h(mean_enc_out)\n",
    "        c = self._init_c(mean_enc_out)\n",
    "        init_o = self._init_o(mean_enc_out)\n",
    "        return (h, c), init_o\n",
    "\n",
    "    def _init_h(self, mean_enc_out):\n",
    "        return torch.tanh(self.init_wh(mean_enc_out))\n",
    "\n",
    "    def _init_c(self, mean_enc_out):\n",
    "        return torch.tanh(self.init_wc(mean_enc_out))\n",
    "\n",
    "    def _init_o(self, mean_enc_out):\n",
    "        return torch.tanh(self.init_wo(mean_enc_out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T04:03:58.515550Z",
     "start_time": "2020-04-20T04:02:46.677079Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process train dataset...\n",
      "D:\\PROJECT_TW\\git\\data\\im2latex\\latex_train_filter.txt\n",
      "Save train dataset to D:\\PROJECT_TW\\git\\data\\im2latex\\train.pkl\n",
      "Process validate dataset...\n",
      "D:\\PROJECT_TW\\git\\data\\im2latex\\latex_validate_filter.txt\n",
      "Save validate dataset to D:\\PROJECT_TW\\git\\data\\im2latex\\validate.pkl\n",
      "Process test dataset...\n",
      "D:\\PROJECT_TW\\git\\data\\im2latex\\latex_test_filter.txt\n",
      "Save test dataset to D:\\PROJECT_TW\\git\\data\\im2latex\\test.pkl\n",
      "Writing Vocab File in  D:\\PROJECT_TW\\git\\data\\im2latex\\vocab.pkl len : 24\n"
     ]
    }
   ],
   "source": [
    "# 执行 build_vocab 生成latex 向量表\n",
    "import lib.im2latex.preprocess as pre \n",
    "import lib.im2latex.build_vocab as bv\n",
    "import importlib\n",
    "from os.path import join\n",
    "importlib.reload(pre)\n",
    "importlib.reload(bv)\n",
    "size = 1000\n",
    "splits = [\"validate\", \"test\", \"train\"]\n",
    "data_path = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex'\n",
    "\n",
    "def write_filter_data(split, datas):\n",
    "    split_file = join(data_path, \"latex_{}_filter.txt\".format(split))\n",
    "    with open(split_file,'w') as f:\n",
    "        wdatas = [f'{x}\\n' for x in datas]\n",
    "        f.writelines(wdatas)\n",
    "# bv.build_vocab(data_path)\n",
    "train_data = [f'{x}.png {x}' for x in range(18000)]\n",
    "valid_data = [f'{x}.png {x}' for x in range(18000,19000)]\n",
    "test_data = [f'{x}.png {x}' for x in range(19000,20000)]\n",
    "write_filter_data('train', train_data)\n",
    "write_filter_data('validate', valid_data)\n",
    "write_filter_data('test', test_data)\n",
    "pre.preprocess(data_path,'train')\n",
    "pre.preprocess(data_path,'validate')\n",
    "pre.preprocess(data_path,'test')\n",
    "bv.build_vocab(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T06:22:19.676580Z",
     "start_time": "2020-04-19T06:22:19.574639Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载gc模块\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:34:19.131276Z",
     "start_time": "2020-04-20T00:34:19.081306Z"
    },
    "code_folding": [
     80,
     104
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from lib.im2latex.utils import cal_loss, cal_epsilon\n",
    "import gc\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, optimizer, model, lr_scheduler,\n",
    "                 train_loader, val_loader, args,\n",
    "                 use_cuda=False, init_epoch=1, last_epoch=15):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.args = args\n",
    "        self.step = 0\n",
    "        self.epoch = init_epoch\n",
    "        self.total_step = (init_epoch-1)*len(train_loader)\n",
    "        self.last_epoch = last_epoch\n",
    "        self.best_val_loss = 1e18\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    def train(self):\n",
    "        mes = \"Epoch {}, step:{}/{} {:.2f}%, Loss:{:.4f}, Perplexity:{:.4f}\"\n",
    "        self.last_epoch = 1000\n",
    "        args.print_freq = 25\n",
    "        print('max epoch --> ', self.last_epoch,  ' print freq:', args.print_freq)\n",
    "        while self.epoch <= self.last_epoch:\n",
    "            self.model.train()\n",
    "            losses = 0.0\n",
    "            #  tgt4training 字符串开始标记<s>, tgt4cal_loss 字符串结束标记</s>    \n",
    "            for imgs, tgt4training, tgt4cal_loss in self.train_loader:\n",
    "                step_loss = self.train_step(imgs, tgt4training, tgt4cal_loss)\n",
    "                losses += step_loss\n",
    "\n",
    "                # log message\n",
    "                if self.step % self.args.print_freq == 0:\n",
    "                    avg_loss = losses / self.args.print_freq\n",
    "                    print(mes.format(\n",
    "                        self.epoch, self.step, len(self.train_loader),\n",
    "                        100 * self.step / len(self.train_loader),\n",
    "                        avg_loss,\n",
    "                        2**avg_loss\n",
    "                    ))\n",
    "                    losses = 0.0\n",
    "                    gc.collect()\n",
    "\n",
    "            # one epoch Finished, calcute val loss\n",
    "            val_loss = self.validate()\n",
    "            self.lr_scheduler.step(val_loss)\n",
    "\n",
    "            self.save_model('ckpt-{}-{:.4f}'.format(self.epoch, val_loss))\n",
    "            self.epoch += 1\n",
    "            self.step = 0\n",
    "\n",
    "    def train_step(self, imgs, tgt4training, tgt4cal_loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        imgs = imgs.to(self.device)\n",
    "        tgt4training = tgt4training.to(self.device)\n",
    "        tgt4cal_loss = tgt4cal_loss.to(self.device)\n",
    "\n",
    "        '''\n",
    "        Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks\n",
    "        See details in https://arxiv.org/pdf/1506.03099.pdf\n",
    "        '''\n",
    "        epsilon = cal_epsilon(self.args.decay_k, self.total_step, self.args.sample_method)\n",
    "\n",
    "        '''\n",
    "        https://www.zhihu.com/question/60751553 如何理解深度学习源码里经常出现的logits？\n",
    "        logits: 一个事件发生与该事件不发生的比值的对数（统计学习方法-李航 p78）。假设一个事件发生的概率为 p，那么该事件的 logits 为 logit(p) = log(p/1-p) .\n",
    "        '''\n",
    "        logits = self.model(imgs, tgt4training, epsilon)\n",
    "\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = cal_loss(logits, tgt4cal_loss)\n",
    "        self.step += 1\n",
    "        self.total_step += 1\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.model.parameters(), self.args.clip)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0.0\n",
    "        mes = \"Epoch {}, validation average loss:{:.4f}, Perplexity:{:.4f}\"\n",
    "        with torch.no_grad():\n",
    "            for imgs, tgt4training, tgt4cal_loss in self.val_loader:\n",
    "                imgs = imgs.to(self.device)\n",
    "                tgt4training = tgt4training.to(self.device)\n",
    "                tgt4cal_loss = tgt4cal_loss.to(self.device)\n",
    "\n",
    "                epsilon = cal_epsilon(\n",
    "                    self.args.decay_k, self.total_step, self.args.sample_method)\n",
    "                logits = self.model(imgs, tgt4training, epsilon)\n",
    "                loss = cal_loss(logits, tgt4cal_loss)\n",
    "                val_total_loss += loss\n",
    "            avg_loss = val_total_loss / len(self.val_loader)\n",
    "            print(mes.format(\n",
    "                self.epoch, avg_loss, 2**avg_loss\n",
    "            ))\n",
    "#             print('predict:',logits)\n",
    "#             print('result:',tgt4cal_loss)\n",
    "            \n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = avg_loss\n",
    "            self.save_model('best_ckpt')\n",
    "        return avg_loss\n",
    "\n",
    "    def save_model(self, model_name):\n",
    "        if not os.path.isdir(self.args.save_dir):\n",
    "            os.makedirs(self.args.save_dir)\n",
    "        save_path = join(self.args.save_dir, model_name+'.pt')\n",
    "        print(\"Saving checkpoint to {}\".format(save_path))\n",
    "\n",
    "        # torch.save(self.model, model_path)\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'lr_sche': self.lr_scheduler.state_dict(),\n",
    "            'epoch': self.epoch,\n",
    "            'args': self.args\n",
    "        }, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:34:26.502984Z",
     "start_time": "2020-04-20T00:34:26.455007Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from lib.im2latex.utils import collate_fn, get_checkpoint\n",
    "from lib.im2latex.data import Im2LatexDataset\n",
    "from lib.im2latex.build_vocab import Vocab, load_vocab\n",
    "def init_loader():\n",
    "    # get args\n",
    "    parser = argparse.ArgumentParser(description=\"Im2Latex Training Program\")\n",
    "    # parser.add_argument('--path', required=True, help='root of the model')\n",
    "    # model args\n",
    "    parser.add_argument(\"--emb_dim\", type=int, default=80, help=\"Embedding size\")\n",
    "    parser.add_argument(\"--dec_rnn_h\", type=int, default=512, help=\"The hidden state of the decoder RNN\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\", help=\"The dataset's dir\")\n",
    "    parser.add_argument(\"--add_position_features\", action='store_true', default=True, help=\"Use position embeddings or not\")\n",
    "    # training args\n",
    "    parser.add_argument(\"--max_len\", type=int, default=150, help=\"Max size of formula\")\n",
    "    parser.add_argument(\"--dropout\", type=float,default=0., help=\"Dropout probility\")\n",
    "    parser.add_argument(\"--cuda\", action='store_true',default=True, help=\"Use cuda or not\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--epoches\", type=int, default=200)\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-4,help=\"Learning Rate\")\n",
    "    parser.add_argument(\"--min_lr\", type=float, default=3e-5, help=\"Learning Rate\")\n",
    "    parser.add_argument(\"--sample_method\", type=str, default=\"teacher_forcing\", choices=('teacher_forcing', 'exp', 'inv_sigmoid'), help=\"The method to schedule sampling\")\n",
    "    parser.add_argument(\"--decay_k\", type=float, default=1.,\n",
    "                        help=\"Base of Exponential decay for Schedule Sampling. \"\n",
    "                        \"When sample method is Exponential deca;\"\n",
    "                        \"Or a constant in Inverse sigmoid decay Equation. \"\n",
    "                        \"See details in https://arxiv.org/pdf/1506.03099.pdf\")\n",
    "    parser.add_argument(\"--lr_decay\", type=float, default=0.5, help=\"Learning Rate Decay Rate\")\n",
    "    parser.add_argument(\"--lr_patience\", type=int, default=3,  help=\"Learning Rate Decay Patience\")\n",
    "    parser.add_argument(\"--clip\", type=float, default=2.0, help=\"The max gradient norm\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\ckpts\", help=\"The dir to save checkpoints\")\n",
    "    parser.add_argument(\"--print_freq\", type=int, default=100, help=\"The frequency to print message\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=2020, help=\"The random seed for reproducing \")\n",
    "    parser.add_argument(\"--from_check_point\", action='store_true', default=False, help=\"Training from checkpoint or not\")\n",
    "    #  注意在 jupyter notebook 需带args=[] 这个参数\n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    from_check_point = args.from_check_point\n",
    "    if from_check_point:\n",
    "        checkpoint_path = get_checkpoint(args.save_dir)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        args = checkpoint['args']\n",
    "    args.epoches=200\n",
    "    args.from_check_point = True\n",
    "    max_epoch = args.epoches\n",
    "    print(\"Training args:\", args)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    # Building vocab\n",
    "    print(\"Load vocab...\")\n",
    "    vocab = load_vocab(args.data_path)\n",
    "#     print('vocab -->', vocab.sign2id)\n",
    "    use_cuda = True if args.cuda and torch.cuda.is_available() else False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # data loader\n",
    "    print(\"Construct data loader...\")\n",
    "#     train_loader = None\n",
    "    train_loader = DataLoader(\n",
    "        Im2LatexDataset(args.data_path, 'train', args.max_len),\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=partial(collate_fn, vocab.sign2id),\n",
    "        pin_memory=True if use_cuda else False,\n",
    "        num_workers=1)\n",
    "    val_loader = DataLoader(\n",
    "        Im2LatexDataset(args.data_path, 'validate', args.max_len),\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=partial(collate_fn, vocab.sign2id),\n",
    "        pin_memory=True if use_cuda else False,\n",
    "        num_workers=1)\n",
    "    print(\"Construct data loader over\")\n",
    "    return train_loader, val_loader,vocab, args\n",
    "\n",
    "def train(train_loader,val_loader, vocab, args):\n",
    "    use_cuda = True if args.cuda and torch.cuda.is_available() else False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")    \n",
    "    # construct model\n",
    "    print(\"Construct model\")\n",
    "    vocab_size = len(vocab)\n",
    "    model = Im2LatexModel(\n",
    "        vocab_size, args.emb_dim, args.dec_rnn_h,\n",
    "        add_pos_feat=args.add_position_features,\n",
    "        dropout=args.dropout\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    print(\"Model Settings:\")\n",
    "    print(model)  \n",
    "    # construct optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    lr_scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        \"min\",\n",
    "        factor=args.lr_decay,\n",
    "        patience=args.lr_patience,\n",
    "        verbose=True,\n",
    "        min_lr=args.min_lr)    \n",
    "    \n",
    "        \n",
    "    if args.from_check_point:\n",
    "        print('from check point .. ')\n",
    "        checkpoint_path = get_checkpoint(args.save_dir)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        args = checkpoint['args']\n",
    "    \n",
    "    if args.from_check_point:\n",
    "        print('from check point .. ')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_sche'])\n",
    "        max_epoch = args.epoches\n",
    "        # init trainer from checkpoint\n",
    "        trainer = Trainer(optimizer, model, lr_scheduler,\n",
    "                          train_loader, val_loader, args,\n",
    "                          use_cuda=use_cuda,\n",
    "                          init_epoch=epoch, last_epoch=max_epoch)\n",
    "    else:\n",
    "        trainer = Trainer(optimizer, model, lr_scheduler,\n",
    "                          train_loader, val_loader, args,\n",
    "                          use_cuda=use_cuda,\n",
    "                          init_epoch=1, last_epoch=1000)\n",
    "    # begin training\n",
    "    trainer.train()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:34:38.447719Z",
     "start_time": "2020-04-20T00:34:29.017800Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args: Namespace(add_position_features=True, batch_size=32, clip=2.0, cuda=True, data_path='D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex', dec_rnn_h=512, decay_k=1.0, dropout=0.0, emb_dim=80, epoches=200, from_check_point=True, lr=0.0003, lr_decay=0.5, lr_patience=3, max_len=150, min_lr=3e-05, print_freq=100, sample_method='teacher_forcing', save_dir='D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\ckpts', seed=2020)\n",
      "Load vocab...\n",
      "Load vocab including 24 words!\n",
      "Construct data loader...\n",
      "Construct data loader over\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "train_loader, val_loader,vocab, args =  init_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T02:05:12.083561Z",
     "start_time": "2020-04-20T00:34:50.114306Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct model\n",
      "Model Settings:\n",
      "Im2LatexModel(\n",
      "  (cnn_encoder): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (12): ReLU()\n",
      "  )\n",
      "  (rnn_decoder): LSTMCell(592, 512)\n",
      "  (embedding): Embedding(24, 80)\n",
      "  (init_wh): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (init_wc): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (init_wo): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (W_1): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (W_2): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (W_3): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (W_out): Linear(in_features=512, out_features=24, bias=False)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "from check point .. \n",
      "Get checkpoint from D:\\PROJECT_TW\\git\\data\\im2latex\\ckpts\\ckpt-7-0.8330.pt for training\n",
      "max epoch -->  1000  print freq: 25\n",
      "Epoch 1, step:100/563 17.76%, Loss:1.1925, Perplexity:2.2854\n",
      "Epoch 1, step:200/563 35.52%, Loss:1.1619, Perplexity:2.2375\n",
      "Epoch 1, step:300/563 53.29%, Loss:0.9388, Perplexity:1.9169\n",
      "Epoch 1, step:400/563 71.05%, Loss:1.3174, Perplexity:2.4922\n",
      "Epoch 1, step:500/563 88.81%, Loss:1.0652, Perplexity:2.0924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8f16a2c1ea67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_check_point\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-e6f7acb0f76e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, val_loader, vocab, args)\u001b[0m\n\u001b[0;32m    128\u001b[0m                           init_epoch=1, last_epoch=1000)\n\u001b[0;32m    129\u001b[0m     \u001b[1;31m# begin training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3c7e34dab433>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m#  tgt4training 字符串开始标记<s>, tgt4cal_loss 字符串结束标记</s>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt4training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt4cal_loss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0mstep_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt4training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt4cal_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstep_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3c7e34dab433>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, imgs, tgt4training, tgt4cal_loss)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project_tw\\twedu\\venv37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project_tw\\twedu\\venv37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager().update('notebook', {'limit_output': 10000})\n",
    "args.from_check_point=True\n",
    "args.print_freq=25\n",
    "train(train_loader, val_loader,vocab, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:33:25.860439Z",
     "start_time": "2020-04-20T00:32:33.893869Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load vocab including 24 words!\n",
      "data loader\n",
      "add_position_features: True\n",
      "begin evulate ..\n",
      "results: ['{ 1 } { 1 } { 5 } - 5 9']\n",
      "tgt4cal_loss: ['{ 1 } \\\\times { 1 } - 5 9']\n",
      "results: ['9 { 1 9 } { 9 9 } 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9', '7 - \\\\frac { 1 } { 1 } - 7 7', '2 { 7 2 2 } { 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2', '1 { 3 1 } + { 5 } - 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1', '9 { 1 9 } + { 6 }', '6 { 1 2 } + { 2 } - 2 2 2 2 2 2 2 2 2 2 2 - 2 { 7 } { 0 } - 2 2 2 2 2 2 2 2 2 2 - 2 { 7 } { 0 } - 2 2 2 2 2 2 2 2 2 2 - 2 { 7 }', '8 { 3 9 } + { 4 }', '8 { 3 2 } { 2 2 } 2 2 2 2 2 2 2 2 2 2 2 2 - 8 { 1 2 } { 2 2 2 2 2 2 2 2 2 2 2 2 2 - 8 { 1 2 } { 2 2 2 2 2 2 2 2 2 2 2 2 2 2 - {', '4 { 5 1 } + { 7 } - 4 4', '4 { 5 1 } + { 2 } - 4 4', '2 { 5 2 } + { 4 } - 1 2 2 2 2 2 2 2 2 2 2 2 2 - { 4 2 } - 2 1 2 2 2 2 2 2 2 2 - { 4 2 } - 2 1 2 2 2 2 2 2 2 2 - { 4 2 } - 2 1 2', '4 { 1 4 } + { 6 }', '3 - { 3 3 3 - 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3', '3 - \\\\frac { 9 3 } { 3 } 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3', '0 { 1 5 } + { 1 } - 5 5']\n",
      "tgt4cal_loss: ['{ 1 } \\\\times { 8 } - 9 9', '{ 3 } \\\\times { 1 } - 7 1', '{ 7 } \\\\times { 2 } - 2 2', '{ 3 } \\\\times { 5 } - 1 1', '{ 1 } \\\\times { 6 } - 9 9', '{ 1 } \\\\times { 7 } - 2 6', '{ 3 } \\\\times { 4 } - 8 5', '{ 3 } \\\\times { 1 } - 8 2', '{ 5 } \\\\times { 7 } - 4 1', '{ 5 } \\\\times { 2 } - 4 1', '{ 5 } \\\\times { 4 } - 1 2', '{ 1 } \\\\times { 8 } - 4 0', '{ 1 } \\\\times { 3 } - 3 8', '{ 7 } \\\\times { 9 } - 3 1', '{ 1 } \\\\times { 1 } - 5 0']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-06c095b2f365>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beam search result:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-06c095b2f365>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mreference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlatex_producer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_idx2formulas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt4cal_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlatex_producer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'results:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tgt4cal_loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\PROJECT_TW\\git\\myproject\\OCR\\lib\\im2latex\\decoding.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, imgs)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_greedy_decoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_beam_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\PROJECT_TW\\git\\myproject\\OCR\\lib\\im2latex\\decoding.py\u001b[0m in \u001b[0;36m_batch_beam_search\u001b[1;34m(self, imgs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'enc_outs'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_outs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         all_top_k_predictions, log_probabilities = self._beam_search.search(\n\u001b[1;32m--> 185\u001b[1;33m             start_predictions, state, self._take_step)\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mall_top_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_top_k_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\PROJECT_TW\\git\\myproject\\OCR\\lib\\im2latex\\beam_search.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, start_predictions, start_state, step)\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m                     \u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mlast_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m                     \u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_backpointer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m                     \u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mlast_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "from functools import partial\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import lib.im2latex.data as idata # import Im2LatexDataset\n",
    "from lib.im2latex.build_vocab import Vocab, load_vocab\n",
    "from lib.im2latex.utils import collate_fn\n",
    "from lib.im2latex.decoding import LatexProducer\n",
    "from lib.im2latex.score import score_files\n",
    "import importlib\n",
    "importlib.reload(idata)\n",
    "\n",
    "def evaluate():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Im2Latex Evaluating Program\")\n",
    "    parser.add_argument('--model_path', required=False, help='path of the evaluated model')\n",
    "\n",
    "    # model args\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\", help=\"The dataset's dir\")\n",
    "    parser.add_argument(\"--cuda\", action='store_true',default=False, help=\"Use cuda or not\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--beam_size\", type=int, default=5)\n",
    "    parser.add_argument(\"--result_path\", type=str, default=\"D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\result.txt\", help=\"The file to store result\")\n",
    "    parser.add_argument(\"--ref_path\", type=str, default=\"D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\ref.txt\", help=\"The file to store reference\")\n",
    "    parser.add_argument(\"--max_len\", type=int, default=64, help=\"Max step of decoding\")\n",
    "    parser.add_argument(\"--split\", type=str,default=\"validate\", help=\"The data split to decode\")\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    # 加载 模型\n",
    "    model_path = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\ckpts\\\\best_ckpt.pt'\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model_args = checkpoint['args']\n",
    "\n",
    "    # 读入词典,设置其他相关参数\n",
    "    vocab = load_vocab(args.data_path)\n",
    "    use_cuda = True if args.cuda and torch.cuda.is_available() else False\n",
    "\n",
    "    # 加载测试集\n",
    "    print('data loader')\n",
    "    data_loader = DataLoader(\n",
    "        idata.Im2LatexDataset(args.data_path, args.split, args.max_len),\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=partial(collate_fn, vocab.sign2id),\n",
    "        pin_memory=True if use_cuda else False,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    print('add_position_features:', model_args.add_position_features)\n",
    "    model = Im2LatexModel(\n",
    "        len(vocab), model_args.emb_dim, model_args.dec_rnn_h,\n",
    "        add_pos_feat=model_args.add_position_features,\n",
    "        dropout=model_args.dropout\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    \n",
    "    result_file = open(args.result_path, 'w')\n",
    "    ref_file = open(args.ref_path, 'w')\n",
    "\n",
    "    \n",
    "    latex_producer = LatexProducer(\n",
    "        model, vocab, max_len=args.max_len,\n",
    "        use_cuda=use_cuda, beam_size=args.beam_size)\n",
    "    \n",
    "    print('begin evulate ..')\n",
    "#     for imgs, tgt4training, tgt4cal_loss in tqdm(data_loader):\n",
    "    for imgs, tgt4training, tgt4cal_loss in data_loader:\n",
    "        \n",
    "        try:\n",
    "            reference = latex_producer._idx2formulas(tgt4cal_loss)\n",
    "            results = latex_producer(imgs)\n",
    "            print('results:', results)\n",
    "            print('tgt4cal_loss:', reference)\n",
    "        except RuntimeError:\n",
    "            break\n",
    "\n",
    "        result_file.write('\\n'.join(results))\n",
    "        ref_file.write('\\n'.join(reference))\n",
    "\n",
    "    result_file.close()\n",
    "    ref_file.close()\n",
    "    score = score_files(args.result_path, args.ref_path)\n",
    "    print(\"beam search result:\", score)\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:45:40.888809Z",
     "start_time": "2020-04-19T14:45:40.779875Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
