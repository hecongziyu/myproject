{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "# http://www.font5.com.cn/font_search.php?searchkey=%CB%CE%CC%E5&Submit=%D7%D6%CC%E5%CB%D1%CB%F7  字体库\n",
    "# https://github.com/BelBES/crnn-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "main_create_data() missing 2 required positional arguments: 'lmdb_path' and 'img_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dad41208dbef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# 根据生成文字图片，保存到lmdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain_create_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: main_create_data() missing 2 required positional arguments: 'lmdb_path' and 'img_path'"
     ]
    }
   ],
   "source": [
    "import lib.data.gen_data as gd\n",
    "import lib.data.dataset as ds\n",
    "import lib.data.char as char\n",
    "import importlib\n",
    "importlib.reload(gd)\n",
    "importlib.reload(char)\n",
    "path = 'D:/PROJECT_TW/git/data/ocr/'\n",
    "font_name = 'stsong'\n",
    "lmdb_path = 'D:/PROJECT_TW/git/data/ocr/lmdb'\n",
    "img_path = 'D:/PROJECT_TW/git/data/ocr/dataline'  \n",
    "# 生成文字图片\n",
    "gd.create_data(10,font_name=font_name, data_path=path)\n",
    "\n",
    "# 根据生成文字图片，保存到lmdb\n",
    "ds.main_create_data(lmdb_path,img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.data.lmdb_dataset as lds\n",
    "import torch\n",
    "train_path = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\ocr\\\\lmdb'\n",
    "batchSize = 10\n",
    "sampler = None\n",
    "workers = 1\n",
    "imgH = 32\n",
    "imgW = 256\n",
    "\n",
    "train_dataset = lds.lmdbDataset(root=train_path)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batchSize,\n",
    "    shuffle=True,\n",
    "    sampler=sampler,\n",
    "    num_workers=int(workers),\n",
    "    collate_fn=lds.alignCollate(imgH=imgH, imgW=imgW, keep_ratio=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle Environment objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-da5b01514d21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# image_0 = image[0][0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# image_0 = image_0.numpy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project_tw\\anly\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_DataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project_tw\\anly\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m  \u001b[1;31m# ensure that the worker exits on process exit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[0m_update_worker_pids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python36\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python36\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python36\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python36\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python36\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't pickle Environment objects"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "_,(image,label) = next(enumerate(train_loader))\n",
    "# image_0 = image[0][0]\n",
    "# image_0 = image_0.numpy()\n",
    "# print(label)\n",
    "# plt.imshow(image_0,'gray')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "\n",
    "# custom weights initialization called on crnn\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def data_parallel(model, input, ngpu):\n",
    "    if ngpu > 1 and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "        output = nn.parallel.data_parallel(model, input, range(ngpu))\n",
    "    else:\n",
    "        output = model(input)\n",
    "    return output\n",
    "    \n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, nIn, nHidden, nOut, ngpu):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = data_parallel(self.rnn, input,\n",
    "                                           self.ngpu)  # [T, b, h * 2]\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        output = data_parallel(self.embedding, \n",
    "                               t_rec,self.ngpu)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, nc, nclass, nh, ngpu, n_rnn=2, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nc if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "                                                                    \n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        # pool = nn.MaxPool2d(kernel_size=2, stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
    "        # 注意MaxPool2d 当stride = (2,1),表示只会根据H轴方向进行Pool，因为W方向是1。\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "        convRelu(6, True)  # 512x1x16\n",
    "\n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, nh, nh, ngpu),\n",
    "            BidirectionalLSTM(nh, nh, nclass, ngpu))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "#         print('input size --> {}'.format(input.size()))\n",
    "        conv = data_parallel(self.cnn, input, self.ngpu)\n",
    "        b, c, h, w = conv.size()\n",
    "#         print('conv out size --> {}'.format(conv.size()))\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        \n",
    "        # rnn features\n",
    "        output = data_parallel(self.rnn, conv, self.ngpu)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0 loss --> tensor([2.0093], grad_fn=<DivBackward0>)\n",
      "0:10 loss --> tensor([50.4357], grad_fn=<DivBackward0>)\n",
      "0:20 loss --> tensor([32.7192], grad_fn=<DivBackward0>)\n",
      "0:30 loss --> tensor([29.6999], grad_fn=<DivBackward0>)\n",
      "0:40 loss --> tensor([30.8185], grad_fn=<DivBackward0>)\n",
      "0:50 loss --> tensor([17.9923], grad_fn=<DivBackward0>)\n",
      "0:60 loss --> tensor([21.9073], grad_fn=<DivBackward0>)\n",
      "0:70 loss --> tensor([19.4085], grad_fn=<DivBackward0>)\n",
      "0:80 loss --> tensor([14.9581], grad_fn=<DivBackward0>)\n",
      "0:90 loss --> tensor([10.8917], grad_fn=<DivBackward0>)\n",
      "0:100 loss --> tensor([17.9379], grad_fn=<DivBackward0>)\n",
      "0:110 loss --> tensor([29.3240], grad_fn=<DivBackward0>)\n",
      "0:120 loss --> tensor([28.2609], grad_fn=<DivBackward0>)\n",
      "0:130 loss --> tensor([18.1639], grad_fn=<DivBackward0>)\n",
      "0:140 loss --> tensor([18.5935], grad_fn=<DivBackward0>)\n",
      "1:0 loss --> tensor([10.9817], grad_fn=<DivBackward0>)\n",
      "1:10 loss --> tensor([28.2779], grad_fn=<DivBackward0>)\n",
      "1:20 loss --> tensor([12.5013], grad_fn=<DivBackward0>)\n",
      "1:30 loss --> tensor([22.7310], grad_fn=<DivBackward0>)\n",
      "1:40 loss --> tensor([11.0990], grad_fn=<DivBackward0>)\n",
      "1:50 loss --> tensor([9.0520], grad_fn=<DivBackward0>)\n",
      "1:60 loss --> tensor([15.1589], grad_fn=<DivBackward0>)\n",
      "1:70 loss --> tensor([16.1022], grad_fn=<DivBackward0>)\n",
      "1:80 loss --> tensor([21.4937], grad_fn=<DivBackward0>)\n",
      "1:90 loss --> tensor([24.7296], grad_fn=<DivBackward0>)\n",
      "1:100 loss --> tensor([9.4360], grad_fn=<DivBackward0>)\n",
      "1:110 loss --> tensor([10.3729], grad_fn=<DivBackward0>)\n",
      "1:120 loss --> tensor([9.8497], grad_fn=<DivBackward0>)\n",
      "1:130 loss --> tensor([8.0697], grad_fn=<DivBackward0>)\n",
      "1:140 loss --> tensor([9.7845], grad_fn=<DivBackward0>)\n",
      "2:0 loss --> tensor([7.6384], grad_fn=<DivBackward0>)\n",
      "2:10 loss --> tensor([8.4288], grad_fn=<DivBackward0>)\n",
      "2:20 loss --> tensor([7.4787], grad_fn=<DivBackward0>)\n",
      "2:30 loss --> tensor([5.1242], grad_fn=<DivBackward0>)\n",
      "2:40 loss --> tensor([5.4999], grad_fn=<DivBackward0>)\n",
      "2:50 loss --> tensor([5.6282], grad_fn=<DivBackward0>)\n",
      "2:60 loss --> tensor([4.4453], grad_fn=<DivBackward0>)\n",
      "2:70 loss --> tensor([9.8130], grad_fn=<DivBackward0>)\n",
      "2:80 loss --> tensor([5.6961], grad_fn=<DivBackward0>)\n",
      "2:90 loss --> tensor([10.1451], grad_fn=<DivBackward0>)\n",
      "2:100 loss --> tensor([4.4921], grad_fn=<DivBackward0>)\n",
      "2:110 loss --> tensor([3.9594], grad_fn=<DivBackward0>)\n",
      "2:120 loss --> tensor([19.1258], grad_fn=<DivBackward0>)\n",
      "2:130 loss --> tensor([3.0576], grad_fn=<DivBackward0>)\n",
      "2:140 loss --> tensor([6.7883], grad_fn=<DivBackward0>)\n",
      "3:0 loss --> tensor([6.2436], grad_fn=<DivBackward0>)\n",
      "3:10 loss --> tensor([5.2135], grad_fn=<DivBackward0>)\n",
      "3:20 loss --> tensor([5.4170], grad_fn=<DivBackward0>)\n",
      "3:30 loss --> tensor([11.4397], grad_fn=<DivBackward0>)\n",
      "3:40 loss --> tensor([5.0069], grad_fn=<DivBackward0>)\n",
      "3:50 loss --> tensor([9.2299], grad_fn=<DivBackward0>)\n",
      "3:60 loss --> tensor([2.7963], grad_fn=<DivBackward0>)\n",
      "3:70 loss --> tensor([2.7076], grad_fn=<DivBackward0>)\n",
      "3:80 loss --> tensor([12.6714], grad_fn=<DivBackward0>)\n",
      "3:90 loss --> tensor([1.5503], grad_fn=<DivBackward0>)\n",
      "3:100 loss --> tensor([12.2525], grad_fn=<DivBackward0>)\n",
      "3:110 loss --> tensor([4.6674], grad_fn=<DivBackward0>)\n",
      "3:120 loss --> tensor([2.8498], grad_fn=<DivBackward0>)\n",
      "3:130 loss --> tensor([2.8989], grad_fn=<DivBackward0>)\n",
      "3:140 loss --> tensor([6.5617], grad_fn=<DivBackward0>)\n",
      "4:0 loss --> tensor([8.6493], grad_fn=<DivBackward0>)\n",
      "4:10 loss --> tensor([2.2715], grad_fn=<DivBackward0>)\n",
      "4:20 loss --> tensor([4.1219], grad_fn=<DivBackward0>)\n",
      "4:30 loss --> tensor([5.0218], grad_fn=<DivBackward0>)\n",
      "4:40 loss --> tensor([3.0168], grad_fn=<DivBackward0>)\n",
      "4:50 loss --> tensor([1.8082], grad_fn=<DivBackward0>)\n",
      "4:60 loss --> tensor([2.5561], grad_fn=<DivBackward0>)\n",
      "4:70 loss --> tensor([2.4951], grad_fn=<DivBackward0>)\n",
      "4:80 loss --> tensor([8.2852], grad_fn=<DivBackward0>)\n",
      "4:90 loss --> tensor([4.6325], grad_fn=<DivBackward0>)\n",
      "4:100 loss --> tensor([3.0762], grad_fn=<DivBackward0>)\n",
      "4:110 loss --> tensor([5.6552], grad_fn=<DivBackward0>)\n",
      "4:120 loss --> tensor([1.9165], grad_fn=<DivBackward0>)\n",
      "4:130 loss --> tensor([5.8676], grad_fn=<DivBackward0>)\n",
      "4:140 loss --> tensor([3.0931], grad_fn=<DivBackward0>)\n",
      "5:0 loss --> tensor([1.3279], grad_fn=<DivBackward0>)\n",
      "5:10 loss --> tensor([4.6664], grad_fn=<DivBackward0>)\n",
      "5:20 loss --> tensor([3.6021], grad_fn=<DivBackward0>)\n",
      "5:30 loss --> tensor([10.9734], grad_fn=<DivBackward0>)\n",
      "5:40 loss --> tensor([3.4750], grad_fn=<DivBackward0>)\n",
      "5:50 loss --> tensor([2.8559], grad_fn=<DivBackward0>)\n",
      "5:60 loss --> tensor([3.5577], grad_fn=<DivBackward0>)\n",
      "5:70 loss --> tensor([4.4905], grad_fn=<DivBackward0>)\n",
      "5:80 loss --> tensor([2.9750], grad_fn=<DivBackward0>)\n",
      "5:90 loss --> tensor([2.9978], grad_fn=<DivBackward0>)\n",
      "5:100 loss --> tensor([5.9025], grad_fn=<DivBackward0>)\n",
      "5:110 loss --> tensor([2.3720], grad_fn=<DivBackward0>)\n",
      "5:120 loss --> tensor([2.4343], grad_fn=<DivBackward0>)\n",
      "5:130 loss --> tensor([28.6452], grad_fn=<DivBackward0>)\n",
      "5:140 loss --> tensor([6.8061], grad_fn=<DivBackward0>)\n",
      "6:0 loss --> tensor([14.3352], grad_fn=<DivBackward0>)\n",
      "6:10 loss --> tensor([9.9875], grad_fn=<DivBackward0>)\n",
      "6:20 loss --> tensor([3.7326], grad_fn=<DivBackward0>)\n",
      "6:30 loss --> tensor([6.0018], grad_fn=<DivBackward0>)\n",
      "6:40 loss --> tensor([5.0011], grad_fn=<DivBackward0>)\n",
      "6:50 loss --> tensor([4.4327], grad_fn=<DivBackward0>)\n",
      "6:60 loss --> tensor([5.4890], grad_fn=<DivBackward0>)\n",
      "6:70 loss --> tensor([4.0963], grad_fn=<DivBackward0>)\n",
      "6:80 loss --> tensor([3.7195], grad_fn=<DivBackward0>)\n",
      "6:90 loss --> tensor([5.6513], grad_fn=<DivBackward0>)\n",
      "6:100 loss --> tensor([1.8962], grad_fn=<DivBackward0>)\n",
      "6:110 loss --> tensor([4.0270], grad_fn=<DivBackward0>)\n",
      "6:120 loss --> tensor([1.8950], grad_fn=<DivBackward0>)\n",
      "6:130 loss --> tensor([3.9196], grad_fn=<DivBackward0>)\n",
      "6:140 loss --> tensor([3.2349], grad_fn=<DivBackward0>)\n",
      "7:0 loss --> tensor([1.7780], grad_fn=<DivBackward0>)\n",
      "7:10 loss --> tensor([1.7843], grad_fn=<DivBackward0>)\n",
      "7:20 loss --> tensor([3.7219], grad_fn=<DivBackward0>)\n",
      "7:30 loss --> tensor([3.1673], grad_fn=<DivBackward0>)\n",
      "7:40 loss --> tensor([2.0574], grad_fn=<DivBackward0>)\n",
      "7:50 loss --> tensor([1.9338], grad_fn=<DivBackward0>)\n",
      "7:60 loss --> tensor([1.6865], grad_fn=<DivBackward0>)\n",
      "7:70 loss --> tensor([48.5906], grad_fn=<DivBackward0>)\n",
      "7:80 loss --> tensor([4.2701], grad_fn=<DivBackward0>)\n",
      "7:90 loss --> tensor([5.7861], grad_fn=<DivBackward0>)\n",
      "7:100 loss --> tensor([4.8526], grad_fn=<DivBackward0>)\n",
      "7:110 loss --> tensor([3.1499], grad_fn=<DivBackward0>)\n",
      "7:120 loss --> tensor([1.8716], grad_fn=<DivBackward0>)\n",
      "7:130 loss --> tensor([6.7886], grad_fn=<DivBackward0>)\n",
      "7:140 loss --> tensor([3.9944], grad_fn=<DivBackward0>)\n",
      "8:0 loss --> tensor([2.5778], grad_fn=<DivBackward0>)\n",
      "8:10 loss --> tensor([1.6472], grad_fn=<DivBackward0>)\n",
      "8:20 loss --> tensor([1.8341], grad_fn=<DivBackward0>)\n",
      "8:30 loss --> tensor([0.8777], grad_fn=<DivBackward0>)\n",
      "8:40 loss --> tensor([2.0932], grad_fn=<DivBackward0>)\n",
      "8:50 loss --> tensor([2.4232], grad_fn=<DivBackward0>)\n",
      "8:60 loss --> tensor([1.4747], grad_fn=<DivBackward0>)\n",
      "8:70 loss --> tensor([2.6397], grad_fn=<DivBackward0>)\n",
      "8:80 loss --> tensor([1.4562], grad_fn=<DivBackward0>)\n",
      "8:90 loss --> tensor([5.6277], grad_fn=<DivBackward0>)\n",
      "8:100 loss --> tensor([2.2702], grad_fn=<DivBackward0>)\n",
      "8:110 loss --> tensor([4.9212], grad_fn=<DivBackward0>)\n",
      "8:120 loss --> tensor([3.8063], grad_fn=<DivBackward0>)\n",
      "8:130 loss --> tensor([4.5863], grad_fn=<DivBackward0>)\n",
      "8:140 loss --> tensor([7.3029], grad_fn=<DivBackward0>)\n",
      "9:0 loss --> tensor([1.6593], grad_fn=<DivBackward0>)\n",
      "9:10 loss --> tensor([4.7982], grad_fn=<DivBackward0>)\n",
      "9:20 loss --> tensor([6.8615], grad_fn=<DivBackward0>)\n",
      "9:30 loss --> tensor([6.2552], grad_fn=<DivBackward0>)\n",
      "9:40 loss --> tensor([0.7976], grad_fn=<DivBackward0>)\n",
      "9:50 loss --> tensor([1.9928], grad_fn=<DivBackward0>)\n",
      "9:60 loss --> tensor([2.1863], grad_fn=<DivBackward0>)\n",
      "9:70 loss --> tensor([2.1109], grad_fn=<DivBackward0>)\n",
      "9:80 loss --> tensor([2.0646], grad_fn=<DivBackward0>)\n",
      "9:90 loss --> tensor([1.9391], grad_fn=<DivBackward0>)\n",
      "9:100 loss --> tensor([1.0768], grad_fn=<DivBackward0>)\n",
      "9:110 loss --> tensor([1.5237], grad_fn=<DivBackward0>)\n",
      "9:120 loss --> tensor([1.2981], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9:130 loss --> tensor([1.2131], grad_fn=<DivBackward0>)\n",
      "9:140 loss --> tensor([2.1582], grad_fn=<DivBackward0>)\n",
      "10:0 loss --> tensor([0.7925], grad_fn=<DivBackward0>)\n",
      "10:10 loss --> tensor([1.0252], grad_fn=<DivBackward0>)\n",
      "10:20 loss --> tensor([0.9202], grad_fn=<DivBackward0>)\n",
      "10:30 loss --> tensor([2.2983], grad_fn=<DivBackward0>)\n",
      "10:40 loss --> tensor([1.7586], grad_fn=<DivBackward0>)\n",
      "10:50 loss --> tensor([1.0025], grad_fn=<DivBackward0>)\n",
      "10:60 loss --> tensor([1.1002], grad_fn=<DivBackward0>)\n",
      "10:70 loss --> tensor([1.6339], grad_fn=<DivBackward0>)\n",
      "10:80 loss --> tensor([1.1685], grad_fn=<DivBackward0>)\n",
      "10:90 loss --> tensor([1.7231], grad_fn=<DivBackward0>)\n",
      "10:100 loss --> tensor([1.5969], grad_fn=<DivBackward0>)\n",
      "10:110 loss --> tensor([1.5072], grad_fn=<DivBackward0>)\n",
      "10:120 loss --> tensor([1.4141], grad_fn=<DivBackward0>)\n",
      "10:130 loss --> tensor([0.9110], grad_fn=<DivBackward0>)\n",
      "10:140 loss --> tensor([0.9626], grad_fn=<DivBackward0>)\n",
      "11:0 loss --> tensor([1.0419], grad_fn=<DivBackward0>)\n",
      "11:10 loss --> tensor([0.6547], grad_fn=<DivBackward0>)\n",
      "11:20 loss --> tensor([0.7174], grad_fn=<DivBackward0>)\n",
      "11:30 loss --> tensor([0.4620], grad_fn=<DivBackward0>)\n",
      "11:40 loss --> tensor([1.2781], grad_fn=<DivBackward0>)\n",
      "11:50 loss --> tensor([6.5450], grad_fn=<DivBackward0>)\n",
      "11:60 loss --> tensor([7.3881], grad_fn=<DivBackward0>)\n",
      "11:70 loss --> tensor([2.4166], grad_fn=<DivBackward0>)\n",
      "11:80 loss --> tensor([2.4075], grad_fn=<DivBackward0>)\n",
      "11:90 loss --> tensor([1.7167], grad_fn=<DivBackward0>)\n",
      "11:100 loss --> tensor([0.8139], grad_fn=<DivBackward0>)\n",
      "11:110 loss --> tensor([1.8643], grad_fn=<DivBackward0>)\n",
      "11:120 loss --> tensor([1.5442], grad_fn=<DivBackward0>)\n",
      "11:130 loss --> tensor([2.2063], grad_fn=<DivBackward0>)\n",
      "11:140 loss --> tensor([1.2729], grad_fn=<DivBackward0>)\n",
      "12:0 loss --> tensor([0.7332], grad_fn=<DivBackward0>)\n",
      "12:10 loss --> tensor([1.1601], grad_fn=<DivBackward0>)\n",
      "12:20 loss --> tensor([0.6064], grad_fn=<DivBackward0>)\n",
      "12:30 loss --> tensor([1.3589], grad_fn=<DivBackward0>)\n",
      "12:40 loss --> tensor([2.3803], grad_fn=<DivBackward0>)\n",
      "12:50 loss --> tensor([1.9258], grad_fn=<DivBackward0>)\n",
      "12:60 loss --> tensor([1.3479], grad_fn=<DivBackward0>)\n",
      "12:70 loss --> tensor([0.5424], grad_fn=<DivBackward0>)\n",
      "12:80 loss --> tensor([1.5136], grad_fn=<DivBackward0>)\n",
      "12:90 loss --> tensor([0.8215], grad_fn=<DivBackward0>)\n",
      "12:100 loss --> tensor([1.0844], grad_fn=<DivBackward0>)\n",
      "12:110 loss --> tensor([0.7729], grad_fn=<DivBackward0>)\n",
      "12:120 loss --> tensor([0.7835], grad_fn=<DivBackward0>)\n",
      "12:130 loss --> tensor([1.4401], grad_fn=<DivBackward0>)\n",
      "12:140 loss --> tensor([0.6424], grad_fn=<DivBackward0>)\n",
      "13:0 loss --> tensor([0.5060], grad_fn=<DivBackward0>)\n",
      "13:10 loss --> tensor([0.8054], grad_fn=<DivBackward0>)\n",
      "13:20 loss --> tensor([1.5803], grad_fn=<DivBackward0>)\n",
      "13:30 loss --> tensor([0.8691], grad_fn=<DivBackward0>)\n",
      "13:40 loss --> tensor([0.7022], grad_fn=<DivBackward0>)\n",
      "13:50 loss --> tensor([2.0343], grad_fn=<DivBackward0>)\n",
      "13:60 loss --> tensor([0.5828], grad_fn=<DivBackward0>)\n",
      "13:70 loss --> tensor([1.5815], grad_fn=<DivBackward0>)\n",
      "13:80 loss --> tensor([0.4141], grad_fn=<DivBackward0>)\n",
      "13:90 loss --> tensor([0.7840], grad_fn=<DivBackward0>)\n",
      "13:100 loss --> tensor([0.8207], grad_fn=<DivBackward0>)\n",
      "13:110 loss --> tensor([0.4739], grad_fn=<DivBackward0>)\n",
      "13:120 loss --> tensor([0.7440], grad_fn=<DivBackward0>)\n",
      "13:130 loss --> tensor([0.4984], grad_fn=<DivBackward0>)\n",
      "13:140 loss --> tensor([3.0751], grad_fn=<DivBackward0>)\n",
      "14:0 loss --> tensor([1.7687], grad_fn=<DivBackward0>)\n",
      "14:10 loss --> tensor([0.6994], grad_fn=<DivBackward0>)\n",
      "14:20 loss --> tensor([0.5382], grad_fn=<DivBackward0>)\n",
      "14:30 loss --> tensor([0.5635], grad_fn=<DivBackward0>)\n",
      "14:40 loss --> tensor([1.2344], grad_fn=<DivBackward0>)\n",
      "14:50 loss --> tensor([0.4917], grad_fn=<DivBackward0>)\n",
      "14:60 loss --> tensor([0.5344], grad_fn=<DivBackward0>)\n",
      "14:70 loss --> tensor([0.7996], grad_fn=<DivBackward0>)\n",
      "14:80 loss --> tensor([1.0381], grad_fn=<DivBackward0>)\n",
      "14:90 loss --> tensor([0.5895], grad_fn=<DivBackward0>)\n",
      "14:100 loss --> tensor([0.2939], grad_fn=<DivBackward0>)\n",
      "14:110 loss --> tensor([0.6370], grad_fn=<DivBackward0>)\n",
      "14:120 loss --> tensor([0.3712], grad_fn=<DivBackward0>)\n",
      "14:130 loss --> tensor([0.9777], grad_fn=<DivBackward0>)\n",
      "14:140 loss --> tensor([0.7015], grad_fn=<DivBackward0>)\n",
      "15:0 loss --> tensor([0.2519], grad_fn=<DivBackward0>)\n",
      "15:10 loss --> tensor([0.5328], grad_fn=<DivBackward0>)\n",
      "15:20 loss --> tensor([0.3816], grad_fn=<DivBackward0>)\n",
      "15:30 loss --> tensor([0.4990], grad_fn=<DivBackward0>)\n",
      "15:40 loss --> tensor([0.3379], grad_fn=<DivBackward0>)\n",
      "15:50 loss --> tensor([0.3161], grad_fn=<DivBackward0>)\n",
      "15:60 loss --> tensor([0.8670], grad_fn=<DivBackward0>)\n",
      "15:70 loss --> tensor([1.2586], grad_fn=<DivBackward0>)\n",
      "15:80 loss --> tensor([0.3472], grad_fn=<DivBackward0>)\n",
      "15:90 loss --> tensor([0.3999], grad_fn=<DivBackward0>)\n",
      "15:100 loss --> tensor([0.4283], grad_fn=<DivBackward0>)\n",
      "15:110 loss --> tensor([0.2552], grad_fn=<DivBackward0>)\n",
      "15:120 loss --> tensor([0.5391], grad_fn=<DivBackward0>)\n",
      "15:130 loss --> tensor([0.5850], grad_fn=<DivBackward0>)\n",
      "15:140 loss --> tensor([0.3354], grad_fn=<DivBackward0>)\n",
      "16:0 loss --> tensor([0.8459], grad_fn=<DivBackward0>)\n",
      "16:10 loss --> tensor([0.2451], grad_fn=<DivBackward0>)\n",
      "16:20 loss --> tensor([0.6537], grad_fn=<DivBackward0>)\n",
      "16:30 loss --> tensor([0.8570], grad_fn=<DivBackward0>)\n",
      "16:40 loss --> tensor([0.2404], grad_fn=<DivBackward0>)\n",
      "16:50 loss --> tensor([0.5455], grad_fn=<DivBackward0>)\n",
      "16:60 loss --> tensor([0.3966], grad_fn=<DivBackward0>)\n",
      "16:70 loss --> tensor([0.5658], grad_fn=<DivBackward0>)\n",
      "16:80 loss --> tensor([0.3174], grad_fn=<DivBackward0>)\n",
      "16:90 loss --> tensor([0.3947], grad_fn=<DivBackward0>)\n",
      "16:100 loss --> tensor([0.5657], grad_fn=<DivBackward0>)\n",
      "16:110 loss --> tensor([0.3207], grad_fn=<DivBackward0>)\n",
      "16:120 loss --> tensor([0.3135], grad_fn=<DivBackward0>)\n",
      "16:130 loss --> tensor([0.4019], grad_fn=<DivBackward0>)\n",
      "16:140 loss --> tensor([0.2041], grad_fn=<DivBackward0>)\n",
      "17:0 loss --> tensor([0.7636], grad_fn=<DivBackward0>)\n",
      "17:10 loss --> tensor([0.5931], grad_fn=<DivBackward0>)\n",
      "17:20 loss --> tensor([0.2087], grad_fn=<DivBackward0>)\n",
      "17:30 loss --> tensor([2.0180], grad_fn=<DivBackward0>)\n",
      "17:40 loss --> tensor([0.4556], grad_fn=<DivBackward0>)\n",
      "17:50 loss --> tensor([0.4006], grad_fn=<DivBackward0>)\n",
      "17:60 loss --> tensor([0.2948], grad_fn=<DivBackward0>)\n",
      "17:70 loss --> tensor([0.2974], grad_fn=<DivBackward0>)\n",
      "17:80 loss --> tensor([0.2688], grad_fn=<DivBackward0>)\n",
      "17:90 loss --> tensor([0.2111], grad_fn=<DivBackward0>)\n",
      "17:100 loss --> tensor([0.3759], grad_fn=<DivBackward0>)\n",
      "17:110 loss --> tensor([0.2699], grad_fn=<DivBackward0>)\n",
      "17:120 loss --> tensor([0.4306], grad_fn=<DivBackward0>)\n",
      "17:130 loss --> tensor([0.2390], grad_fn=<DivBackward0>)\n",
      "17:140 loss --> tensor([0.3765], grad_fn=<DivBackward0>)\n",
      "18:0 loss --> tensor([0.3233], grad_fn=<DivBackward0>)\n",
      "18:10 loss --> tensor([0.3022], grad_fn=<DivBackward0>)\n",
      "18:20 loss --> tensor([0.2504], grad_fn=<DivBackward0>)\n",
      "18:30 loss --> tensor([0.2636], grad_fn=<DivBackward0>)\n",
      "18:40 loss --> tensor([0.3139], grad_fn=<DivBackward0>)\n",
      "18:50 loss --> tensor([0.3870], grad_fn=<DivBackward0>)\n",
      "18:60 loss --> tensor([0.1966], grad_fn=<DivBackward0>)\n",
      "18:70 loss --> tensor([0.1337], grad_fn=<DivBackward0>)\n",
      "18:80 loss --> tensor([0.2214], grad_fn=<DivBackward0>)\n",
      "18:90 loss --> tensor([0.1435], grad_fn=<DivBackward0>)\n",
      "18:100 loss --> tensor([0.2211], grad_fn=<DivBackward0>)\n",
      "18:110 loss --> tensor([0.5994], grad_fn=<DivBackward0>)\n",
      "18:120 loss --> tensor([0.2253], grad_fn=<DivBackward0>)\n",
      "18:130 loss --> tensor([0.1877], grad_fn=<DivBackward0>)\n",
      "18:140 loss --> tensor([0.2907], grad_fn=<DivBackward0>)\n",
      "19:0 loss --> tensor([0.3078], grad_fn=<DivBackward0>)\n",
      "19:10 loss --> tensor([0.6215], grad_fn=<DivBackward0>)\n",
      "19:20 loss --> tensor([0.1777], grad_fn=<DivBackward0>)\n",
      "19:30 loss --> tensor([0.2519], grad_fn=<DivBackward0>)\n",
      "19:40 loss --> tensor([0.1926], grad_fn=<DivBackward0>)\n",
      "19:50 loss --> tensor([0.2600], grad_fn=<DivBackward0>)\n",
      "19:60 loss --> tensor([0.2780], grad_fn=<DivBackward0>)\n",
      "19:70 loss --> tensor([0.3205], grad_fn=<DivBackward0>)\n",
      "19:80 loss --> tensor([0.3586], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:90 loss --> tensor([0.2520], grad_fn=<DivBackward0>)\n",
      "19:100 loss --> tensor([0.4761], grad_fn=<DivBackward0>)\n",
      "19:110 loss --> tensor([0.4002], grad_fn=<DivBackward0>)\n",
      "19:120 loss --> tensor([0.2502], grad_fn=<DivBackward0>)\n",
      "19:130 loss --> tensor([0.3875], grad_fn=<DivBackward0>)\n",
      "19:140 loss --> tensor([0.4349], grad_fn=<DivBackward0>)\n",
      "20:0 loss --> tensor([0.2993], grad_fn=<DivBackward0>)\n",
      "20:10 loss --> tensor([0.2748], grad_fn=<DivBackward0>)\n",
      "20:20 loss --> tensor([0.3232], grad_fn=<DivBackward0>)\n",
      "20:30 loss --> tensor([0.2080], grad_fn=<DivBackward0>)\n",
      "20:40 loss --> tensor([0.6131], grad_fn=<DivBackward0>)\n",
      "20:50 loss --> tensor([0.2125], grad_fn=<DivBackward0>)\n",
      "20:60 loss --> tensor([0.3636], grad_fn=<DivBackward0>)\n",
      "20:70 loss --> tensor([0.1854], grad_fn=<DivBackward0>)\n",
      "20:80 loss --> tensor([0.1873], grad_fn=<DivBackward0>)\n",
      "20:90 loss --> tensor([0.2647], grad_fn=<DivBackward0>)\n",
      "20:100 loss --> tensor([0.2375], grad_fn=<DivBackward0>)\n",
      "20:110 loss --> tensor([0.2226], grad_fn=<DivBackward0>)\n",
      "20:120 loss --> tensor([0.3103], grad_fn=<DivBackward0>)\n",
      "20:130 loss --> tensor([0.2169], grad_fn=<DivBackward0>)\n",
      "20:140 loss --> tensor([0.2205], grad_fn=<DivBackward0>)\n",
      "21:0 loss --> tensor([0.1945], grad_fn=<DivBackward0>)\n",
      "21:10 loss --> tensor([0.1795], grad_fn=<DivBackward0>)\n",
      "21:20 loss --> tensor([0.2733], grad_fn=<DivBackward0>)\n",
      "21:30 loss --> tensor([0.5972], grad_fn=<DivBackward0>)\n",
      "21:40 loss --> tensor([0.2206], grad_fn=<DivBackward0>)\n",
      "21:50 loss --> tensor([0.5153], grad_fn=<DivBackward0>)\n",
      "21:60 loss --> tensor([0.2282], grad_fn=<DivBackward0>)\n",
      "21:70 loss --> tensor([0.1128], grad_fn=<DivBackward0>)\n",
      "21:80 loss --> tensor([0.3529], grad_fn=<DivBackward0>)\n",
      "21:90 loss --> tensor([0.1691], grad_fn=<DivBackward0>)\n",
      "21:100 loss --> tensor([0.8482], grad_fn=<DivBackward0>)\n",
      "21:110 loss --> tensor([0.5682], grad_fn=<DivBackward0>)\n",
      "21:120 loss --> tensor([0.2313], grad_fn=<DivBackward0>)\n",
      "21:130 loss --> tensor([0.3063], grad_fn=<DivBackward0>)\n",
      "21:140 loss --> tensor([0.1426], grad_fn=<DivBackward0>)\n",
      "22:0 loss --> tensor([0.4840], grad_fn=<DivBackward0>)\n",
      "22:10 loss --> tensor([0.1278], grad_fn=<DivBackward0>)\n",
      "22:20 loss --> tensor([0.5040], grad_fn=<DivBackward0>)\n",
      "22:30 loss --> tensor([0.2317], grad_fn=<DivBackward0>)\n",
      "22:40 loss --> tensor([0.1825], grad_fn=<DivBackward0>)\n",
      "22:50 loss --> tensor([0.1236], grad_fn=<DivBackward0>)\n",
      "22:60 loss --> tensor([0.1399], grad_fn=<DivBackward0>)\n",
      "22:70 loss --> tensor([0.6543], grad_fn=<DivBackward0>)\n",
      "22:80 loss --> tensor([0.2270], grad_fn=<DivBackward0>)\n",
      "22:90 loss --> tensor([0.1622], grad_fn=<DivBackward0>)\n",
      "22:100 loss --> tensor([0.3706], grad_fn=<DivBackward0>)\n",
      "22:110 loss --> tensor([0.2215], grad_fn=<DivBackward0>)\n",
      "22:120 loss --> tensor([0.1461], grad_fn=<DivBackward0>)\n",
      "22:130 loss --> tensor([0.3308], grad_fn=<DivBackward0>)\n",
      "22:140 loss --> tensor([0.6865], grad_fn=<DivBackward0>)\n",
      "23:0 loss --> tensor([0.4283], grad_fn=<DivBackward0>)\n",
      "23:10 loss --> tensor([0.1716], grad_fn=<DivBackward0>)\n",
      "23:20 loss --> tensor([0.1999], grad_fn=<DivBackward0>)\n",
      "23:30 loss --> tensor([0.2259], grad_fn=<DivBackward0>)\n",
      "23:40 loss --> tensor([0.6879], grad_fn=<DivBackward0>)\n",
      "23:50 loss --> tensor([0.6804], grad_fn=<DivBackward0>)\n",
      "23:60 loss --> tensor([0.6476], grad_fn=<DivBackward0>)\n",
      "23:70 loss --> tensor([0.2104], grad_fn=<DivBackward0>)\n",
      "23:80 loss --> tensor([0.1802], grad_fn=<DivBackward0>)\n",
      "23:90 loss --> tensor([0.1758], grad_fn=<DivBackward0>)\n",
      "23:100 loss --> tensor([0.2035], grad_fn=<DivBackward0>)\n",
      "23:110 loss --> tensor([0.3017], grad_fn=<DivBackward0>)\n",
      "23:120 loss --> tensor([0.1398], grad_fn=<DivBackward0>)\n",
      "23:130 loss --> tensor([0.1866], grad_fn=<DivBackward0>)\n",
      "23:140 loss --> tensor([0.2867], grad_fn=<DivBackward0>)\n",
      "24:0 loss --> tensor([0.2228], grad_fn=<DivBackward0>)\n",
      "24:10 loss --> tensor([0.2641], grad_fn=<DivBackward0>)\n",
      "24:20 loss --> tensor([0.4042], grad_fn=<DivBackward0>)\n",
      "24:30 loss --> tensor([0.1942], grad_fn=<DivBackward0>)\n",
      "24:40 loss --> tensor([0.2116], grad_fn=<DivBackward0>)\n",
      "24:50 loss --> tensor([0.1308], grad_fn=<DivBackward0>)\n",
      "24:60 loss --> tensor([0.2126], grad_fn=<DivBackward0>)\n",
      "24:70 loss --> tensor([0.3621], grad_fn=<DivBackward0>)\n",
      "24:80 loss --> tensor([0.1701], grad_fn=<DivBackward0>)\n",
      "24:90 loss --> tensor([0.6316], grad_fn=<DivBackward0>)\n",
      "24:100 loss --> tensor([0.1923], grad_fn=<DivBackward0>)\n",
      "24:110 loss --> tensor([0.2499], grad_fn=<DivBackward0>)\n",
      "24:120 loss --> tensor([0.1392], grad_fn=<DivBackward0>)\n",
      "24:130 loss --> tensor([0.1432], grad_fn=<DivBackward0>)\n",
      "24:140 loss --> tensor([0.3798], grad_fn=<DivBackward0>)\n",
      "25:0 loss --> tensor([0.4480], grad_fn=<DivBackward0>)\n",
      "25:10 loss --> tensor([0.1204], grad_fn=<DivBackward0>)\n",
      "25:20 loss --> tensor([0.1125], grad_fn=<DivBackward0>)\n",
      "25:30 loss --> tensor([0.2967], grad_fn=<DivBackward0>)\n",
      "25:40 loss --> tensor([0.1840], grad_fn=<DivBackward0>)\n",
      "25:50 loss --> tensor([0.5188], grad_fn=<DivBackward0>)\n",
      "25:60 loss --> tensor([0.1903], grad_fn=<DivBackward0>)\n",
      "25:70 loss --> tensor([0.3976], grad_fn=<DivBackward0>)\n",
      "25:80 loss --> tensor([0.5232], grad_fn=<DivBackward0>)\n",
      "25:90 loss --> tensor([0.1803], grad_fn=<DivBackward0>)\n",
      "25:100 loss --> tensor([0.1965], grad_fn=<DivBackward0>)\n",
      "25:110 loss --> tensor([0.1019], grad_fn=<DivBackward0>)\n",
      "25:120 loss --> tensor([0.2039], grad_fn=<DivBackward0>)\n",
      "25:130 loss --> tensor([0.4120], grad_fn=<DivBackward0>)\n",
      "25:140 loss --> tensor([0.1892], grad_fn=<DivBackward0>)\n",
      "26:0 loss --> tensor([0.2924], grad_fn=<DivBackward0>)\n",
      "26:10 loss --> tensor([0.1450], grad_fn=<DivBackward0>)\n",
      "26:20 loss --> tensor([0.1686], grad_fn=<DivBackward0>)\n",
      "26:30 loss --> tensor([0.1820], grad_fn=<DivBackward0>)\n",
      "26:40 loss --> tensor([0.0995], grad_fn=<DivBackward0>)\n",
      "26:50 loss --> tensor([0.1749], grad_fn=<DivBackward0>)\n",
      "26:60 loss --> tensor([0.1148], grad_fn=<DivBackward0>)\n",
      "26:70 loss --> tensor([0.3823], grad_fn=<DivBackward0>)\n",
      "26:80 loss --> tensor([0.1242], grad_fn=<DivBackward0>)\n",
      "26:90 loss --> tensor([0.1387], grad_fn=<DivBackward0>)\n",
      "26:100 loss --> tensor([0.1987], grad_fn=<DivBackward0>)\n",
      "26:110 loss --> tensor([0.2785], grad_fn=<DivBackward0>)\n",
      "26:120 loss --> tensor([0.7383], grad_fn=<DivBackward0>)\n",
      "26:130 loss --> tensor([0.1110], grad_fn=<DivBackward0>)\n",
      "26:140 loss --> tensor([0.2265], grad_fn=<DivBackward0>)\n",
      "27:0 loss --> tensor([0.2480], grad_fn=<DivBackward0>)\n",
      "27:10 loss --> tensor([0.1217], grad_fn=<DivBackward0>)\n",
      "27:20 loss --> tensor([0.5772], grad_fn=<DivBackward0>)\n",
      "27:30 loss --> tensor([0.1190], grad_fn=<DivBackward0>)\n",
      "27:40 loss --> tensor([0.1580], grad_fn=<DivBackward0>)\n",
      "27:50 loss --> tensor([0.3171], grad_fn=<DivBackward0>)\n",
      "27:60 loss --> tensor([0.1394], grad_fn=<DivBackward0>)\n",
      "27:70 loss --> tensor([0.1429], grad_fn=<DivBackward0>)\n",
      "27:80 loss --> tensor([0.1151], grad_fn=<DivBackward0>)\n",
      "27:90 loss --> tensor([0.2224], grad_fn=<DivBackward0>)\n",
      "27:100 loss --> tensor([0.2588], grad_fn=<DivBackward0>)\n",
      "27:110 loss --> tensor([0.0994], grad_fn=<DivBackward0>)\n",
      "27:120 loss --> tensor([0.1347], grad_fn=<DivBackward0>)\n",
      "27:130 loss --> tensor([0.1507], grad_fn=<DivBackward0>)\n",
      "27:140 loss --> tensor([0.1248], grad_fn=<DivBackward0>)\n",
      "28:0 loss --> tensor([0.1605], grad_fn=<DivBackward0>)\n",
      "28:10 loss --> tensor([0.1131], grad_fn=<DivBackward0>)\n",
      "28:20 loss --> tensor([0.1793], grad_fn=<DivBackward0>)\n",
      "28:30 loss --> tensor([0.1291], grad_fn=<DivBackward0>)\n",
      "28:40 loss --> tensor([0.1934], grad_fn=<DivBackward0>)\n",
      "28:50 loss --> tensor([0.1760], grad_fn=<DivBackward0>)\n",
      "28:60 loss --> tensor([0.1438], grad_fn=<DivBackward0>)\n",
      "28:70 loss --> tensor([0.1467], grad_fn=<DivBackward0>)\n",
      "28:80 loss --> tensor([0.2403], grad_fn=<DivBackward0>)\n",
      "28:90 loss --> tensor([0.2009], grad_fn=<DivBackward0>)\n",
      "28:100 loss --> tensor([0.1159], grad_fn=<DivBackward0>)\n",
      "28:110 loss --> tensor([0.1700], grad_fn=<DivBackward0>)\n",
      "28:120 loss --> tensor([0.3202], grad_fn=<DivBackward0>)\n",
      "28:130 loss --> tensor([0.1458], grad_fn=<DivBackward0>)\n",
      "28:140 loss --> tensor([0.2102], grad_fn=<DivBackward0>)\n",
      "29:0 loss --> tensor([0.1132], grad_fn=<DivBackward0>)\n",
      "29:10 loss --> tensor([0.1841], grad_fn=<DivBackward0>)\n",
      "29:20 loss --> tensor([0.1211], grad_fn=<DivBackward0>)\n",
      "29:30 loss --> tensor([0.2081], grad_fn=<DivBackward0>)\n",
      "29:40 loss --> tensor([0.1610], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29:50 loss --> tensor([0.1220], grad_fn=<DivBackward0>)\n",
      "29:60 loss --> tensor([0.1549], grad_fn=<DivBackward0>)\n",
      "29:70 loss --> tensor([0.1106], grad_fn=<DivBackward0>)\n",
      "29:80 loss --> tensor([0.1326], grad_fn=<DivBackward0>)\n",
      "29:90 loss --> tensor([0.1022], grad_fn=<DivBackward0>)\n",
      "29:100 loss --> tensor([0.2556], grad_fn=<DivBackward0>)\n",
      "29:110 loss --> tensor([0.0985], grad_fn=<DivBackward0>)\n",
      "29:120 loss --> tensor([0.1253], grad_fn=<DivBackward0>)\n",
      "29:130 loss --> tensor([0.1685], grad_fn=<DivBackward0>)\n",
      "29:140 loss --> tensor([0.2129], grad_fn=<DivBackward0>)\n",
      "30:0 loss --> tensor([0.3538], grad_fn=<DivBackward0>)\n",
      "30:10 loss --> tensor([0.1281], grad_fn=<DivBackward0>)\n",
      "30:20 loss --> tensor([0.1093], grad_fn=<DivBackward0>)\n",
      "30:30 loss --> tensor([0.1004], grad_fn=<DivBackward0>)\n",
      "30:40 loss --> tensor([0.1302], grad_fn=<DivBackward0>)\n",
      "30:50 loss --> tensor([0.2209], grad_fn=<DivBackward0>)\n",
      "30:60 loss --> tensor([0.2022], grad_fn=<DivBackward0>)\n",
      "30:70 loss --> tensor([0.1259], grad_fn=<DivBackward0>)\n",
      "30:80 loss --> tensor([0.1509], grad_fn=<DivBackward0>)\n",
      "30:90 loss --> tensor([0.0927], grad_fn=<DivBackward0>)\n",
      "30:100 loss --> tensor([0.1401], grad_fn=<DivBackward0>)\n",
      "30:110 loss --> tensor([0.1174], grad_fn=<DivBackward0>)\n",
      "30:120 loss --> tensor([0.1314], grad_fn=<DivBackward0>)\n",
      "30:130 loss --> tensor([0.1074], grad_fn=<DivBackward0>)\n",
      "30:140 loss --> tensor([0.0943], grad_fn=<DivBackward0>)\n",
      "31:0 loss --> tensor([0.1456], grad_fn=<DivBackward0>)\n",
      "31:10 loss --> tensor([0.1727], grad_fn=<DivBackward0>)\n",
      "31:20 loss --> tensor([0.1765], grad_fn=<DivBackward0>)\n",
      "31:30 loss --> tensor([0.2947], grad_fn=<DivBackward0>)\n",
      "31:40 loss --> tensor([0.1631], grad_fn=<DivBackward0>)\n",
      "31:50 loss --> tensor([0.1256], grad_fn=<DivBackward0>)\n",
      "31:60 loss --> tensor([0.1593], grad_fn=<DivBackward0>)\n",
      "31:70 loss --> tensor([0.1158], grad_fn=<DivBackward0>)\n",
      "31:80 loss --> tensor([0.0981], grad_fn=<DivBackward0>)\n",
      "31:90 loss --> tensor([0.7890], grad_fn=<DivBackward0>)\n",
      "31:100 loss --> tensor([0.2218], grad_fn=<DivBackward0>)\n",
      "31:110 loss --> tensor([0.1219], grad_fn=<DivBackward0>)\n",
      "31:120 loss --> tensor([0.1342], grad_fn=<DivBackward0>)\n",
      "31:130 loss --> tensor([0.1283], grad_fn=<DivBackward0>)\n",
      "31:140 loss --> tensor([0.3037], grad_fn=<DivBackward0>)\n",
      "32:0 loss --> tensor([0.0918], grad_fn=<DivBackward0>)\n",
      "32:10 loss --> tensor([0.1039], grad_fn=<DivBackward0>)\n",
      "32:20 loss --> tensor([0.0852], grad_fn=<DivBackward0>)\n",
      "32:30 loss --> tensor([0.2435], grad_fn=<DivBackward0>)\n",
      "32:40 loss --> tensor([0.1182], grad_fn=<DivBackward0>)\n",
      "32:50 loss --> tensor([0.2883], grad_fn=<DivBackward0>)\n",
      "32:60 loss --> tensor([0.1487], grad_fn=<DivBackward0>)\n",
      "32:70 loss --> tensor([0.3298], grad_fn=<DivBackward0>)\n",
      "32:80 loss --> tensor([0.2597], grad_fn=<DivBackward0>)\n",
      "32:90 loss --> tensor([0.2056], grad_fn=<DivBackward0>)\n",
      "32:100 loss --> tensor([0.2085], grad_fn=<DivBackward0>)\n",
      "32:110 loss --> tensor([0.0967], grad_fn=<DivBackward0>)\n",
      "32:120 loss --> tensor([0.2013], grad_fn=<DivBackward0>)\n",
      "32:130 loss --> tensor([0.0907], grad_fn=<DivBackward0>)\n",
      "32:140 loss --> tensor([0.0935], grad_fn=<DivBackward0>)\n",
      "33:0 loss --> tensor([0.1353], grad_fn=<DivBackward0>)\n",
      "33:10 loss --> tensor([0.1394], grad_fn=<DivBackward0>)\n",
      "33:20 loss --> tensor([0.0904], grad_fn=<DivBackward0>)\n",
      "33:30 loss --> tensor([0.1138], grad_fn=<DivBackward0>)\n",
      "33:40 loss --> tensor([0.0739], grad_fn=<DivBackward0>)\n",
      "33:50 loss --> tensor([0.1927], grad_fn=<DivBackward0>)\n",
      "33:60 loss --> tensor([0.1201], grad_fn=<DivBackward0>)\n",
      "33:70 loss --> tensor([0.0894], grad_fn=<DivBackward0>)\n",
      "33:80 loss --> tensor([0.2045], grad_fn=<DivBackward0>)\n",
      "33:90 loss --> tensor([0.2226], grad_fn=<DivBackward0>)\n",
      "33:100 loss --> tensor([0.2642], grad_fn=<DivBackward0>)\n",
      "33:110 loss --> tensor([0.0814], grad_fn=<DivBackward0>)\n",
      "33:120 loss --> tensor([0.1246], grad_fn=<DivBackward0>)\n",
      "33:130 loss --> tensor([0.1243], grad_fn=<DivBackward0>)\n",
      "33:140 loss --> tensor([0.2314], grad_fn=<DivBackward0>)\n",
      "34:0 loss --> tensor([0.1745], grad_fn=<DivBackward0>)\n",
      "34:10 loss --> tensor([0.0981], grad_fn=<DivBackward0>)\n",
      "34:20 loss --> tensor([0.2813], grad_fn=<DivBackward0>)\n",
      "34:30 loss --> tensor([0.2197], grad_fn=<DivBackward0>)\n",
      "34:40 loss --> tensor([0.2510], grad_fn=<DivBackward0>)\n",
      "34:50 loss --> tensor([0.0774], grad_fn=<DivBackward0>)\n",
      "34:60 loss --> tensor([0.1177], grad_fn=<DivBackward0>)\n",
      "34:70 loss --> tensor([0.1260], grad_fn=<DivBackward0>)\n",
      "34:80 loss --> tensor([0.0923], grad_fn=<DivBackward0>)\n",
      "34:90 loss --> tensor([0.0764], grad_fn=<DivBackward0>)\n",
      "34:100 loss --> tensor([0.1421], grad_fn=<DivBackward0>)\n",
      "34:110 loss --> tensor([0.0935], grad_fn=<DivBackward0>)\n",
      "34:120 loss --> tensor([0.0772], grad_fn=<DivBackward0>)\n",
      "34:130 loss --> tensor([0.2435], grad_fn=<DivBackward0>)\n",
      "34:140 loss --> tensor([0.2277], grad_fn=<DivBackward0>)\n",
      "35:0 loss --> tensor([0.8505], grad_fn=<DivBackward0>)\n",
      "35:10 loss --> tensor([0.3195], grad_fn=<DivBackward0>)\n",
      "35:20 loss --> tensor([0.1292], grad_fn=<DivBackward0>)\n",
      "35:30 loss --> tensor([0.0888], grad_fn=<DivBackward0>)\n",
      "35:40 loss --> tensor([0.0875], grad_fn=<DivBackward0>)\n",
      "35:50 loss --> tensor([0.0966], grad_fn=<DivBackward0>)\n",
      "35:60 loss --> tensor([0.1363], grad_fn=<DivBackward0>)\n",
      "35:70 loss --> tensor([0.1410], grad_fn=<DivBackward0>)\n",
      "35:80 loss --> tensor([0.1186], grad_fn=<DivBackward0>)\n",
      "35:90 loss --> tensor([0.3423], grad_fn=<DivBackward0>)\n",
      "35:100 loss --> tensor([0.1131], grad_fn=<DivBackward0>)\n",
      "35:110 loss --> tensor([0.2190], grad_fn=<DivBackward0>)\n",
      "35:120 loss --> tensor([0.1662], grad_fn=<DivBackward0>)\n",
      "35:130 loss --> tensor([0.2361], grad_fn=<DivBackward0>)\n",
      "35:140 loss --> tensor([0.1854], grad_fn=<DivBackward0>)\n",
      "36:0 loss --> tensor([0.0795], grad_fn=<DivBackward0>)\n",
      "36:10 loss --> tensor([0.0690], grad_fn=<DivBackward0>)\n",
      "36:20 loss --> tensor([0.1189], grad_fn=<DivBackward0>)\n",
      "36:30 loss --> tensor([0.1005], grad_fn=<DivBackward0>)\n",
      "36:40 loss --> tensor([0.0604], grad_fn=<DivBackward0>)\n",
      "36:50 loss --> tensor([0.0631], grad_fn=<DivBackward0>)\n",
      "36:60 loss --> tensor([0.0948], grad_fn=<DivBackward0>)\n",
      "36:70 loss --> tensor([0.1114], grad_fn=<DivBackward0>)\n",
      "36:80 loss --> tensor([0.0975], grad_fn=<DivBackward0>)\n",
      "36:90 loss --> tensor([0.1018], grad_fn=<DivBackward0>)\n",
      "36:100 loss --> tensor([0.0887], grad_fn=<DivBackward0>)\n",
      "36:110 loss --> tensor([0.1100], grad_fn=<DivBackward0>)\n",
      "36:120 loss --> tensor([0.2512], grad_fn=<DivBackward0>)\n",
      "36:130 loss --> tensor([0.0786], grad_fn=<DivBackward0>)\n",
      "36:140 loss --> tensor([0.0960], grad_fn=<DivBackward0>)\n",
      "37:0 loss --> tensor([0.0916], grad_fn=<DivBackward0>)\n",
      "37:10 loss --> tensor([0.1330], grad_fn=<DivBackward0>)\n",
      "37:20 loss --> tensor([0.0927], grad_fn=<DivBackward0>)\n",
      "37:30 loss --> tensor([0.1012], grad_fn=<DivBackward0>)\n",
      "37:40 loss --> tensor([0.0704], grad_fn=<DivBackward0>)\n",
      "37:50 loss --> tensor([0.2084], grad_fn=<DivBackward0>)\n",
      "37:60 loss --> tensor([0.1598], grad_fn=<DivBackward0>)\n",
      "37:70 loss --> tensor([0.3623], grad_fn=<DivBackward0>)\n",
      "37:80 loss --> tensor([0.1091], grad_fn=<DivBackward0>)\n",
      "37:90 loss --> tensor([0.1809], grad_fn=<DivBackward0>)\n",
      "37:100 loss --> tensor([0.0755], grad_fn=<DivBackward0>)\n",
      "37:110 loss --> tensor([0.1033], grad_fn=<DivBackward0>)\n",
      "37:120 loss --> tensor([0.2436], grad_fn=<DivBackward0>)\n",
      "37:130 loss --> tensor([0.1593], grad_fn=<DivBackward0>)\n",
      "37:140 loss --> tensor([0.0765], grad_fn=<DivBackward0>)\n",
      "38:0 loss --> tensor([0.2501], grad_fn=<DivBackward0>)\n",
      "38:10 loss --> tensor([0.2694], grad_fn=<DivBackward0>)\n",
      "38:20 loss --> tensor([0.1022], grad_fn=<DivBackward0>)\n",
      "38:30 loss --> tensor([0.0717], grad_fn=<DivBackward0>)\n",
      "38:40 loss --> tensor([0.0709], grad_fn=<DivBackward0>)\n",
      "38:50 loss --> tensor([0.1377], grad_fn=<DivBackward0>)\n",
      "38:60 loss --> tensor([0.0838], grad_fn=<DivBackward0>)\n",
      "38:70 loss --> tensor([0.1077], grad_fn=<DivBackward0>)\n",
      "38:80 loss --> tensor([0.0823], grad_fn=<DivBackward0>)\n",
      "38:90 loss --> tensor([0.0821], grad_fn=<DivBackward0>)\n",
      "38:100 loss --> tensor([0.0958], grad_fn=<DivBackward0>)\n",
      "38:110 loss --> tensor([0.0818], grad_fn=<DivBackward0>)\n",
      "38:120 loss --> tensor([0.1386], grad_fn=<DivBackward0>)\n",
      "38:130 loss --> tensor([0.1328], grad_fn=<DivBackward0>)\n",
      "38:140 loss --> tensor([0.1798], grad_fn=<DivBackward0>)\n",
      "39:0 loss --> tensor([0.2380], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39:10 loss --> tensor([0.1860], grad_fn=<DivBackward0>)\n",
      "39:20 loss --> tensor([0.0797], grad_fn=<DivBackward0>)\n",
      "39:30 loss --> tensor([0.0885], grad_fn=<DivBackward0>)\n",
      "39:40 loss --> tensor([0.1416], grad_fn=<DivBackward0>)\n",
      "39:50 loss --> tensor([0.1131], grad_fn=<DivBackward0>)\n",
      "39:60 loss --> tensor([0.1277], grad_fn=<DivBackward0>)\n",
      "39:70 loss --> tensor([0.0795], grad_fn=<DivBackward0>)\n",
      "39:80 loss --> tensor([0.0667], grad_fn=<DivBackward0>)\n",
      "39:90 loss --> tensor([0.1610], grad_fn=<DivBackward0>)\n",
      "39:100 loss --> tensor([0.0903], grad_fn=<DivBackward0>)\n",
      "39:110 loss --> tensor([0.0821], grad_fn=<DivBackward0>)\n",
      "39:120 loss --> tensor([0.0649], grad_fn=<DivBackward0>)\n",
      "39:130 loss --> tensor([0.1426], grad_fn=<DivBackward0>)\n",
      "39:140 loss --> tensor([0.0893], grad_fn=<DivBackward0>)\n",
      "40:0 loss --> tensor([0.0796], grad_fn=<DivBackward0>)\n",
      "40:10 loss --> tensor([0.1631], grad_fn=<DivBackward0>)\n",
      "40:20 loss --> tensor([0.1261], grad_fn=<DivBackward0>)\n",
      "40:30 loss --> tensor([0.0960], grad_fn=<DivBackward0>)\n",
      "40:40 loss --> tensor([0.0792], grad_fn=<DivBackward0>)\n",
      "40:50 loss --> tensor([0.0782], grad_fn=<DivBackward0>)\n",
      "40:60 loss --> tensor([0.0512], grad_fn=<DivBackward0>)\n",
      "40:70 loss --> tensor([0.0788], grad_fn=<DivBackward0>)\n",
      "40:80 loss --> tensor([0.0916], grad_fn=<DivBackward0>)\n",
      "40:90 loss --> tensor([0.0782], grad_fn=<DivBackward0>)\n",
      "40:100 loss --> tensor([0.1431], grad_fn=<DivBackward0>)\n",
      "40:110 loss --> tensor([0.1095], grad_fn=<DivBackward0>)\n",
      "40:120 loss --> tensor([0.2681], grad_fn=<DivBackward0>)\n",
      "40:130 loss --> tensor([0.0814], grad_fn=<DivBackward0>)\n",
      "40:140 loss --> tensor([0.1379], grad_fn=<DivBackward0>)\n",
      "41:0 loss --> tensor([0.1477], grad_fn=<DivBackward0>)\n",
      "41:10 loss --> tensor([0.0723], grad_fn=<DivBackward0>)\n",
      "41:20 loss --> tensor([0.1357], grad_fn=<DivBackward0>)\n",
      "41:30 loss --> tensor([0.0880], grad_fn=<DivBackward0>)\n",
      "41:40 loss --> tensor([0.1157], grad_fn=<DivBackward0>)\n",
      "41:50 loss --> tensor([0.1320], grad_fn=<DivBackward0>)\n",
      "41:60 loss --> tensor([0.1032], grad_fn=<DivBackward0>)\n",
      "41:70 loss --> tensor([0.0720], grad_fn=<DivBackward0>)\n",
      "41:80 loss --> tensor([0.1126], grad_fn=<DivBackward0>)\n",
      "41:90 loss --> tensor([0.1883], grad_fn=<DivBackward0>)\n",
      "41:100 loss --> tensor([0.1225], grad_fn=<DivBackward0>)\n",
      "41:110 loss --> tensor([0.0845], grad_fn=<DivBackward0>)\n",
      "41:120 loss --> tensor([0.8526], grad_fn=<DivBackward0>)\n",
      "41:130 loss --> tensor([0.1468], grad_fn=<DivBackward0>)\n",
      "41:140 loss --> tensor([0.1140], grad_fn=<DivBackward0>)\n",
      "42:0 loss --> tensor([0.1035], grad_fn=<DivBackward0>)\n",
      "42:10 loss --> tensor([0.2274], grad_fn=<DivBackward0>)\n",
      "42:20 loss --> tensor([0.0948], grad_fn=<DivBackward0>)\n",
      "42:30 loss --> tensor([0.0849], grad_fn=<DivBackward0>)\n",
      "42:40 loss --> tensor([0.0620], grad_fn=<DivBackward0>)\n",
      "42:50 loss --> tensor([0.1077], grad_fn=<DivBackward0>)\n",
      "42:60 loss --> tensor([0.0729], grad_fn=<DivBackward0>)\n",
      "42:70 loss --> tensor([0.0872], grad_fn=<DivBackward0>)\n",
      "42:80 loss --> tensor([0.0840], grad_fn=<DivBackward0>)\n",
      "42:90 loss --> tensor([0.1650], grad_fn=<DivBackward0>)\n",
      "42:100 loss --> tensor([0.1066], grad_fn=<DivBackward0>)\n",
      "42:110 loss --> tensor([0.0717], grad_fn=<DivBackward0>)\n",
      "42:120 loss --> tensor([0.0753], grad_fn=<DivBackward0>)\n",
      "42:130 loss --> tensor([0.1158], grad_fn=<DivBackward0>)\n",
      "42:140 loss --> tensor([0.1334], grad_fn=<DivBackward0>)\n",
      "43:0 loss --> tensor([0.1250], grad_fn=<DivBackward0>)\n",
      "43:10 loss --> tensor([0.0933], grad_fn=<DivBackward0>)\n",
      "43:20 loss --> tensor([0.0740], grad_fn=<DivBackward0>)\n",
      "43:30 loss --> tensor([0.0433], grad_fn=<DivBackward0>)\n",
      "43:40 loss --> tensor([0.1369], grad_fn=<DivBackward0>)\n",
      "43:50 loss --> tensor([0.0876], grad_fn=<DivBackward0>)\n",
      "43:60 loss --> tensor([0.0798], grad_fn=<DivBackward0>)\n",
      "43:70 loss --> tensor([0.1634], grad_fn=<DivBackward0>)\n",
      "43:80 loss --> tensor([0.1059], grad_fn=<DivBackward0>)\n",
      "43:90 loss --> tensor([0.2038], grad_fn=<DivBackward0>)\n",
      "43:100 loss --> tensor([0.0500], grad_fn=<DivBackward0>)\n",
      "43:110 loss --> tensor([0.0824], grad_fn=<DivBackward0>)\n",
      "43:120 loss --> tensor([0.0936], grad_fn=<DivBackward0>)\n",
      "43:130 loss --> tensor([0.1075], grad_fn=<DivBackward0>)\n",
      "43:140 loss --> tensor([0.0614], grad_fn=<DivBackward0>)\n",
      "44:0 loss --> tensor([0.0958], grad_fn=<DivBackward0>)\n",
      "44:10 loss --> tensor([0.0826], grad_fn=<DivBackward0>)\n",
      "44:20 loss --> tensor([0.1438], grad_fn=<DivBackward0>)\n",
      "44:30 loss --> tensor([0.0915], grad_fn=<DivBackward0>)\n",
      "44:40 loss --> tensor([0.0857], grad_fn=<DivBackward0>)\n",
      "44:50 loss --> tensor([0.1806], grad_fn=<DivBackward0>)\n",
      "44:60 loss --> tensor([0.2760], grad_fn=<DivBackward0>)\n",
      "44:70 loss --> tensor([0.0694], grad_fn=<DivBackward0>)\n",
      "44:80 loss --> tensor([0.0534], grad_fn=<DivBackward0>)\n",
      "44:90 loss --> tensor([0.0753], grad_fn=<DivBackward0>)\n",
      "44:100 loss --> tensor([0.1193], grad_fn=<DivBackward0>)\n",
      "44:110 loss --> tensor([0.1304], grad_fn=<DivBackward0>)\n",
      "44:120 loss --> tensor([0.0840], grad_fn=<DivBackward0>)\n",
      "44:130 loss --> tensor([0.1864], grad_fn=<DivBackward0>)\n",
      "44:140 loss --> tensor([0.0735], grad_fn=<DivBackward0>)\n",
      "45:0 loss --> tensor([0.0708], grad_fn=<DivBackward0>)\n",
      "45:10 loss --> tensor([0.1066], grad_fn=<DivBackward0>)\n",
      "45:20 loss --> tensor([0.0623], grad_fn=<DivBackward0>)\n",
      "45:30 loss --> tensor([0.0620], grad_fn=<DivBackward0>)\n",
      "45:40 loss --> tensor([0.1462], grad_fn=<DivBackward0>)\n",
      "45:50 loss --> tensor([0.1526], grad_fn=<DivBackward0>)\n",
      "45:60 loss --> tensor([0.0667], grad_fn=<DivBackward0>)\n",
      "45:70 loss --> tensor([0.1695], grad_fn=<DivBackward0>)\n",
      "45:80 loss --> tensor([0.1129], grad_fn=<DivBackward0>)\n",
      "45:90 loss --> tensor([0.2218], grad_fn=<DivBackward0>)\n",
      "45:100 loss --> tensor([0.0627], grad_fn=<DivBackward0>)\n",
      "45:110 loss --> tensor([0.0886], grad_fn=<DivBackward0>)\n",
      "45:120 loss --> tensor([0.0750], grad_fn=<DivBackward0>)\n",
      "45:130 loss --> tensor([0.0651], grad_fn=<DivBackward0>)\n",
      "45:140 loss --> tensor([0.0940], grad_fn=<DivBackward0>)\n",
      "46:0 loss --> tensor([0.0821], grad_fn=<DivBackward0>)\n",
      "46:10 loss --> tensor([0.0972], grad_fn=<DivBackward0>)\n",
      "46:20 loss --> tensor([0.0588], grad_fn=<DivBackward0>)\n",
      "46:30 loss --> tensor([0.1152], grad_fn=<DivBackward0>)\n",
      "46:40 loss --> tensor([0.1022], grad_fn=<DivBackward0>)\n",
      "46:50 loss --> tensor([0.0705], grad_fn=<DivBackward0>)\n",
      "46:60 loss --> tensor([0.0673], grad_fn=<DivBackward0>)\n",
      "46:70 loss --> tensor([0.1202], grad_fn=<DivBackward0>)\n",
      "46:80 loss --> tensor([0.0818], grad_fn=<DivBackward0>)\n",
      "46:90 loss --> tensor([0.1059], grad_fn=<DivBackward0>)\n",
      "46:100 loss --> tensor([0.0519], grad_fn=<DivBackward0>)\n",
      "46:110 loss --> tensor([0.0568], grad_fn=<DivBackward0>)\n",
      "46:120 loss --> tensor([0.0916], grad_fn=<DivBackward0>)\n",
      "46:130 loss --> tensor([0.0947], grad_fn=<DivBackward0>)\n",
      "46:140 loss --> tensor([0.0787], grad_fn=<DivBackward0>)\n",
      "47:0 loss --> tensor([0.1555], grad_fn=<DivBackward0>)\n",
      "47:10 loss --> tensor([0.0528], grad_fn=<DivBackward0>)\n",
      "47:20 loss --> tensor([0.1205], grad_fn=<DivBackward0>)\n",
      "47:30 loss --> tensor([0.1854], grad_fn=<DivBackward0>)\n",
      "47:40 loss --> tensor([0.1183], grad_fn=<DivBackward0>)\n",
      "47:50 loss --> tensor([0.1963], grad_fn=<DivBackward0>)\n",
      "47:60 loss --> tensor([0.0597], grad_fn=<DivBackward0>)\n",
      "47:70 loss --> tensor([0.0482], grad_fn=<DivBackward0>)\n",
      "47:80 loss --> tensor([0.0801], grad_fn=<DivBackward0>)\n",
      "47:90 loss --> tensor([0.1513], grad_fn=<DivBackward0>)\n",
      "47:100 loss --> tensor([0.1557], grad_fn=<DivBackward0>)\n",
      "47:110 loss --> tensor([0.2377], grad_fn=<DivBackward0>)\n",
      "47:120 loss --> tensor([0.0759], grad_fn=<DivBackward0>)\n",
      "47:130 loss --> tensor([0.2052], grad_fn=<DivBackward0>)\n",
      "47:140 loss --> tensor([0.0726], grad_fn=<DivBackward0>)\n",
      "48:0 loss --> tensor([0.0667], grad_fn=<DivBackward0>)\n",
      "48:10 loss --> tensor([0.0606], grad_fn=<DivBackward0>)\n",
      "48:20 loss --> tensor([0.0602], grad_fn=<DivBackward0>)\n",
      "48:30 loss --> tensor([0.0575], grad_fn=<DivBackward0>)\n",
      "48:40 loss --> tensor([0.0643], grad_fn=<DivBackward0>)\n",
      "48:50 loss --> tensor([0.0789], grad_fn=<DivBackward0>)\n",
      "48:60 loss --> tensor([0.0742], grad_fn=<DivBackward0>)\n",
      "48:70 loss --> tensor([0.0651], grad_fn=<DivBackward0>)\n",
      "48:80 loss --> tensor([0.0726], grad_fn=<DivBackward0>)\n",
      "48:90 loss --> tensor([0.0722], grad_fn=<DivBackward0>)\n",
      "48:100 loss --> tensor([0.3702], grad_fn=<DivBackward0>)\n",
      "48:110 loss --> tensor([0.0442], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48:120 loss --> tensor([0.1019], grad_fn=<DivBackward0>)\n",
      "48:130 loss --> tensor([0.1465], grad_fn=<DivBackward0>)\n",
      "48:140 loss --> tensor([0.0457], grad_fn=<DivBackward0>)\n",
      "49:0 loss --> tensor([0.0611], grad_fn=<DivBackward0>)\n",
      "49:10 loss --> tensor([0.1714], grad_fn=<DivBackward0>)\n",
      "49:20 loss --> tensor([0.0749], grad_fn=<DivBackward0>)\n",
      "49:30 loss --> tensor([0.1451], grad_fn=<DivBackward0>)\n",
      "49:40 loss --> tensor([0.0926], grad_fn=<DivBackward0>)\n",
      "49:50 loss --> tensor([0.0763], grad_fn=<DivBackward0>)\n",
      "49:60 loss --> tensor([0.0895], grad_fn=<DivBackward0>)\n",
      "49:70 loss --> tensor([0.0620], grad_fn=<DivBackward0>)\n",
      "49:80 loss --> tensor([0.0934], grad_fn=<DivBackward0>)\n",
      "49:90 loss --> tensor([0.0709], grad_fn=<DivBackward0>)\n",
      "49:100 loss --> tensor([0.1125], grad_fn=<DivBackward0>)\n",
      "49:110 loss --> tensor([0.0861], grad_fn=<DivBackward0>)\n",
      "49:120 loss --> tensor([0.0731], grad_fn=<DivBackward0>)\n",
      "49:130 loss --> tensor([0.1325], grad_fn=<DivBackward0>)\n",
      "49:140 loss --> tensor([0.1481], grad_fn=<DivBackward0>)\n",
      "50:0 loss --> tensor([0.1210], grad_fn=<DivBackward0>)\n",
      "50:10 loss --> tensor([0.0490], grad_fn=<DivBackward0>)\n",
      "50:20 loss --> tensor([0.0626], grad_fn=<DivBackward0>)\n",
      "50:30 loss --> tensor([0.0662], grad_fn=<DivBackward0>)\n",
      "50:40 loss --> tensor([0.1070], grad_fn=<DivBackward0>)\n",
      "50:50 loss --> tensor([0.0652], grad_fn=<DivBackward0>)\n",
      "50:60 loss --> tensor([0.0732], grad_fn=<DivBackward0>)\n",
      "50:70 loss --> tensor([0.0590], grad_fn=<DivBackward0>)\n",
      "50:80 loss --> tensor([0.0489], grad_fn=<DivBackward0>)\n",
      "50:90 loss --> tensor([0.2185], grad_fn=<DivBackward0>)\n",
      "50:100 loss --> tensor([0.1642], grad_fn=<DivBackward0>)\n",
      "50:110 loss --> tensor([0.0657], grad_fn=<DivBackward0>)\n",
      "50:120 loss --> tensor([0.0710], grad_fn=<DivBackward0>)\n",
      "50:130 loss --> tensor([0.0719], grad_fn=<DivBackward0>)\n",
      "50:140 loss --> tensor([0.0803], grad_fn=<DivBackward0>)\n",
      "51:0 loss --> tensor([0.0582], grad_fn=<DivBackward0>)\n",
      "51:10 loss --> tensor([0.0572], grad_fn=<DivBackward0>)\n",
      "51:20 loss --> tensor([0.0501], grad_fn=<DivBackward0>)\n",
      "51:30 loss --> tensor([0.0556], grad_fn=<DivBackward0>)\n",
      "51:40 loss --> tensor([0.0846], grad_fn=<DivBackward0>)\n",
      "51:50 loss --> tensor([0.0512], grad_fn=<DivBackward0>)\n",
      "51:60 loss --> tensor([0.0654], grad_fn=<DivBackward0>)\n",
      "51:70 loss --> tensor([0.0597], grad_fn=<DivBackward0>)\n",
      "51:80 loss --> tensor([0.0474], grad_fn=<DivBackward0>)\n",
      "51:90 loss --> tensor([0.0659], grad_fn=<DivBackward0>)\n",
      "51:100 loss --> tensor([0.0717], grad_fn=<DivBackward0>)\n",
      "51:110 loss --> tensor([0.0958], grad_fn=<DivBackward0>)\n",
      "51:120 loss --> tensor([0.0762], grad_fn=<DivBackward0>)\n",
      "51:130 loss --> tensor([0.0700], grad_fn=<DivBackward0>)\n",
      "51:140 loss --> tensor([0.1950], grad_fn=<DivBackward0>)\n",
      "52:0 loss --> tensor([0.1135], grad_fn=<DivBackward0>)\n",
      "52:10 loss --> tensor([0.1100], grad_fn=<DivBackward0>)\n",
      "52:20 loss --> tensor([0.5752], grad_fn=<DivBackward0>)\n",
      "52:30 loss --> tensor([0.0773], grad_fn=<DivBackward0>)\n",
      "52:40 loss --> tensor([0.0805], grad_fn=<DivBackward0>)\n",
      "52:50 loss --> tensor([0.0469], grad_fn=<DivBackward0>)\n",
      "52:60 loss --> tensor([0.0806], grad_fn=<DivBackward0>)\n",
      "52:70 loss --> tensor([0.1091], grad_fn=<DivBackward0>)\n",
      "52:80 loss --> tensor([0.0537], grad_fn=<DivBackward0>)\n",
      "52:90 loss --> tensor([0.1563], grad_fn=<DivBackward0>)\n",
      "52:100 loss --> tensor([0.0723], grad_fn=<DivBackward0>)\n",
      "52:110 loss --> tensor([0.1221], grad_fn=<DivBackward0>)\n",
      "52:120 loss --> tensor([0.2011], grad_fn=<DivBackward0>)\n",
      "52:130 loss --> tensor([0.0554], grad_fn=<DivBackward0>)\n",
      "52:140 loss --> tensor([0.0601], grad_fn=<DivBackward0>)\n",
      "53:0 loss --> tensor([0.0808], grad_fn=<DivBackward0>)\n",
      "53:10 loss --> tensor([0.0529], grad_fn=<DivBackward0>)\n",
      "53:20 loss --> tensor([0.0572], grad_fn=<DivBackward0>)\n",
      "53:30 loss --> tensor([0.0648], grad_fn=<DivBackward0>)\n",
      "53:40 loss --> tensor([0.0805], grad_fn=<DivBackward0>)\n",
      "53:50 loss --> tensor([0.0834], grad_fn=<DivBackward0>)\n",
      "53:60 loss --> tensor([0.0719], grad_fn=<DivBackward0>)\n",
      "53:70 loss --> tensor([0.0614], grad_fn=<DivBackward0>)\n",
      "53:80 loss --> tensor([0.0589], grad_fn=<DivBackward0>)\n",
      "53:90 loss --> tensor([0.0853], grad_fn=<DivBackward0>)\n",
      "53:100 loss --> tensor([0.0568], grad_fn=<DivBackward0>)\n",
      "53:110 loss --> tensor([0.0635], grad_fn=<DivBackward0>)\n",
      "53:120 loss --> tensor([0.1033], grad_fn=<DivBackward0>)\n",
      "53:130 loss --> tensor([0.0500], grad_fn=<DivBackward0>)\n",
      "53:140 loss --> tensor([0.0510], grad_fn=<DivBackward0>)\n",
      "54:0 loss --> tensor([0.0979], grad_fn=<DivBackward0>)\n",
      "54:10 loss --> tensor([0.0468], grad_fn=<DivBackward0>)\n",
      "54:20 loss --> tensor([0.0751], grad_fn=<DivBackward0>)\n",
      "54:30 loss --> tensor([0.1009], grad_fn=<DivBackward0>)\n",
      "54:40 loss --> tensor([0.0406], grad_fn=<DivBackward0>)\n",
      "54:50 loss --> tensor([0.0863], grad_fn=<DivBackward0>)\n",
      "54:60 loss --> tensor([0.0520], grad_fn=<DivBackward0>)\n",
      "54:70 loss --> tensor([0.1224], grad_fn=<DivBackward0>)\n",
      "54:80 loss --> tensor([0.0551], grad_fn=<DivBackward0>)\n",
      "54:90 loss --> tensor([0.0625], grad_fn=<DivBackward0>)\n",
      "54:100 loss --> tensor([0.0657], grad_fn=<DivBackward0>)\n",
      "54:110 loss --> tensor([0.0655], grad_fn=<DivBackward0>)\n",
      "54:120 loss --> tensor([0.0607], grad_fn=<DivBackward0>)\n",
      "54:130 loss --> tensor([0.0458], grad_fn=<DivBackward0>)\n",
      "54:140 loss --> tensor([0.0722], grad_fn=<DivBackward0>)\n",
      "55:0 loss --> tensor([0.0901], grad_fn=<DivBackward0>)\n",
      "55:10 loss --> tensor([0.0513], grad_fn=<DivBackward0>)\n",
      "55:20 loss --> tensor([0.0522], grad_fn=<DivBackward0>)\n",
      "55:30 loss --> tensor([0.0625], grad_fn=<DivBackward0>)\n",
      "55:40 loss --> tensor([0.0537], grad_fn=<DivBackward0>)\n",
      "55:50 loss --> tensor([0.0647], grad_fn=<DivBackward0>)\n",
      "55:60 loss --> tensor([0.0674], grad_fn=<DivBackward0>)\n",
      "55:70 loss --> tensor([0.0740], grad_fn=<DivBackward0>)\n",
      "55:80 loss --> tensor([0.1081], grad_fn=<DivBackward0>)\n",
      "55:90 loss --> tensor([0.0389], grad_fn=<DivBackward0>)\n",
      "55:100 loss --> tensor([0.0554], grad_fn=<DivBackward0>)\n",
      "55:110 loss --> tensor([0.0340], grad_fn=<DivBackward0>)\n",
      "55:120 loss --> tensor([0.0666], grad_fn=<DivBackward0>)\n",
      "55:130 loss --> tensor([0.0585], grad_fn=<DivBackward0>)\n",
      "55:140 loss --> tensor([0.2171], grad_fn=<DivBackward0>)\n",
      "56:0 loss --> tensor([0.0572], grad_fn=<DivBackward0>)\n",
      "56:10 loss --> tensor([0.0965], grad_fn=<DivBackward0>)\n",
      "56:20 loss --> tensor([0.0582], grad_fn=<DivBackward0>)\n",
      "56:30 loss --> tensor([0.0433], grad_fn=<DivBackward0>)\n",
      "56:40 loss --> tensor([0.0550], grad_fn=<DivBackward0>)\n",
      "56:50 loss --> tensor([0.0623], grad_fn=<DivBackward0>)\n",
      "56:60 loss --> tensor([0.0533], grad_fn=<DivBackward0>)\n",
      "56:70 loss --> tensor([0.1159], grad_fn=<DivBackward0>)\n",
      "56:80 loss --> tensor([0.0596], grad_fn=<DivBackward0>)\n",
      "56:90 loss --> tensor([0.0570], grad_fn=<DivBackward0>)\n",
      "56:100 loss --> tensor([0.0788], grad_fn=<DivBackward0>)\n",
      "56:110 loss --> tensor([0.0940], grad_fn=<DivBackward0>)\n",
      "56:120 loss --> tensor([0.0595], grad_fn=<DivBackward0>)\n",
      "56:130 loss --> tensor([0.0650], grad_fn=<DivBackward0>)\n",
      "56:140 loss --> tensor([0.0770], grad_fn=<DivBackward0>)\n",
      "57:0 loss --> tensor([0.0624], grad_fn=<DivBackward0>)\n",
      "57:10 loss --> tensor([0.0847], grad_fn=<DivBackward0>)\n",
      "57:20 loss --> tensor([3.4022], grad_fn=<DivBackward0>)\n",
      "57:30 loss --> tensor([9.9447], grad_fn=<DivBackward0>)\n",
      "57:40 loss --> tensor([16.4048], grad_fn=<DivBackward0>)\n",
      "57:50 loss --> tensor([21.5102], grad_fn=<DivBackward0>)\n",
      "57:60 loss --> tensor([21.6429], grad_fn=<DivBackward0>)\n",
      "57:70 loss --> tensor([11.5656], grad_fn=<DivBackward0>)\n",
      "57:80 loss --> tensor([12.1413], grad_fn=<DivBackward0>)\n",
      "57:90 loss --> tensor([14.7159], grad_fn=<DivBackward0>)\n",
      "57:100 loss --> tensor([4.5711], grad_fn=<DivBackward0>)\n",
      "57:110 loss --> tensor([10.8564], grad_fn=<DivBackward0>)\n",
      "57:120 loss --> tensor([4.1304], grad_fn=<DivBackward0>)\n",
      "57:130 loss --> tensor([13.4019], grad_fn=<DivBackward0>)\n",
      "57:140 loss --> tensor([13.7998], grad_fn=<DivBackward0>)\n",
      "58:0 loss --> tensor([5.2142], grad_fn=<DivBackward0>)\n",
      "58:10 loss --> tensor([3.1334], grad_fn=<DivBackward0>)\n",
      "58:20 loss --> tensor([5.7519], grad_fn=<DivBackward0>)\n",
      "58:30 loss --> tensor([2.8351], grad_fn=<DivBackward0>)\n",
      "58:40 loss --> tensor([6.1557], grad_fn=<DivBackward0>)\n",
      "58:50 loss --> tensor([2.6427], grad_fn=<DivBackward0>)\n",
      "58:60 loss --> tensor([3.2629], grad_fn=<DivBackward0>)\n",
      "58:70 loss --> tensor([1.2779], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58:80 loss --> tensor([4.4251], grad_fn=<DivBackward0>)\n",
      "58:90 loss --> tensor([2.0784], grad_fn=<DivBackward0>)\n",
      "58:100 loss --> tensor([3.4789], grad_fn=<DivBackward0>)\n",
      "58:110 loss --> tensor([2.0481], grad_fn=<DivBackward0>)\n",
      "58:120 loss --> tensor([3.2647], grad_fn=<DivBackward0>)\n",
      "58:130 loss --> tensor([2.9361], grad_fn=<DivBackward0>)\n",
      "58:140 loss --> tensor([1.0645], grad_fn=<DivBackward0>)\n",
      "59:0 loss --> tensor([1.4378], grad_fn=<DivBackward0>)\n",
      "59:10 loss --> tensor([0.8588], grad_fn=<DivBackward0>)\n",
      "59:20 loss --> tensor([1.3759], grad_fn=<DivBackward0>)\n",
      "59:30 loss --> tensor([2.0949], grad_fn=<DivBackward0>)\n",
      "59:40 loss --> tensor([0.6558], grad_fn=<DivBackward0>)\n",
      "59:50 loss --> tensor([1.0538], grad_fn=<DivBackward0>)\n",
      "59:60 loss --> tensor([0.8373], grad_fn=<DivBackward0>)\n",
      "59:70 loss --> tensor([0.8074], grad_fn=<DivBackward0>)\n",
      "59:80 loss --> tensor([0.9266], grad_fn=<DivBackward0>)\n",
      "59:90 loss --> tensor([1.1950], grad_fn=<DivBackward0>)\n",
      "59:100 loss --> tensor([0.8516], grad_fn=<DivBackward0>)\n",
      "59:110 loss --> tensor([1.5343], grad_fn=<DivBackward0>)\n",
      "59:120 loss --> tensor([0.8791], grad_fn=<DivBackward0>)\n",
      "59:130 loss --> tensor([0.6115], grad_fn=<DivBackward0>)\n",
      "59:140 loss --> tensor([0.8940], grad_fn=<DivBackward0>)\n",
      "60:0 loss --> tensor([2.8170], grad_fn=<DivBackward0>)\n",
      "60:10 loss --> tensor([1.8884], grad_fn=<DivBackward0>)\n",
      "60:20 loss --> tensor([2.8654], grad_fn=<DivBackward0>)\n",
      "60:30 loss --> tensor([0.4791], grad_fn=<DivBackward0>)\n",
      "60:40 loss --> tensor([0.2849], grad_fn=<DivBackward0>)\n",
      "60:50 loss --> tensor([0.4480], grad_fn=<DivBackward0>)\n",
      "60:60 loss --> tensor([0.9389], grad_fn=<DivBackward0>)\n",
      "60:70 loss --> tensor([0.8761], grad_fn=<DivBackward0>)\n",
      "60:80 loss --> tensor([1.0118], grad_fn=<DivBackward0>)\n",
      "60:90 loss --> tensor([0.6532], grad_fn=<DivBackward0>)\n",
      "60:100 loss --> tensor([0.8287], grad_fn=<DivBackward0>)\n",
      "60:110 loss --> tensor([0.6260], grad_fn=<DivBackward0>)\n",
      "60:120 loss --> tensor([1.5400], grad_fn=<DivBackward0>)\n",
      "60:130 loss --> tensor([0.4415], grad_fn=<DivBackward0>)\n",
      "60:140 loss --> tensor([1.0203], grad_fn=<DivBackward0>)\n",
      "61:0 loss --> tensor([0.2919], grad_fn=<DivBackward0>)\n",
      "61:10 loss --> tensor([0.3358], grad_fn=<DivBackward0>)\n",
      "61:20 loss --> tensor([0.8916], grad_fn=<DivBackward0>)\n",
      "61:30 loss --> tensor([1.2408], grad_fn=<DivBackward0>)\n",
      "61:40 loss --> tensor([0.3908], grad_fn=<DivBackward0>)\n",
      "61:50 loss --> tensor([0.4140], grad_fn=<DivBackward0>)\n",
      "61:60 loss --> tensor([0.1920], grad_fn=<DivBackward0>)\n",
      "61:70 loss --> tensor([2.2829], grad_fn=<DivBackward0>)\n",
      "61:80 loss --> tensor([0.8797], grad_fn=<DivBackward0>)\n",
      "61:90 loss --> tensor([1.3787], grad_fn=<DivBackward0>)\n",
      "61:100 loss --> tensor([1.5900], grad_fn=<DivBackward0>)\n",
      "61:110 loss --> tensor([0.4128], grad_fn=<DivBackward0>)\n",
      "61:120 loss --> tensor([0.7773], grad_fn=<DivBackward0>)\n",
      "61:130 loss --> tensor([1.6292], grad_fn=<DivBackward0>)\n",
      "61:140 loss --> tensor([0.4005], grad_fn=<DivBackward0>)\n",
      "62:0 loss --> tensor([1.5718], grad_fn=<DivBackward0>)\n",
      "62:10 loss --> tensor([1.6002], grad_fn=<DivBackward0>)\n",
      "62:20 loss --> tensor([0.6693], grad_fn=<DivBackward0>)\n",
      "62:30 loss --> tensor([0.3175], grad_fn=<DivBackward0>)\n",
      "62:40 loss --> tensor([0.4632], grad_fn=<DivBackward0>)\n",
      "62:50 loss --> tensor([0.3199], grad_fn=<DivBackward0>)\n",
      "62:60 loss --> tensor([0.3003], grad_fn=<DivBackward0>)\n",
      "62:70 loss --> tensor([0.2534], grad_fn=<DivBackward0>)\n",
      "62:80 loss --> tensor([0.5483], grad_fn=<DivBackward0>)\n",
      "62:90 loss --> tensor([1.7910], grad_fn=<DivBackward0>)\n",
      "62:100 loss --> tensor([0.4748], grad_fn=<DivBackward0>)\n",
      "62:110 loss --> tensor([2.4929], grad_fn=<DivBackward0>)\n",
      "62:120 loss --> tensor([0.7229], grad_fn=<DivBackward0>)\n",
      "62:130 loss --> tensor([0.3221], grad_fn=<DivBackward0>)\n",
      "62:140 loss --> tensor([0.2379], grad_fn=<DivBackward0>)\n",
      "63:0 loss --> tensor([0.3731], grad_fn=<DivBackward0>)\n",
      "63:10 loss --> tensor([0.3776], grad_fn=<DivBackward0>)\n",
      "63:20 loss --> tensor([0.3585], grad_fn=<DivBackward0>)\n",
      "63:30 loss --> tensor([0.1885], grad_fn=<DivBackward0>)\n",
      "63:40 loss --> tensor([0.5387], grad_fn=<DivBackward0>)\n",
      "63:50 loss --> tensor([0.2644], grad_fn=<DivBackward0>)\n",
      "63:60 loss --> tensor([0.7096], grad_fn=<DivBackward0>)\n",
      "63:70 loss --> tensor([0.1696], grad_fn=<DivBackward0>)\n",
      "63:80 loss --> tensor([0.2924], grad_fn=<DivBackward0>)\n",
      "63:90 loss --> tensor([0.2151], grad_fn=<DivBackward0>)\n",
      "63:100 loss --> tensor([0.2178], grad_fn=<DivBackward0>)\n",
      "63:110 loss --> tensor([0.2516], grad_fn=<DivBackward0>)\n",
      "63:120 loss --> tensor([0.2470], grad_fn=<DivBackward0>)\n",
      "63:130 loss --> tensor([0.8904], grad_fn=<DivBackward0>)\n",
      "63:140 loss --> tensor([0.2378], grad_fn=<DivBackward0>)\n",
      "64:0 loss --> tensor([0.1719], grad_fn=<DivBackward0>)\n",
      "64:10 loss --> tensor([0.7317], grad_fn=<DivBackward0>)\n",
      "64:20 loss --> tensor([0.5398], grad_fn=<DivBackward0>)\n",
      "64:30 loss --> tensor([0.2442], grad_fn=<DivBackward0>)\n",
      "64:40 loss --> tensor([0.1300], grad_fn=<DivBackward0>)\n",
      "64:50 loss --> tensor([0.1491], grad_fn=<DivBackward0>)\n",
      "64:60 loss --> tensor([0.2767], grad_fn=<DivBackward0>)\n",
      "64:70 loss --> tensor([0.1390], grad_fn=<DivBackward0>)\n",
      "64:80 loss --> tensor([0.2683], grad_fn=<DivBackward0>)\n",
      "64:90 loss --> tensor([0.3036], grad_fn=<DivBackward0>)\n",
      "64:100 loss --> tensor([0.3564], grad_fn=<DivBackward0>)\n",
      "64:110 loss --> tensor([0.4248], grad_fn=<DivBackward0>)\n",
      "64:120 loss --> tensor([0.2656], grad_fn=<DivBackward0>)\n",
      "64:130 loss --> tensor([0.7259], grad_fn=<DivBackward0>)\n",
      "64:140 loss --> tensor([0.2081], grad_fn=<DivBackward0>)\n",
      "65:0 loss --> tensor([0.8729], grad_fn=<DivBackward0>)\n",
      "65:10 loss --> tensor([0.2784], grad_fn=<DivBackward0>)\n",
      "65:20 loss --> tensor([0.1666], grad_fn=<DivBackward0>)\n",
      "65:30 loss --> tensor([0.4864], grad_fn=<DivBackward0>)\n",
      "65:40 loss --> tensor([0.2346], grad_fn=<DivBackward0>)\n",
      "65:50 loss --> tensor([0.7711], grad_fn=<DivBackward0>)\n",
      "65:60 loss --> tensor([0.2281], grad_fn=<DivBackward0>)\n",
      "65:70 loss --> tensor([0.5715], grad_fn=<DivBackward0>)\n",
      "65:80 loss --> tensor([1.4477], grad_fn=<DivBackward0>)\n",
      "65:90 loss --> tensor([0.1623], grad_fn=<DivBackward0>)\n",
      "65:100 loss --> tensor([0.3099], grad_fn=<DivBackward0>)\n",
      "65:110 loss --> tensor([0.2257], grad_fn=<DivBackward0>)\n",
      "65:120 loss --> tensor([0.3226], grad_fn=<DivBackward0>)\n",
      "65:130 loss --> tensor([0.3414], grad_fn=<DivBackward0>)\n",
      "65:140 loss --> tensor([0.3331], grad_fn=<DivBackward0>)\n",
      "66:0 loss --> tensor([1.1191], grad_fn=<DivBackward0>)\n",
      "66:10 loss --> tensor([0.4810], grad_fn=<DivBackward0>)\n",
      "66:20 loss --> tensor([0.1281], grad_fn=<DivBackward0>)\n",
      "66:30 loss --> tensor([0.2133], grad_fn=<DivBackward0>)\n",
      "66:40 loss --> tensor([0.2722], grad_fn=<DivBackward0>)\n",
      "66:50 loss --> tensor([0.3331], grad_fn=<DivBackward0>)\n",
      "66:60 loss --> tensor([0.2784], grad_fn=<DivBackward0>)\n",
      "66:70 loss --> tensor([0.1718], grad_fn=<DivBackward0>)\n",
      "66:80 loss --> tensor([0.2414], grad_fn=<DivBackward0>)\n",
      "66:90 loss --> tensor([0.4626], grad_fn=<DivBackward0>)\n",
      "66:100 loss --> tensor([0.3433], grad_fn=<DivBackward0>)\n",
      "66:110 loss --> tensor([0.1731], grad_fn=<DivBackward0>)\n",
      "66:120 loss --> tensor([0.7181], grad_fn=<DivBackward0>)\n",
      "66:130 loss --> tensor([0.1520], grad_fn=<DivBackward0>)\n",
      "66:140 loss --> tensor([0.1513], grad_fn=<DivBackward0>)\n",
      "67:0 loss --> tensor([0.1491], grad_fn=<DivBackward0>)\n",
      "67:10 loss --> tensor([0.3112], grad_fn=<DivBackward0>)\n",
      "67:20 loss --> tensor([0.1441], grad_fn=<DivBackward0>)\n",
      "67:30 loss --> tensor([0.1785], grad_fn=<DivBackward0>)\n",
      "67:40 loss --> tensor([0.3715], grad_fn=<DivBackward0>)\n",
      "67:50 loss --> tensor([0.1290], grad_fn=<DivBackward0>)\n",
      "67:60 loss --> tensor([0.1705], grad_fn=<DivBackward0>)\n",
      "67:70 loss --> tensor([0.6034], grad_fn=<DivBackward0>)\n",
      "67:80 loss --> tensor([0.1160], grad_fn=<DivBackward0>)\n",
      "67:90 loss --> tensor([0.1283], grad_fn=<DivBackward0>)\n",
      "67:100 loss --> tensor([0.1394], grad_fn=<DivBackward0>)\n",
      "67:110 loss --> tensor([0.2879], grad_fn=<DivBackward0>)\n",
      "67:120 loss --> tensor([0.1026], grad_fn=<DivBackward0>)\n",
      "67:130 loss --> tensor([0.3092], grad_fn=<DivBackward0>)\n",
      "67:140 loss --> tensor([0.1229], grad_fn=<DivBackward0>)\n",
      "68:0 loss --> tensor([0.0915], grad_fn=<DivBackward0>)\n",
      "68:10 loss --> tensor([0.1004], grad_fn=<DivBackward0>)\n",
      "68:20 loss --> tensor([0.3287], grad_fn=<DivBackward0>)\n",
      "68:30 loss --> tensor([0.6754], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68:40 loss --> tensor([0.1940], grad_fn=<DivBackward0>)\n",
      "68:50 loss --> tensor([0.1369], grad_fn=<DivBackward0>)\n",
      "68:60 loss --> tensor([0.1075], grad_fn=<DivBackward0>)\n",
      "68:70 loss --> tensor([0.1858], grad_fn=<DivBackward0>)\n",
      "68:80 loss --> tensor([0.1167], grad_fn=<DivBackward0>)\n",
      "68:90 loss --> tensor([0.9398], grad_fn=<DivBackward0>)\n",
      "68:100 loss --> tensor([0.1030], grad_fn=<DivBackward0>)\n",
      "68:110 loss --> tensor([0.0967], grad_fn=<DivBackward0>)\n",
      "68:120 loss --> tensor([0.1802], grad_fn=<DivBackward0>)\n",
      "68:130 loss --> tensor([0.1276], grad_fn=<DivBackward0>)\n",
      "68:140 loss --> tensor([0.4271], grad_fn=<DivBackward0>)\n",
      "69:0 loss --> tensor([0.2381], grad_fn=<DivBackward0>)\n",
      "69:10 loss --> tensor([0.1186], grad_fn=<DivBackward0>)\n",
      "69:20 loss --> tensor([0.1660], grad_fn=<DivBackward0>)\n",
      "69:30 loss --> tensor([0.2088], grad_fn=<DivBackward0>)\n",
      "69:40 loss --> tensor([0.1419], grad_fn=<DivBackward0>)\n",
      "69:50 loss --> tensor([0.1348], grad_fn=<DivBackward0>)\n",
      "69:60 loss --> tensor([0.1773], grad_fn=<DivBackward0>)\n",
      "69:70 loss --> tensor([0.1231], grad_fn=<DivBackward0>)\n",
      "69:80 loss --> tensor([0.3870], grad_fn=<DivBackward0>)\n",
      "69:90 loss --> tensor([0.1348], grad_fn=<DivBackward0>)\n",
      "69:100 loss --> tensor([0.0877], grad_fn=<DivBackward0>)\n",
      "69:110 loss --> tensor([0.3021], grad_fn=<DivBackward0>)\n",
      "69:120 loss --> tensor([0.1276], grad_fn=<DivBackward0>)\n",
      "69:130 loss --> tensor([0.1192], grad_fn=<DivBackward0>)\n",
      "69:140 loss --> tensor([0.1689], grad_fn=<DivBackward0>)\n",
      "70:0 loss --> tensor([0.1468], grad_fn=<DivBackward0>)\n",
      "70:10 loss --> tensor([0.7175], grad_fn=<DivBackward0>)\n",
      "70:20 loss --> tensor([0.7906], grad_fn=<DivBackward0>)\n",
      "70:30 loss --> tensor([0.2661], grad_fn=<DivBackward0>)\n",
      "70:40 loss --> tensor([0.0900], grad_fn=<DivBackward0>)\n",
      "70:50 loss --> tensor([0.0994], grad_fn=<DivBackward0>)\n",
      "70:60 loss --> tensor([0.2009], grad_fn=<DivBackward0>)\n",
      "70:70 loss --> tensor([0.0991], grad_fn=<DivBackward0>)\n",
      "70:80 loss --> tensor([0.1057], grad_fn=<DivBackward0>)\n",
      "70:90 loss --> tensor([0.3856], grad_fn=<DivBackward0>)\n",
      "70:100 loss --> tensor([0.1463], grad_fn=<DivBackward0>)\n",
      "70:110 loss --> tensor([0.1318], grad_fn=<DivBackward0>)\n",
      "70:120 loss --> tensor([0.0892], grad_fn=<DivBackward0>)\n",
      "70:130 loss --> tensor([0.1156], grad_fn=<DivBackward0>)\n",
      "70:140 loss --> tensor([0.0972], grad_fn=<DivBackward0>)\n",
      "71:0 loss --> tensor([0.2592], grad_fn=<DivBackward0>)\n",
      "71:10 loss --> tensor([0.0816], grad_fn=<DivBackward0>)\n",
      "71:20 loss --> tensor([0.1128], grad_fn=<DivBackward0>)\n",
      "71:30 loss --> tensor([0.2759], grad_fn=<DivBackward0>)\n",
      "71:40 loss --> tensor([0.2751], grad_fn=<DivBackward0>)\n",
      "71:50 loss --> tensor([0.1248], grad_fn=<DivBackward0>)\n",
      "71:60 loss --> tensor([0.0835], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-91:\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hecong/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/hecong/tools/python3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7be988c99af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mpreds_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/warpctc_pytorch-0.1-py3.6-linux-x86_64.egg/warpctc_pytorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, acts, labels, act_lens, label_lens)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         return self.ctc(acts, labels, act_lens, label_lens, self.size_average,\n\u001b[0;32m---> 82\u001b[0;31m                         self.length_average, self.blank)\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/warpctc_pytorch-0.1-py3.6-linux-x86_64.egg/warpctc_pytorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, acts, labels, act_lens, label_lens, size_average, length_average, blank)\u001b[0m\n\u001b[1;32m     30\u001b[0m                   \u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                   \u001b[0mcosts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                   blank)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/torch/utils/ffi/__init__.py\u001b[0m in \u001b[0;36msafe_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m                      for arg in args)\n\u001b[1;32m    201\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mtypeof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import lib.data.char as c\n",
    "from warpctc_pytorch import CTCLoss\n",
    "import lib.utils as utils\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "ngpu = 0\n",
    "# size of the lstm hidden state\n",
    "nh = 256\n",
    "nclass = len(c.alphabet) + 1\n",
    "# input channel ， 因为训练图片是转成灰度图，所以该值为1\n",
    "nc = 1\n",
    "lr = 0.001\n",
    "beta1=0.5\n",
    "MOMENTUM = 0.9\n",
    "EPOCH = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 字符转换编码\n",
    "converter = utils.strLabelConverter(c.alphabet)\n",
    "# 损失函数\n",
    "criterion = CTCLoss()\n",
    "\n",
    "crnn = CRNN(imgH, nc, nclass, nh, ngpu)\n",
    "crnn.apply(weights_init)\n",
    "if os.path.exists('/home/hecong/temp/data/ocr/simple_ocr.pkl'):\n",
    "    crnn.load_state_dict(torch.load('/home/hecong/temp/data/ocr/simple_ocr.pkl'))\n",
    "\n",
    "image = torch.FloatTensor(batchSize, 3, imgH, imgH)\n",
    "text = torch.IntTensor(batchSize * 5)\n",
    "length = torch.IntTensor(batchSize)\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#     crnn.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer = optim.SGD(\n",
    "    crnn.parameters(), lr=lr, momentum=MOMENTUM)\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for step,(t_image,t_label) in enumerate(train_loader):\n",
    "        batch_size = t_image.size(0)\n",
    "        utils.loadData(image, t_image)\n",
    "        t, l = converter.encode(t_label)\n",
    "        utils.loadData(text, t)\n",
    "        utils.loadData(length, l)\n",
    "        preds = crnn(image)\n",
    "        preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "        optimizer.zero_grad()\n",
    "        cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print('{}:{} loss --> {}'.format(epoch, step, cost))\n",
    "            torch.save(crnn.state_dict(), '/home/hecong/temp/data/ocr/simple_ocr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 1, 42])\n",
      "torch.Size([10])\n",
      "tensor([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], dtype=torch.int32)\n",
      "tensor([49], dtype=torch.int32)\n",
      "tensor([1], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([12.0142], grad_fn=<_CTCBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds_size = torch.IntTensor([49])\n",
    "# length = torch.IntTensor([2])\n",
    "print(preds.size())\n",
    "print(text.size())\n",
    "print(text)\n",
    "print(preds_size)\n",
    "print(length)\n",
    "\n",
    "criterion(preds, text, preds_size, length)\n",
    "\n",
    "# prob size --> torch.Size([2, 1, 5])\n",
    "# labels size --> torch.Size([2])\n",
    "# prob sizes -->tensor([2], dtype=torch.int32)\n",
    "# label sizes -->tensor([2], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(crnn.state_dict(), '/home/hecong/temp/data/ocr/simple_ocr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.data.char as c\n",
    "import lib.utils as utils\n",
    "import os\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "ngpu = 0\n",
    "# size of the lstm hidden state\n",
    "nh = 256\n",
    "nclass = len(c.alphabet) + 1\n",
    "# input channel ， 因为训练图片是转成灰度图，所以该值为1\n",
    "nc = 1\n",
    "lr = 0.001\n",
    "beta1=0.5\n",
    "converter = utils.strLabelConverter(c.alphabet)\n",
    "\n",
    "crnn = CRNN(imgH, nc, nclass, nh, ngpu)\n",
    "crnn.apply(weights_init)\n",
    "if os.path.exists('/home/hecong/temp/data/ocr/simple_ocr.pkl'):\n",
    "    crnn.load_state_dict(torch.load('/home/hecong/temp/data/ocr/simple_ocr.pkl'))\n",
    "image = torch.FloatTensor(batchSize, 3, imgH, imgH)\n",
    "_,(v_image,v_text) = next(enumerate(train_loader))\n",
    "utils.loadData(image,v_image)\n",
    "preds_s = crnn(image)\n",
    "batch_size = v_image.size(0)\n",
    "preds_size = Variable(torch.IntTensor([preds_s.size(0)] * batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 49, 1441])\n",
      "存而页匠夸夺灰达列死伪自血向似后行舟全会阶阴防奸如妇好她妈戏直茄茎茅林枝杯柜析板丰王井开夫天无元专云诗肩房诚衬衫视话诞询冲冰庄庆亦刘齐交次衣连步坚旱盯呈时吴助县引丑巴孔队办以允予劝诱说诵垦退既屋昼费陡\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "preds = preds_s.clone()\n",
    "\n",
    "preds = preds.permute(1,0,2)\n",
    "print(preds.size())\n",
    "_,preds = preds.max(2)\n",
    "preds = preds.view(-1)\n",
    "# print(preds)\n",
    "preds_size = Variable(torch.IntTensor([preds_s.size(0)])) * batchSize\n",
    "sim_preds = converter.decode(preds.data, preds_size.data, raw=False)\n",
    "\n",
    "print(sim_preds)\n",
    "image_0 = v_image[1][0]\n",
    "image_0 = image_0.numpy()\n",
    "plt.imshow(image_0,'gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
