{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 生成模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'd:\\\\img\\\\ocr'\n",
    "txt_file_name = os.path.join(path,'hand','char_words.txt')\n",
    "fonts_path = os.path.join(path, 'fonts')\n",
    "output_path = os.path.join(path,'hand','train')\n",
    "b_image_path = os.path.join(path, 'background')\n",
    "img_height = 32\n",
    "img_width = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 生成字符\n",
    "import lib.gen_image as gen\n",
    "import importlib\n",
    "importlib.reload(gen)\n",
    "\n",
    "\n",
    "\n",
    "alpha = 'ABQP'\n",
    "gen.gen_words(txt_file_name, alpha,max_number=4, total=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n"
     ]
    }
   ],
   "source": [
    "# 生成训练图片\n",
    "import lib.gen_image as gen\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import importlib\n",
    "import shutil\n",
    "import cv2\n",
    "importlib.reload(gen)\n",
    "\n",
    "# 清空目录\n",
    "shutil.rmtree(output_path)  \n",
    "os.mkdir(output_path) \n",
    "\n",
    "# 字体\n",
    "all_fonts = []\n",
    "all_fonts_name = []\n",
    "for root, dirs, files in os.walk(fonts_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".ttf\"):\n",
    "            all_fonts.append(os.path.join(root, file))\n",
    "            all_fonts_name.append(file[:-4])\n",
    "            \n",
    "with open(txt_file_name) as f:\n",
    "    text_lines = f.readlines()\n",
    "text_lines = [t.strip() for t in text_lines]\n",
    "text_lines = [t for t in text_lines if len(t) > 0]\n",
    "\n",
    "bg_file_lists = [os.path.join(b_image_path, x) for x in os.listdir(b_image_path)]\n",
    "# text_set = [[32, 100, [20,24]],[64,200,[30,28]],[48,150,[26,24]]]\n",
    "font_size_listt = [20,24,28,32]\n",
    "number_of_examples = 100\n",
    "\n",
    "\n",
    "\n",
    "image_list = []\n",
    "output_data =[]\n",
    "\n",
    "for i, txt in enumerate(random.sample(text_lines, number_of_examples)):\n",
    "    select_font_idx = np.random.randint(len(all_fonts))\n",
    "    img_height, img_width = 32, 100\n",
    "    font_size = font_size_list[np.random.randint(len(font_size_list))]\n",
    "    image = gen.make_image(test_str=txt, fonttype=all_fonts[select_font_idx], \n",
    "                         fontsize=font_size,target_width=img_width, target_height=img_height, \n",
    "                         back_ground_img_list=bg_file_lists, \n",
    "                         need_include=True)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "    thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11,10)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "    dilate = cv2.dilate(thresh, kernel, iterations=1)\n",
    "    erode = cv2.erode(dilate, kernel, iterations=1)       \n",
    "    image = erode.copy()\n",
    "#     print('image size -->', image.shape)\n",
    "    img_path = os.path.join(output_path, str(i)+'_'+all_fonts_name[select_font_idx] + \".png\")\n",
    "#     image_list.append(image)\n",
    "    cv2.imwrite(img_path, image)    \n",
    "    txt = txt.replace('＋','+')\n",
    "    txt = txt.replace('－','-')\n",
    "    txt = txt.replace('（','(')\n",
    "    txt = txt.replace('）',')')\n",
    "    output_data.append({\n",
    "        \"image_path\": \"{}_{}.png\".format(str(i), all_fonts_name[select_font_idx]),\n",
    "        \"gt\": txt.lower()\n",
    "    })    \n",
    "            \n",
    "train_cnt = int(number_of_examples * 1.0)\n",
    "val_cnt = number_of_examples - train_cnt\n",
    "with open(os.path.join(path,'hand', 'training.json'), 'w') as f:\n",
    "    json.dump(output_data[:train_cnt], f)\n",
    "\n",
    "with open(os.path.join(path,'hand', 'validation.json'), 'w') as f:\n",
    "    json.dump(output_data[train_cnt:], f)\n",
    "\n",
    "print('over')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples length -- > 100\n",
      "Created dataset with 100 samples\n"
     ]
    }
   ],
   "source": [
    "# 生成训练库\n",
    "import json\n",
    "from lib.gen_dataset import createDataset\n",
    "import os\n",
    "img_path = os.path.join(path,'hand','train')\n",
    "train_file = os.path.join(path, 'hand','training.json') \n",
    "valid_file = os.path.join(path, 'hand','validation.json') \n",
    "with open(train_file,'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "imgLabelLists = []\n",
    "for item in train_data:\n",
    "    imgLabelLists.append((os.path.join(path,'hand','train',item['image_path']) ,str(item['gt'])))\n",
    "\n",
    "random.shuffle(imgLabelLists)\n",
    "train_lmdb_path = os.path.join(path,'hand','lmdb')   # 训练数据\n",
    "trainImgPaths = [x[0] for x in imgLabelLists]\n",
    "trainTxtLists = [x[1] for x in imgLabelLists]\n",
    "createDataset(train_lmdb_path, trainImgPaths, trainTxtLists, lexiconList=None, checkValid=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step --> torch.Size([90, 3, 32, 115]) --> 90\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "# traindata loader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import lib.dataset as dataset\n",
    "import lib.utils as utils\n",
    "import time\n",
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(utils)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_dataset = dataset.lmdbDataset(root= os.path.join(path,'hand','lmdb'))\n",
    "# print('train data set length -->{}'.format(len(train_dataset)))\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset,\n",
    "                                                   [int(0.9*len(train_dataset)), \n",
    "                                                    int(0.1*len(train_dataset))])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=dataset.adjustCollate(imgH=32, keep_ratio=False))\n",
    "#     \n",
    "\n",
    "for step, values in enumerate(train_loader):\n",
    "    images = Variable(values[0].type(torch.FloatTensor), requires_grad=False)\n",
    "#     images = values[0]\n",
    "    print('step -->', images.size(), '-->', len(images))\n",
    "    break\n",
    "\n",
    "print('over')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0667, 0.0667, 0.0745,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0667, 0.0667, 0.0745,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0745, 0.0745, 0.0745,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.0980, 0.0980, 0.0980,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0980, 0.0980, 0.0980,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0980, 0.0980, 0.0980,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.0745, 0.0745, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0745, 0.0745, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.1059, 0.1059, 0.1059,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.1059, 0.1059, 0.1059,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.1059, 0.1059, 0.1059,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.0588, 0.0588, 0.0667,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0588, 0.0588, 0.0667,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0667, 0.0667, 0.0667,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.0902, 0.0902, 0.0902,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# image 统一长度\n",
    "PADDING_CONSTANT = 255\n",
    "assert len(set([b.shape[0] for b in image_list])) == 1\n",
    "assert len(set([b.shape[2] for b in image_list])) == 1\n",
    "\n",
    "dim0 = image_list[0].shape[0]\n",
    "dim1 = max([b.shape[1] for b in image_list])\n",
    "dim2 = image_list[0].shape[2]\n",
    "\n",
    "image_list_new = np.full((len(image_list), dim0, dim1, dim2), PADDING_CONSTANT).astype(np.long)\n",
    "print(len(image_list_new))\n",
    "for idx, image in enumerate(image_list_new):\n",
    "    image[:,:image_list[idx].shape[1],:] = image_list[idx]\n",
    "    img_path = os.path.join(output_path, str(idx)+'_'+all_fonts_name[idx%len(all_fonts)] + \".png\")\n",
    "    cv2.imwrite(img_path, image)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# opencv 文本检测"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://www.learnopencv.com/tag/text-detection/\n",
    "https://stackoverflow.com/questions/46105401/improve-text-area-detection-opencv-python\n",
    "https://stackoverflow.com/questions/44334078/recognize-text-in-images-using-canny-edge-detection-in-opencv\n",
    "https://stackoverflow.com/questions/37771263/detect-text-area-in-an-image-using-python-and-opencv\n",
    "https://stackoverflow.com/questions/42174563/easy-ways-to-detect-and-crop-blocks-paragraphs-of-text-out-of-image\n",
    "\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('1.png')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (9,9), 0)\n",
    "thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11,30)\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9,9))\n",
    "dilate = cv2.dilate(thresh, kernel, iterations=4)\n",
    "\n",
    "cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "ROI_number = 0\n",
    "for c in cnts:\n",
    "    area = cv2.contourArea(c)\n",
    "    if area > 10000:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 3)\n",
    "        # ROI = image[y:y+h, x:x+w]\n",
    "        # cv2.imwrite('ROI_{}.png'.format(ROI_number), ROI)\n",
    "        # ROI_number += 1\n",
    "\n",
    "cv2.imshow('thresh', thresh)\n",
    "cv2.imshow('dilate', dilate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 63, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 二值化数据\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 10, 5\n",
    "\n",
    "image = cv2.imread('d:\\\\img\\\\exam\\\\F_2.png')\n",
    "print(image.shape)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,31,10)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "dilate = cv2.dilate(thresh, kernel, iterations=1)\n",
    "erode = cv2.erode(dilate, kernel, iterations=1)\n",
    "img = cv2.bitwise_not(erode)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "cv2.imwrite('d:\\\\img\\\\exam\\\\D_5.png',img)\n",
    "plt.imshow(img,'brg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量修改文件名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修改前： ['img_crop001390e3-4857-4e6e-8d3c-3c8bbce7ccc0.png', 'img_crop0149dc90-da5f-45bc-8f21-6de5931e58da.png', 'img_crop075bdb3b-b170-4cf2-9f60-ce3de9d834eb.png', 'img_crop08ce8245-4828-4319-8245-ed31a00a9774.png', 'img_crop0b1f5a54-2bde-42dc-81f5-268dd0fa2006.png', 'img_crop0c63c1dd-ad07-4eca-9246-7aa8d6f28a16.png', 'img_crop0fb42cb3-e940-47cd-af33-792176fc8162.png', 'img_crop14e4cb10-cdee-4a8d-8074-bd9b3c13c54e.png', 'img_crop15b31cfa-04f3-4fa4-ab0a-970456f37e94.png', 'img_crop17a002af-a7cc-4073-8131-a23d46b109b6.png', 'img_crop19856bc3-c96b-4683-9d5c-80e8b492a5a9.png', 'img_crop1a60f9ca-870b-4da3-b0ef-220f78dfe41b.png', 'img_crop1a745b74-545f-4614-b6ae-5be899b767e1.png', 'img_crop1a8fc4ce-6ad2-440a-b354-08f9491b5af5.png', 'img_crop1da678a3-d7bc-4d57-be36-a00e0a255f5d.png', 'img_crop1ec0481b-94b0-4344-9089-cec7609a77ab.png', 'img_crop2531ba28-4591-4fb3-9934-40ebfb4cf303.png', 'img_crop27cbde28-1f16-4cb8-bde6-61e59aa55f1f.png', 'img_crop29856699-4417-4b27-b11c-939d2111a492.png', 'img_crop2b22daaa-1e74-46f3-9aaa-018da5c8d590.png', 'img_crop2de1ce5f-8e42-4795-a77f-f6a849391577.png', 'img_crop3181496a-0e59-4442-8f5c-faf0ffd7614a.png', 'img_crop33281982-cf68-489c-9997-0fc0e4e556a5.png', 'img_crop3393436d-a4a5-4d01-ba78-fbeca6f7d0ce.png', 'img_crop340646cf-e92f-4898-b880-0bb305171eee.png', 'img_crop38811025-04f3-4c11-9cab-dac40ad7b836.png', 'img_crop3c39708d-1c02-4ee7-b841-283a88dc2772.png', 'img_crop3f407165-f685-458b-9f4b-61562a703ddc.png', 'img_crop415ca48f-40b2-4a33-ab4c-9babbee0495a.png', 'img_crop42d91bf1-1885-43a2-bcae-51b404aa5951.png', 'img_crop43d48b5e-c14b-4186-89d2-0282aff6a913.png', 'img_crop4b205c41-ca6f-4c24-818b-2bc9b5640196.png', 'img_crop4f02f9d8-bed6-4946-9b58-c2c40d26968f.png', 'img_crop512092d3-45a9-4cf4-97fc-a724d5936864.png', 'img_crop51bcfc0c-737c-438c-8fb4-59302596ae93.png', 'img_crop54a612f3-e96a-4fdd-8f49-afc941947998.png', 'img_crop55ec6294-8408-4f35-8db8-10bc6c84df1d.png', 'img_crop5864ad24-c150-486a-8ff3-7d4c395c8178.png', 'img_crop5cac7246-0d7d-4b7f-8868-817534a03acb.png', 'img_crop5e58db02-f10b-4772-b1a8-20e8bbefba79.png', 'img_crop5f515a36-4d4c-45d5-b91b-5c25b6c69d72.png', 'img_crop605f7ed0-a09f-49f3-be1b-410df7788e98.png', 'img_crop60977624-981d-400f-87ba-04587e010c29.png', 'img_crop64aaffcb-8cb0-442a-9f54-018b38744490.png', 'img_crop67ab1de2-b62c-4293-81ec-9ea8dd42d511.png', 'img_crop69c1342c-7db9-4172-b182-fe769b544e26.png', 'img_crop6ac3e91e-f591-49b8-a79f-2c5a0092b90c.png', 'img_crop6e0e88fb-c3fa-461f-a722-a54320ef816f.png', 'img_crop7089dee4-5a94-4f54-ad21-6a8ac37a9c40.png', 'img_crop716b48ef-e876-4e2b-ac95-c5d641eb7e5a.png', 'img_crop71b4841e-e8a6-4415-a6e7-904d5f5766fc.png', 'img_crop74bf7bdd-ed61-49b1-b05d-246ff8ead6fb.png', 'img_crop78c3a456-d224-4e62-8be0-8e4c55ee0488.png', 'img_crop78cb928a-0a39-42da-af4d-f493a39818da.png', 'img_crop7b6f4fb4-71b8-4e4e-ba80-e2052eeb7fb3.png', 'img_crop7f1a6b71-7bbb-40dc-a050-b92e63365c01.png', 'img_crop8275ce13-9cbe-4f1b-bb82-38deddd0007a.png', 'img_crop84338e88-8c76-461e-998e-687000cbcf35.png', 'img_crop8525a0d5-8f04-49d3-9cb3-0127efa1d21e.png', 'img_crop8880e386-2c00-4e5c-9aa3-f9d07110a796.png', 'img_crop8ac79bc1-2133-495a-b0cb-9e548d82285f.png', 'img_crop8bbf2f32-c6e5-47a3-9222-0ea5f5d8f3be.png', 'img_crop93a5a597-ddfa-4fce-897d-9c2e25aff770.png', 'img_crop9412748e-6568-4845-b473-52453037484b.png', 'img_crop943b6727-ad82-4a9d-a95d-3b3eec0cc568.png', 'img_crop9934cf01-ef2e-4701-a1e6-9044bb4b1b2c.png', 'img_crop9a13ba13-31b5-4e8b-b8fe-253c5fcc1714.png', 'img_crop9cf1ec68-c51a-4ae8-99a6-681313b228eb.png', 'img_crop9d852d6e-4220-4267-bd9f-c7f97746930d.png', 'img_crop9e184a41-2e97-4ee5-a8ad-11d151571ef7.png', 'img_crop9f3818e9-ca0d-419a-9a47-3e63665c334f.png', 'img_cropa1fd1fd1-5040-4b73-9e3c-b89cd1c94905.png', 'img_cropa6cb51e6-f4b2-4e32-a66b-e6c03db2c752.png', 'img_cropa6edd19f-cb26-443e-89bd-5886c1631a67.png', 'img_cropa741e855-f9c0-4ffb-adf9-0e3f40750f2f.png', 'img_cropabcfbbc2-e60f-4efe-9841-8d51701d64bc.png', 'img_cropae74d8b7-64b9-4d66-b8f1-01442efc3701.png', 'img_cropaf26bceb-f539-4656-b00e-d7775ab368f9.png', 'img_cropb6957fa3-95d9-4ba5-9521-85149f8bea27.png', 'img_cropbb2eaf3e-ff6b-4746-bcb1-ce33fa8bdff2.png', 'img_cropbb391138-9e63-42fb-b713-28bddba67b97.png', 'img_cropbb850d39-6f95-434b-bfdd-4a63cafe0f15.png', 'img_cropbbd53909-2216-4d4b-8cad-18b9e1880e10.png', 'img_cropbbd7759a-be5a-42f6-8cea-7f0368c35c98.png', 'img_cropbe575bb3-9bff-4e66-80bf-648a42a6e894.png', 'img_cropc0667dc6-e963-4064-a3e1-f0b069dbf262.png', 'img_cropc1972033-49fe-4c88-9bf6-665abec6e4a2.png', 'img_cropc21466a7-d02d-444e-accc-ef3ff5308073.png', 'img_cropc3070975-57a4-4572-819c-36663f57c805.png', 'img_cropc451aab3-1265-467e-bb46-445a181567a6.png', 'img_cropc52cf16b-db34-4560-904c-77ed37f41ac1.png', 'img_cropc777dba2-99af-4ad2-80b3-d8061af32c2a.png', 'img_cropc9325218-d5da-4def-9019-face8cfb52e9.png', 'img_cropc9b4e9a1-f3ee-4ebb-bf2e-c3c9704d9d12.png', 'img_cropcdc11e9a-3daa-4d62-8fa7-7dea1af0b379.png', 'img_cropce8aa960-d21c-4741-9b09-0c8f990bcc4f.png', 'img_cropd0496b04-c13f-4df5-89cc-d9e1c0964449.png', 'img_cropd0a3eefb-05ef-4bf9-b6aa-1b656d373cc2.png', 'img_cropd15bf784-eebc-4eb1-a236-2014b0c64dc1.png', 'img_cropd7988e9b-4019-4219-8c53-ada02944e15f.png', 'img_cropd821f5f4-ec3c-4d60-b919-8d32bd414457.png', 'img_cropda6744d8-a04b-4b6f-acd6-0af6d98a5742.png', 'img_cropdb407001-8c84-4178-81f4-743db974f4a4.png', 'img_cropdbb6a092-7da2-4c02-9f5c-1a5b3f35db17.png', 'img_cropddfee334-1375-4713-9207-0550f143dfde.png', 'img_crope0b2921f-5fd6-4496-a5d6-6142823a5c41.png', 'img_crope270cb60-5234-4555-9507-981e720fd685.png', 'img_crope2f69038-4965-451d-9bfd-d9350a5ca6bc.png', 'img_crope3f4052e-ef85-4e64-a67e-869aa91be7a3.png', 'img_crope498c3ae-c326-49bf-88fe-ff5366f3768a.png', 'img_crope4bb9232-716d-4f6c-b9ff-1955afebe6ad.png', 'img_crope841978f-119e-4193-a52a-2ff7f00e184d.png', 'img_crope97561b0-1028-48f0-89ea-f8912575ac3d.png', 'img_crope9fa3950-581f-49d0-bb9e-58ae10554f6b.png', 'img_cropef3712bc-afcf-47ea-bc60-8d92eca14cbf.png', 'img_cropf2abae0c-a388-4492-8b70-926bef3b9203.png', 'img_cropf30b1bce-c36b-4237-8a77-c79c01c8bb68.png', 'img_cropf47d9dd0-6d78-4d21-ae46-8db8556cc992.png', 'img_cropf6643f34-d29c-4cd4-9eed-a60a06c01afe.png', 'img_cropfe795967-b205-4b99-8918-2bfdbedd7a5a.png']\n",
      "修改前： ['img_crop027edf94-409a-4484-a281-c1b4febacef3.png', 'img_crop0922a2ab-5f20-4006-9844-ab446ed3894e.png', 'img_crop0a000d74-3cef-4109-99c9-38c76ff2714b.png', 'img_crop0da34929-6bf1-46db-8c22-fc6fa0b48c15.png', 'img_crop11a3cd51-ac2e-42be-a4b0-43a4bfc097da.png', 'img_crop15072dd0-07e8-4ebb-9032-7cfe8d05a314.png', 'img_crop1677b458-0e92-4d45-a974-8dc7a1173698.png', 'img_crop16b6e818-dede-4fb8-a5ad-46a0bb94281d.png', 'img_crop16d6d44e-ab16-493d-bcbb-a79a0c3a9879.png', 'img_crop18d42339-58c7-4793-b678-e745032df54c.png', 'img_crop1a4225fa-6fe6-433c-92b6-f1fa4ca0518e.png', 'img_crop1b02275c-a431-4a71-9a98-0fb27c34fb6e.png', 'img_crop1b2ef09f-98b7-4b73-8260-0c05ecb67dc0.png', 'img_crop1b677df0-4e86-49b5-9a53-bed4ccd39b19.png', 'img_crop1c3eb8c1-db3c-4a14-a71a-1541fbd9650e.png', 'img_crop1ebdace1-b635-4123-b9ae-a3eeb7bbe5a6.png', 'img_crop201cd060-339b-4f74-9c41-accd5d2e7f67.png', 'img_crop25ad1913-e49a-4e92-8335-6b25ac260b7e.png', 'img_crop298bf73e-fdab-4da8-a20e-abbf98a65b20.png', 'img_crop2bccc78d-d199-49e8-b4f6-ac6e8fd8eb8c.png', 'img_crop2e501bfd-0292-4603-85bd-57835aefccd4.png', 'img_crop32247363-9854-4acf-9963-0d002e0edd8c.png', 'img_crop32aab39a-a215-4f8a-8e03-a462c2cddf22.png', 'img_crop35ea17a5-6183-4be1-a6fd-d0449a8f512e.png', 'img_crop3744230f-2dd6-4c9a-9451-31970ba611cb.png', 'img_crop3e2aeb04-4966-4733-9eda-cef6d1d6b93b.png', 'img_crop3f7763f6-01fb-4e57-9df0-5d9a69d47117.png', 'img_crop401f1433-ccd0-406c-8e73-f105272cb6a8.png', 'img_crop405c182d-fbab-4f3b-a7da-f1a2f18c45ab.png', 'img_crop4242a77d-1423-4851-9444-4f25ed98e4ac.png', 'img_crop42545c41-8951-489c-be04-88d57c8b1ce9.png', 'img_crop42f62160-5ba7-41c9-a9e6-9eca9f1ee936.png', 'img_crop46dde851-11f6-4cb3-946b-d7c44ef140e6.png', 'img_crop4adb2b7f-dfd4-44fb-92af-55df27fc8e4c.png', 'img_crop4ae7ad7a-8e9a-47ff-a0ff-976421600bab.png', 'img_crop4f071cf7-f055-4486-a2c3-9693f569f6e3.png', 'img_crop53011ca3-db30-4fbc-8a3e-7da4d042aded.png', 'img_crop5334ac7e-2c2d-4c9e-9620-876ec67fc63d.png', 'img_crop55728d63-8087-49de-b894-5ef5f6b58a64.png', 'img_crop55aa9d1e-46b1-4a66-a917-f063f500ca7f.png', 'img_crop5b72c237-ac3b-48c6-ba6a-d7023c3d4fdc.png', 'img_crop5ea8738b-c3e7-4023-8a9f-ae376ecbee3a.png', 'img_crop5f975f20-58ac-4d5f-bd7f-02e3a1ec5961.png', 'img_crop6525b7e4-dd19-4f39-acba-669fddc97f7a.png', 'img_crop691efc00-a8b8-4f46-9143-57e6b42b163a.png', 'img_crop6b46ba20-79c3-40a3-aefd-7d9bec8ecb0e.png', 'img_crop6e610b44-cfe4-42b5-b812-439b28fef97d.png', 'img_crop6ec4675e-8962-4031-aa15-d0c1eae3709e.png', 'img_crop703db43e-cf9d-4846-bd63-255a681c6d5e.png', 'img_crop7077a59b-95c8-45de-b007-b59e73d1eb2b.png', 'img_crop7384b0d2-c39e-4c25-8721-4bbd45fa3639.png', 'img_crop73fa59db-4f39-4ebe-be6f-e42c632c31bc.png', 'img_crop74663deb-2633-434c-8007-968391f23e8f.png', 'img_crop74bcc1b8-907f-4992-9c80-e05a0f9b31cf.png', 'img_crop778a1eaf-f575-4465-b0c2-3b8cb8817177.png', 'img_crop7a511c4b-1b28-4ad5-87af-4b9f96c2a292.png', 'img_crop7b88a7f4-65d2-43c4-8643-712507cff846.png', 'img_crop80cde545-c68c-449c-9f57-3326cf72d766.png', 'img_crop80e3f5b8-3c81-48a7-b868-f1868bf34d4b.png', 'img_crop8318fc95-0474-4566-a533-ed6e0359cb9a.png', 'img_crop87b52317-18c6-4315-bab2-ecadf6b8a237.png', 'img_crop89b1827d-b5d6-4af6-9a8e-61264da926c1.png', 'img_crop8bba8029-ab59-4635-b142-4916cc7bcd0a.png', 'img_crop94e7b7a6-5070-4047-935c-0d2630c35834.png', 'img_crop969a724d-2c24-4535-99f9-276e2b962cff.png', 'img_crop97ba1863-d7a9-4dba-a0d0-e2e830b83b89.png', 'img_crop9b34b54c-fc4a-41da-808a-277c41291c2a.png', 'img_crop9d34eff3-6ee1-4c64-951d-93d63337fcdb.png', 'img_cropa3016efb-09ca-4bdc-a12a-d2ee769a6077.png', 'img_cropa3e86b95-58ae-4573-ac94-7c0a8857d3e1.png', 'img_cropa443805e-15c1-4094-9b00-75a9ed6d7253.png', 'img_cropa455a954-77fe-433c-8c94-b2ad04e34584.png', 'img_cropa4b08135-d185-4ce0-9995-a6fe1186f73d.png', 'img_cropa61f0d68-8a2f-43ec-8b90-210b4ddf6cf0.png', 'img_cropa67088c8-3b45-4f14-b2ef-19e8b97cf059.png', 'img_cropa7ab2fbd-dbfd-49ec-bae2-b19e8ba4a44e.png', 'img_cropa8e09e22-82ab-4dad-b239-ed79481fae89.png', 'img_cropaa3ce78b-b37f-4830-be9d-ca47a4d53327.png', 'img_cropac7afc62-990d-481f-9694-1f9f90c62666.png', 'img_cropad5d42f8-b16b-4983-81b0-c42229498bc3.png', 'img_cropaf501a81-21d8-433f-a639-bfb739ca5fb6.png', 'img_cropb030993b-12cf-4931-8ea8-81f2b48471a5.png', 'img_cropb2191c36-13e1-41c3-8108-bd6c38c6e9a3.png', 'img_cropb63e93d0-e259-4785-ae42-75d003790faa.png', 'img_cropb83e9ac1-cdae-4415-9bd2-fa95b72eeae5.png', 'img_cropbad7f9a8-88e8-47ee-9e82-57c2ced23b70.png', 'img_cropc199457c-c0a1-4c24-a01e-713c4a26ecdf.png', 'img_cropc3f5fb23-08a2-4114-894a-7474ed2b3f3e.png', 'img_cropc64a7220-7838-482c-899c-69e90043dac8.png', 'img_cropc6690e51-5df5-44bb-a834-99e7b301a446.png', 'img_cropc7c0108a-40a2-47ec-9cbc-1f72fd4b0f4a.png', 'img_cropc85a5a26-d0b1-4f12-a787-7fbc710b9ea8.png', 'img_cropc960bbf1-70e2-4860-af06-8f0df978dec7.png', 'img_cropd179afd5-bdc9-4ec4-b780-2eaabdeb4272.png', 'img_cropd30bf0d2-e1d0-4cb9-a532-5e7ce921c2a2.png', 'img_cropd3fda547-a8e9-438b-b018-1a6c88cbd59d.png', 'img_cropd6dbfb9a-b4ec-4404-83ae-aa54a89fc4b8.png', 'img_cropd801263b-6e9f-41b4-8026-7fd91c5594d1.png', 'img_cropddf62054-be6c-4ad5-ad66-a49807fa4625.png', 'img_crope0a7074a-f2ce-4779-9b68-f574678c7522.png', 'img_crope3275e6e-2c42-4350-a7ba-7fb9d9853752.png', 'img_crope3d63a89-89de-4657-957b-3e8ca09c866c.png', 'img_crope3ee01a3-ed2b-41e3-9979-0542c46c1fd0.png', 'img_crope570a525-b08f-4b7f-8d0a-5d3e4013510a.png', 'img_crope83cdd58-8017-4036-9d94-6df26942afbf.png', 'img_cropeb53e619-9174-4537-af39-7f054d7afef3.png', 'img_cropec263c5c-0e23-433b-8d9b-320d8b220653.png', 'img_cropee213005-768e-4af3-ae39-09645ea35e48.png', 'img_cropeee7a343-9cd9-4fdd-872c-63ed602d45ac.png', 'img_cropf2d474d9-0aa6-491b-811b-3257ec0e59f3.png', 'img_cropf3d5d9a5-357f-4b02-879f-e4d929fa7605.png', 'img_cropf60ce8fe-357a-4838-b007-995b48f9dc50.png', 'img_cropfa76de8a-0a7c-4034-9025-eef69f761b66.png', 'img_cropfdd57a99-300a-47d1-b34d-e2ad39aa003a.png', 'img_cropfeda24df-1310-46f9-81fe-e2cbc0abca67.png', 'img_cropff978580-1aa3-4429-97b8-d3c3063c17f7.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "path = os.path.join('D:\\\\img','exam','handle','special')\n",
    "# alpha = ['A','B','C','D','E','F','G','H','I']\n",
    "alpha = ['×','√']\n",
    "# alpha = ['A',]\n",
    "\n",
    "for item in alpha:\n",
    "#     subpath = os.path.join(path,'Sample{}'.format(item))\n",
    "    subpath = os.path.join(path,item)\n",
    "    fileList = os.listdir(subpath)\n",
    "    # 输出此文件夹中包含的文件名称\n",
    "    print(\"修改前：\" , fileList)\n",
    "    currentpath = os.getcwd()\n",
    "    os.chdir(subpath)\n",
    "    number = 100\n",
    "    for fileName in fileList:\n",
    "        os.rename(fileName, '{}_{}.png'.format(item, number))\n",
    "        number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
