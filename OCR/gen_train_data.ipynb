{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 生成模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'd:\\\\img\\\\ocr'\n",
    "txt_file_name = os.path.join(path,'hand','char_words.txt')\n",
    "fonts_path = os.path.join(path, 'fonts')\n",
    "output_path = os.path.join(path,'hand','train')\n",
    "b_image_path = os.path.join(path, 'background')\n",
    "img_height = 32\n",
    "img_width = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 生成字符\n",
    "import lib.gen_image as gen\n",
    "import importlib\n",
    "importlib.reload(gen)\n",
    "\n",
    "\n",
    "\n",
    "alpha = 'ABQP'\n",
    "gen.gen_words(txt_file_name, alpha,max_number=4, total=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n"
     ]
    }
   ],
   "source": [
    "# 生成训练图片\n",
    "import lib.gen_image as gen\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import importlib\n",
    "import shutil\n",
    "import cv2\n",
    "importlib.reload(gen)\n",
    "\n",
    "# 清空目录\n",
    "shutil.rmtree(output_path)  \n",
    "os.mkdir(output_path) \n",
    "\n",
    "# 字体\n",
    "all_fonts = []\n",
    "all_fonts_name = []\n",
    "for root, dirs, files in os.walk(fonts_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".ttf\"):\n",
    "            all_fonts.append(os.path.join(root, file))\n",
    "            all_fonts_name.append(file[:-4])\n",
    "            \n",
    "with open(txt_file_name) as f:\n",
    "    text_lines = f.readlines()\n",
    "text_lines = [t.strip() for t in text_lines]\n",
    "text_lines = [t for t in text_lines if len(t) > 0]\n",
    "\n",
    "bg_file_lists = [os.path.join(b_image_path, x) for x in os.listdir(b_image_path)]\n",
    "# text_set = [[32, 100, [20,24]],[64,200,[30,28]],[48,150,[26,24]]]\n",
    "font_size_listt = [20,24,28,32]\n",
    "number_of_examples = 100\n",
    "\n",
    "\n",
    "\n",
    "image_list = []\n",
    "output_data =[]\n",
    "\n",
    "for i, txt in enumerate(random.sample(text_lines, number_of_examples)):\n",
    "    select_font_idx = np.random.randint(len(all_fonts))\n",
    "    img_height, img_width = 32, 100\n",
    "    font_size = font_size_list[np.random.randint(len(font_size_list))]\n",
    "    image = gen.make_image(test_str=txt, fonttype=all_fonts[select_font_idx], \n",
    "                         fontsize=font_size,target_width=img_width, target_height=img_height, \n",
    "                         back_ground_img_list=bg_file_lists, \n",
    "                         need_include=True)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "    thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11,10)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "    dilate = cv2.dilate(thresh, kernel, iterations=1)\n",
    "    erode = cv2.erode(dilate, kernel, iterations=1)       \n",
    "    image = erode.copy()\n",
    "#     print('image size -->', image.shape)\n",
    "    img_path = os.path.join(output_path, str(i)+'_'+all_fonts_name[select_font_idx] + \".png\")\n",
    "#     image_list.append(image)\n",
    "    cv2.imwrite(img_path, image)    \n",
    "    txt = txt.replace('＋','+')\n",
    "    txt = txt.replace('－','-')\n",
    "    txt = txt.replace('（','(')\n",
    "    txt = txt.replace('）',')')\n",
    "    output_data.append({\n",
    "        \"image_path\": \"{}_{}.png\".format(str(i), all_fonts_name[select_font_idx]),\n",
    "        \"gt\": txt.lower()\n",
    "    })    \n",
    "            \n",
    "train_cnt = int(number_of_examples * 1.0)\n",
    "val_cnt = number_of_examples - train_cnt\n",
    "with open(os.path.join(path,'hand', 'training.json'), 'w') as f:\n",
    "    json.dump(output_data[:train_cnt], f)\n",
    "\n",
    "with open(os.path.join(path,'hand', 'validation.json'), 'w') as f:\n",
    "    json.dump(output_data[train_cnt:], f)\n",
    "\n",
    "print('over')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples length -- > 100\n",
      "Created dataset with 100 samples\n"
     ]
    }
   ],
   "source": [
    "# 生成训练库\n",
    "import json\n",
    "from lib.gen_dataset import createDataset\n",
    "import os\n",
    "img_path = os.path.join(path,'hand','train')\n",
    "train_file = os.path.join(path, 'hand','training.json') \n",
    "valid_file = os.path.join(path, 'hand','validation.json') \n",
    "with open(train_file,'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "imgLabelLists = []\n",
    "for item in train_data:\n",
    "    imgLabelLists.append((os.path.join(path,'hand','train',item['image_path']) ,str(item['gt'])))\n",
    "\n",
    "random.shuffle(imgLabelLists)\n",
    "train_lmdb_path = os.path.join(path,'hand','lmdb')   # 训练数据\n",
    "trainImgPaths = [x[0] for x in imgLabelLists]\n",
    "trainTxtLists = [x[1] for x in imgLabelLists]\n",
    "createDataset(train_lmdb_path, trainImgPaths, trainTxtLists, lexiconList=None, checkValid=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step --> torch.Size([90, 3, 32, 115]) --> 90\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "# traindata loader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import lib.dataset as dataset\n",
    "import lib.utils as utils\n",
    "import time\n",
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(utils)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_dataset = dataset.lmdbDataset(root= os.path.join(path,'hand','lmdb'))\n",
    "# print('train data set length -->{}'.format(len(train_dataset)))\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset,\n",
    "                                                   [int(0.9*len(train_dataset)), \n",
    "                                                    int(0.1*len(train_dataset))])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=dataset.adjustCollate(imgH=32, keep_ratio=False))\n",
    "#     \n",
    "\n",
    "for step, values in enumerate(train_loader):\n",
    "    images = Variable(values[0].type(torch.FloatTensor), requires_grad=False)\n",
    "#     images = values[0]\n",
    "    print('step -->', images.size(), '-->', len(images))\n",
    "    break\n",
    "\n",
    "print('over')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0667, 0.0667, 0.0745,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0667, 0.0667, 0.0745,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0745, 0.0745, 0.0745,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.0980, 0.0980, 0.0980,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0980, 0.0980, 0.0980,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0980, 0.0980, 0.0980,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.0745, 0.0745, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0745, 0.0745, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.1059, 0.1059, 0.1059,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.1059, 0.1059, 0.1059,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.1059, 0.1059, 0.1059,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.0588, 0.0588, 0.0667,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0588, 0.0588, 0.0667,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0667, 0.0667, 0.0667,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.0902, 0.0902, 0.0902,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# image 统一长度\n",
    "PADDING_CONSTANT = 255\n",
    "assert len(set([b.shape[0] for b in image_list])) == 1\n",
    "assert len(set([b.shape[2] for b in image_list])) == 1\n",
    "\n",
    "dim0 = image_list[0].shape[0]\n",
    "dim1 = max([b.shape[1] for b in image_list])\n",
    "dim2 = image_list[0].shape[2]\n",
    "\n",
    "image_list_new = np.full((len(image_list), dim0, dim1, dim2), PADDING_CONSTANT).astype(np.long)\n",
    "print(len(image_list_new))\n",
    "for idx, image in enumerate(image_list_new):\n",
    "    image[:,:image_list[idx].shape[1],:] = image_list[idx]\n",
    "    img_path = os.path.join(output_path, str(idx)+'_'+all_fonts_name[idx%len(all_fonts)] + \".png\")\n",
    "    cv2.imwrite(img_path, image)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# opencv 文本检测"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://www.learnopencv.com/tag/text-detection/\n",
    "https://stackoverflow.com/questions/46105401/improve-text-area-detection-opencv-python\n",
    "https://stackoverflow.com/questions/44334078/recognize-text-in-images-using-canny-edge-detection-in-opencv\n",
    "https://stackoverflow.com/questions/37771263/detect-text-area-in-an-image-using-python-and-opencv\n",
    "https://stackoverflow.com/questions/42174563/easy-ways-to-detect-and-crop-blocks-paragraphs-of-text-out-of-image\n",
    "\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('1.png')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (9,9), 0)\n",
    "thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11,30)\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9,9))\n",
    "dilate = cv2.dilate(thresh, kernel, iterations=4)\n",
    "\n",
    "cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "ROI_number = 0\n",
    "for c in cnts:\n",
    "    area = cv2.contourArea(c)\n",
    "    if area > 10000:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 3)\n",
    "        # ROI = image[y:y+h, x:x+w]\n",
    "        # cv2.imwrite('ROI_{}.png'.format(ROI_number), ROI)\n",
    "        # ROI_number += 1\n",
    "\n",
    "cv2.imshow('thresh', thresh)\n",
    "cv2.imshow('dilate', dilate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 63, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 二值化数据\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 10, 5\n",
    "\n",
    "image = cv2.imread('d:\\\\img\\\\exam\\\\F_2.png')\n",
    "print(image.shape)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,31,10)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "dilate = cv2.dilate(thresh, kernel, iterations=1)\n",
    "erode = cv2.erode(dilate, kernel, iterations=1)\n",
    "img = cv2.bitwise_not(erode)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "cv2.imwrite('d:\\\\img\\\\exam\\\\D_5.png',img)\n",
    "plt.imshow(img,'brg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量修改文件名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:08:02.930659Z",
     "start_time": "2020-06-09T03:08:02.758680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub path: D:\\PROJECT_TW\\git\\data\\ocr\\images\\SampleG\n",
      "修改前： ['G_1000.png', 'G_1001.png', 'G_1002.png', 'G_1003.png', 'G_1004.png', 'G_1005.png', 'G_1006.png', 'G_1007.png', 'G_1008.png', 'G_1009.png', 'G_1010.png', 'G_1011.png', 'G_1012.png', 'G_1013.png', 'G_1014.png', 'G_1015.png', 'G_1016.png', 'G_1017.png', 'G_1018.png', 'G_1019.png', 'G_1020.png', 'G_1021.png', 'G_1022.png', 'G_1023.png', 'G_1024.png', 'G_1025.png', 'G_1026.png', 'G_1027.png', 'G_1028.png', 'G_1029.png', 'img_crop029d28d3-0226-44b4-98b4-bc13a64917ad.png', 'img_crop0402718b-7ac4-447f-9f27-0b86579b4c5e.png', 'img_crop04f37785-f924-456b-a5c2-48037c9e7294.png', 'img_crop06215a95-8933-4d1a-bf43-df44300bbb21.png', 'img_crop0912a574-362b-4c17-9519-712ac71583fc.png', 'img_crop0abd4aa6-4e3c-4273-a85f-23ecdb64f3de.png', 'img_crop0adb5d52-e8d1-4fbc-8b8a-b3948900df03.png', 'img_crop0c4e2e9c-a70d-4a05-968b-3f4006f31094.png', 'img_crop0ede03e5-9852-4527-bcbb-882d83368c57.png', 'img_crop0f7f697d-a018-41fb-9121-71f63eba169e.png', 'img_crop117cfeed-f0a9-440a-89e6-9a9f1b890760.png', 'img_crop13e708a1-9cbe-4e0b-ab50-2b500d07d23c.png', 'img_crop167ed816-3b83-47ef-9b3c-b063efdf32e4.png', 'img_crop1799e3a1-2cde-4184-97ee-11bec7bb0631.png', 'img_crop1cb0d891-0994-49e1-b877-1ba1b099b71e.png', 'img_crop203a8578-0126-4a7d-a554-c56de48b6bbb.png', 'img_crop20b76f2c-3f0f-4bf5-8292-64e09f8fcdfd.png', 'img_crop23b90121-d346-4874-842d-83d53393eb46.png', 'img_crop257144ef-3364-4123-89d7-6c01b4a32d5d.png', 'img_crop26378254-e611-4ab2-86ae-b4a5dce5ad6e.png', 'img_crop29246b1c-34d4-457d-8d69-9b2bf4df1ec0.png', 'img_crop2a83c368-87a0-47be-be84-15bda922eb9f.png', 'img_crop2c2769a0-593b-40b3-96a6-0143bf249b33.png', 'img_crop2c59a88f-e760-4484-998e-26fd2b31ee0d.png', 'img_crop2e995b7e-41d5-499b-805a-557d362e57b1.png', 'img_crop2f3750af-54e6-4eec-8f68-41f77231b6ff.png', 'img_crop2f5a859b-2b5b-4f68-9dce-6649cce303fd.png', 'img_crop31c26e00-0ca2-4dc0-9402-4563459f9f78.png', 'img_crop3343aba5-e406-426e-b02d-b3001a7e391c.png', 'img_crop34afa66a-e224-4271-a04c-0efd48fc273f.png', 'img_crop34e903a4-5c1d-4fdf-8630-65f71bb72dd0.png', 'img_crop365396c8-c3a4-4e51-bc3c-175d101fe17a.png', 'img_crop37a0ce43-25f9-4314-9e74-423ae20dfe8b.png', 'img_crop391cbc9b-c24e-4c6b-90e0-9ee931d7ae7d.png', 'img_crop3b1d2ca4-b297-4261-aa4c-4689f1061cbb.png', 'img_crop3c73db8d-7a29-488a-98a0-ec566f375784.png', 'img_crop3ef1558a-177f-428f-80c0-ebdacd45b798.png', 'img_crop43b51ff1-b157-4fe2-b678-4450a0581e99.png', 'img_crop43cb7617-c2d0-4223-8faa-e0a5fc6ffb0b.png', 'img_crop47779b6a-3b6f-4e21-b3b9-22a639bf8771.png', 'img_crop47eb9c39-f1a1-4d62-a2a4-c2246d62c7dd.png', 'img_crop495fcbc4-ef9a-4a3d-ba5c-20d7ba9de913.png', 'img_crop49685efa-d05c-4801-a42a-adc6a2884bd4.png', 'img_crop4a3e2e39-365a-4bd0-a8e5-a288eb1468c3.png', 'img_crop4bf90d0f-c316-4e3e-a5e2-d653754b0c27.png', 'img_crop4db14d8d-d3e6-494c-ade6-13c5029e401b.png', 'img_crop4e70654f-b0e1-4806-b893-65cb26148bbe.png', 'img_crop54b91049-b96c-4dbc-b64e-bc55dd37fe8b.png', 'img_crop55ea13c3-05b2-423e-9f45-a6d316b370df.png', 'img_crop55fcdb85-8f78-4b54-9f81-c425ec696f6e.png', 'img_crop56518190-b6b5-46bd-94fa-7273ae5dbf31.png', 'img_crop5ca34622-81c2-4ff2-97d0-ab2ff26b5701.png', 'img_crop5eaca417-d6a1-4c7e-8094-f182539c9755.png', 'img_crop5fb08c4c-9a94-422e-8438-742a8b62c2c8.png', 'img_crop5fc400ea-57e1-4e61-9642-dfebba9cf056.png', 'img_crop625070e0-d3c2-4f22-a2b9-d8bee139d1ef.png', 'img_crop655956c7-71a2-4411-82af-870ed2241054.png', 'img_crop682be954-fda8-4a2c-98de-068e889c1f0b.png', 'img_crop6c471e76-800f-4f57-a536-a92f1f571999.png', 'img_crop71da5bef-18b0-45ee-a08b-ede4b1cf370b.png', 'img_crop72c9680b-42dc-4124-abbb-733e877df1e9.png', 'img_crop745a76bb-4a15-438b-b2d7-6ebf956a3ac0.png', 'img_crop76a0b9e2-a19b-41cc-9c5c-2352c5b1e8a2.png', 'img_crop787f329b-6899-4f7b-b969-0c0377bd6b15.png', 'img_crop7afaef2a-5cde-4494-b17e-e5a1a0fff1e4.png', 'img_crop7c8a71da-159b-4cb3-8c9d-b018de602754.png', 'img_crop7c9b4788-71a3-495f-8ab8-02a18074fdd4.png', 'img_crop7cc12e2c-1672-4737-9b96-2cd92932c464.png', 'img_crop8429afb8-74d8-4eab-b817-ebbd6b55321d.png', 'img_crop934ceb57-1260-4c84-87a8-29c881cdd94f.png', 'img_crop93e1bbdf-5ac9-4f20-bc41-31aa6c782e86.png', 'img_crop94de2d11-7caf-4909-9990-b7e3e35f14b5.png', 'img_crop961672c8-faea-4336-a74e-7497b684df8e.png', 'img_crop98a4b453-8bf4-4a66-b5d6-999ba5ae10a5.png', 'img_crop9ad0de97-cdbb-4121-927d-66d4837c41f6.png', 'img_cropa0770f94-4b9d-4adc-a7db-881e2b553859.png', 'img_cropa203da4c-9ffa-4ec1-89e5-3c40230fba7c.png', 'img_cropa4af470e-64be-4c68-9c5f-e49462e8b337.png', 'img_cropa4d85bb3-b77d-4cda-82e6-1504147e52dd.png', 'img_cropa8e6564d-e68b-4b04-8418-8778149d0017.png', 'img_cropab63daa1-0660-4001-a813-060efc9b7c7a.png', 'img_cropace95329-520a-4eee-94e5-26a27115a1bc.png', 'img_cropaf70045c-a79e-480f-9371-ed56eb689f89.png', 'img_cropb230fa48-6d33-4500-9f3e-89c619561818.png', 'img_cropb3774231-d11f-4dc1-a41e-fff82e2c18ed.png', 'img_cropb42f948c-a30c-473f-9982-8ca5c4710a63.png', 'img_cropb8e3a1d2-7610-4145-8174-a6b7051c30a5.png', 'img_cropbf57420a-a1a9-49b7-a343-6edc0c38f758.png', 'img_cropbf64b888-4934-4d6e-91db-1da50a9a71f0.png', 'img_cropbf99e033-dd98-4153-95d2-0177965afb33.png', 'img_cropc0fa25ec-b3bf-42f9-a8e7-ab6211cdb5e1.png', 'img_cropc0fdde3a-2b6d-4a28-a47a-acd1dc2acf51.png', 'img_cropc3b8680d-7905-4935-a061-c122382cd8eb.png', 'img_cropc44b49ae-bf66-4280-80cc-f2fa6f2b6cab.png', 'img_cropc44c4db5-0731-4cda-af37-d2c8ec6f996e.png', 'img_cropc5981d44-6aaa-40cc-b5cd-4409df8d0edd.png', 'img_cropc5db3c06-8429-41f0-bb60-168795f6f500.png', 'img_cropc6d17044-9f8f-4c7e-9481-58712761031f.png', 'img_cropc82fbc46-109d-4e82-8a27-6cc6d04f41f9.png', 'img_cropcb29716c-2ddd-46d9-adfa-f8dcd709c1ef.png', 'img_cropd34d699f-1356-4de1-800c-cbc96efb875e.png', 'img_cropd96b2607-9335-4132-bff5-d629a999ea30.png', 'img_cropda4b43d9-d118-4c12-b9e6-299ed89157a8.png', 'img_cropdcc04843-4df1-4aaa-81b0-ad1f48808fd4.png', 'img_cropded122f9-8d12-483e-a3ee-8c33a92ebdf5.png', 'img_crope056d4cb-32a7-4f5e-82b5-20620c5cc6ba.png', 'img_crope93f7f1b-540e-45c9-8015-285c747f0330.png', 'img_cropef190bec-d9a2-48ae-9c4c-42148ec097ec.png', 'img_cropef1cbc67-5cbe-4ff2-9467-2bb8d27dc246.png', 'img_cropefe959d2-738e-4cee-880b-d7fb2b47e91c.png', 'img_cropf76f5944-5917-4302-a939-3fd008f50c86.png', 'img_cropf804c8a4-b30b-40ac-92e2-73ba5f78f87c.png', 'img_cropf80d7263-3c70-454d-a31d-0e08f1f30d81.png', 'img_cropf8bc8046-9a4a-4ffb-b246-4059d0bf55f6.png', 'img_cropfbad2dfc-f138-4e1d-bc3e-6998e7c6f039.png', 'img_cropfca912d9-dd65-41b6-b56a-0a89674387f2.png', 'img_cropfd8f1a7c-251f-4085-adf8-cbf33d6a76b8.png', 'img_cropfecf44c8-5861-4d53-8498-78c9bde1b35a.png', 'img_cropff536f60-ca15-48a3-bb42-45b053fa4b1c.png', 'img_cropffdf2d8d-aa62-46c2-83f4-89b5cf30fb78.png']\n"
     ]
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "# D:\\PROJECT_TW\\git\\data\\ocr\\images\\SampleF\n",
    "path = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\ocr\\\\images'\n",
    "# alpha = ['A','B','C','D','E','F','G','H','I']\n",
    "# alpha = ['×','√']\n",
    "alpha = [u'G',]\n",
    "'''F,G,H'''\n",
    "for item in alpha:\n",
    "    subpath = os.path.sep.join([path,'Sample{}'.format(item)])\n",
    "    print('sub path:', subpath)\n",
    "#     subpath = os.path.join(path,item)\n",
    "    fileList = os.listdir(subpath)\n",
    "    # 输出此文件夹中包含的文件名称\n",
    "    print(\"修改前：\" , fileList)\n",
    "    currentpath = os.getcwd()\n",
    "    os.chdir(subpath)\n",
    "    number = 1000\n",
    "    for fileName in fileList:\n",
    "        os.rename(fileName, '{}_{}.png'.format(item, number))\n",
    "        number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
