{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 生成模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'd:\\\\img\\\\ocr'\n",
    "txt_file_name = os.path.join(path,'hand','char_words.txt')\n",
    "fonts_path = os.path.join(path, 'fonts')\n",
    "output_path = os.path.join(path,'hand','train')\n",
    "b_image_path = os.path.join(path, 'background')\n",
    "img_height = 32\n",
    "img_width = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 生成字符\n",
    "import lib.gen_image as gen\n",
    "import importlib\n",
    "importlib.reload(gen)\n",
    "\n",
    "\n",
    "\n",
    "alpha = 'ABQP'\n",
    "gen.gen_words(txt_file_name, alpha,max_number=4, total=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over\n"
     ]
    }
   ],
   "source": [
    "# 生成训练图片\n",
    "import lib.gen_image as gen\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import importlib\n",
    "import shutil\n",
    "import cv2\n",
    "importlib.reload(gen)\n",
    "\n",
    "# 清空目录\n",
    "shutil.rmtree(output_path)  \n",
    "os.mkdir(output_path) \n",
    "\n",
    "# 字体\n",
    "all_fonts = []\n",
    "all_fonts_name = []\n",
    "for root, dirs, files in os.walk(fonts_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".ttf\"):\n",
    "            all_fonts.append(os.path.join(root, file))\n",
    "            all_fonts_name.append(file[:-4])\n",
    "            \n",
    "with open(txt_file_name) as f:\n",
    "    text_lines = f.readlines()\n",
    "text_lines = [t.strip() for t in text_lines]\n",
    "text_lines = [t for t in text_lines if len(t) > 0]\n",
    "\n",
    "bg_file_lists = [os.path.join(b_image_path, x) for x in os.listdir(b_image_path)]\n",
    "text_set = [[32, 100, [20,24]],[64,200,[30,28]],[48,150,[26,24]]]\n",
    "number_of_examples = 100\n",
    "\n",
    "\n",
    "\n",
    "image_list = []\n",
    "output_data =[]\n",
    "\n",
    "for i, txt in enumerate(random.sample(text_lines, number_of_examples)):\n",
    "    select_font_idx = np.random.randint(len(all_fonts))\n",
    "    img_height, img_width, font_size_list = text_set[np.random.randint(len(text_set))]\n",
    "    font_size = font_size_list[np.random.randint(len(font_size_list))]\n",
    "    image = gen.make_image(test_str=txt, fonttype=all_fonts[select_font_idx], \n",
    "                         fontsize=font_size,target_width=img_width, target_height=img_height, \n",
    "                         back_ground_img_list=bg_file_lists, \n",
    "                         need_include=True)\n",
    "#     print('image size -->', image.shape)\n",
    "    img_path = os.path.join(output_path, str(i)+'_'+all_fonts_name[select_font_idx] + \".png\")\n",
    "#     image_list.append(image)\n",
    "    cv2.imwrite(img_path, image)    \n",
    "    txt = txt.replace('＋','+')\n",
    "    txt = txt.replace('－','-')\n",
    "    txt = txt.replace('（','(')\n",
    "    txt = txt.replace('）',')')\n",
    "    output_data.append({\n",
    "        \"image_path\": \"{}_{}.png\".format(str(i), all_fonts_name[select_font_idx]),\n",
    "        \"gt\": txt.lower()\n",
    "    })    \n",
    "            \n",
    "train_cnt = int(number_of_examples * 1.0)\n",
    "val_cnt = number_of_examples - train_cnt\n",
    "with open(os.path.join(path,'hand', 'training.json'), 'w') as f:\n",
    "    json.dump(output_data[:train_cnt], f)\n",
    "\n",
    "with open(os.path.join(path,'hand', 'validation.json'), 'w') as f:\n",
    "    json.dump(output_data[train_cnt:], f)\n",
    "\n",
    "print('over')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples length -- > 100\n",
      "Created dataset with 100 samples\n"
     ]
    }
   ],
   "source": [
    "# 生成训练库\n",
    "import json\n",
    "from lib.gen_dataset import createDataset\n",
    "import os\n",
    "img_path = os.path.join(path,'hand','train')\n",
    "train_file = os.path.join(path, 'hand','training.json') \n",
    "valid_file = os.path.join(path, 'hand','validation.json') \n",
    "with open(train_file,'r') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "imgLabelLists = []\n",
    "for item in train_data:\n",
    "    imgLabelLists.append((os.path.join(path,'hand','train',item['image_path']) ,str(item['gt'])))\n",
    "\n",
    "random.shuffle(imgLabelLists)\n",
    "train_lmdb_path = os.path.join(path,'hand','lmdb')   # 训练数据\n",
    "trainImgPaths = [x[0] for x in imgLabelLists]\n",
    "trainTxtLists = [x[1] for x in imgLabelLists]\n",
    "createDataset(train_lmdb_path, trainImgPaths, trainTxtLists, lexiconList=None, checkValid=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step --> torch.Size([90, 3, 32, 115]) --> 90\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "# traindata loader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import lib.dataset as dataset\n",
    "import lib.utils as utils\n",
    "import time\n",
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(utils)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_dataset = dataset.lmdbDataset(root= os.path.join(path,'hand','lmdb'))\n",
    "# print('train data set length -->{}'.format(len(train_dataset)))\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset,\n",
    "                                                   [int(0.9*len(train_dataset)), \n",
    "                                                    int(0.1*len(train_dataset))])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=dataset.adjustCollate(imgH=32, keep_ratio=False))\n",
    "#     \n",
    "\n",
    "for step, values in enumerate(train_loader):\n",
    "    images = Variable(values[0].type(torch.FloatTensor), requires_grad=False)\n",
    "#     images = values[0]\n",
    "    print('step -->', images.size(), '-->', len(images))\n",
    "    break\n",
    "\n",
    "print('over')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0667, 0.0667, 0.0745,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0667, 0.0667, 0.0745,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0745, 0.0745, 0.0745,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.0980, 0.0980, 0.0980,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0980, 0.0980, 0.0980,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0980, 0.0980, 0.0980,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.0745, 0.0745, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0745, 0.0745, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.1059, 0.1059, 0.1059,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.1059, 0.1059, 0.1059,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.1059, 0.1059, 0.1059,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.0588, 0.0588, 0.0667,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0588, 0.0588, 0.0667,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0667, 0.0667, 0.0667,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.0902, 0.0902, 0.0902,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0824, 0.0824, 0.0824,  ..., 1.0000, 1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# image 统一长度\n",
    "PADDING_CONSTANT = 255\n",
    "assert len(set([b.shape[0] for b in image_list])) == 1\n",
    "assert len(set([b.shape[2] for b in image_list])) == 1\n",
    "\n",
    "dim0 = image_list[0].shape[0]\n",
    "dim1 = max([b.shape[1] for b in image_list])\n",
    "dim2 = image_list[0].shape[2]\n",
    "\n",
    "image_list_new = np.full((len(image_list), dim0, dim1, dim2), PADDING_CONSTANT).astype(np.long)\n",
    "print(len(image_list_new))\n",
    "for idx, image in enumerate(image_list_new):\n",
    "    image[:,:image_list[idx].shape[1],:] = image_list[idx]\n",
    "    img_path = os.path.join(output_path, str(idx)+'_'+all_fonts_name[idx%len(all_fonts)] + \".png\")\n",
    "    cv2.imwrite(img_path, image)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# opencv 文本检测"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.learnopencv.com/tag/text-detection/\n",
    "https://stackoverflow.com/questions/46105401/improve-text-area-detection-opencv-python\n",
    "https://stackoverflow.com/questions/44334078/recognize-text-in-images-using-canny-edge-detection-in-opencv\n",
    "https://stackoverflow.com/questions/37771263/detect-text-area-in-an-image-using-python-and-opencv\n",
    "https://stackoverflow.com/questions/42174563/easy-ways-to-detect-and-crop-blocks-paragraphs-of-text-out-of-image\n",
    "\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('1.png')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (9,9), 0)\n",
    "thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11,30)\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9,9))\n",
    "dilate = cv2.dilate(thresh, kernel, iterations=4)\n",
    "\n",
    "cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "\n",
    "ROI_number = 0\n",
    "for c in cnts:\n",
    "    area = cv2.contourArea(c)\n",
    "    if area > 10000:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 3)\n",
    "        # ROI = image[y:y+h, x:x+w]\n",
    "        # cv2.imwrite('ROI_{}.png'.format(ROI_number), ROI)\n",
    "        # ROI_number += 1\n",
    "\n",
    "cv2.imshow('thresh', thresh)\n",
    "cv2.imshow('dilate', dilate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 565, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 10, 5\n",
    "\n",
    "image = cv2.imread('d:\\\\img\\\\exam\\\\mh_1.png')\n",
    "print(image.shape)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11,15)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n",
    "dilate = cv2.dilate(thresh, kernel, iterations=4)\n",
    "\n",
    "plt.imshow(dilate,'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
