{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "goolge ocr mathematics formula\n",
    "2、https://blog.csdn.net/Jeremy_lf/article/details/102728778  https://blog.csdn.net/Jeremy_lf/article/details/102869629 数学公式识别论文一\n",
    "3、https://github.com/luopeixiang/im2latex ！！！\n",
    "4、https://guillaumegenthial.github.io/image-to-latex.html ！！！\n",
    "5、https://github.com/harvardnlp/im2markup\n",
    "5、https://www.ocr.org.uk/Images/73472-datasheet-examination-formulae-and-statistical-tables.pdf ！！\n",
    "6、http://lstm.seas.harvard.edu/latex/ Image-to-Markup Generation with Coarse-to-Fine Attention\n",
    "7、https://github.com/LinXueyuanStdio/LaTeX_OCR ！！！\n",
    "8、https://blog.csdn.net/wxplol/article/details/99941160 ！！！！  其中包括位置信息的解释\n",
    "9、https://www.ocr.org.uk/Images/73472-datasheet-examination-formulae-and-statistical-tables.pdf \n",
    "10、https://arxiv.org/ftp/arxiv/papers/1908/1908.11415.pdf Translating Math Formula Images to LaTeX Sequences Using Deep Neural Networks with Sequence-level Training \n",
    "11、https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=1599&context=etd_projects ！\n",
    "12、https://towardsdatascience.com/deep-learning-for-symbolic-mathematics-5830b22063d0\n",
    "13、http://lstm.seas.harvard.edu/latex/\n",
    "14、https://arxiv.org/ftp/arxiv/papers/2003/2003.00817.pdf 手写体数学公式识别\n",
    "15、https://blog.csdn.net/AckClinkz/article/details/78279074  BLEU1（bilingual evaluation understudy）最早由IBM提出，用来评价翻译质量\n",
    "16、https://blog.csdn.net/lnformat/article/details/88639607\n",
    "17、https://zhidao.baidu.com/question/1990959368206778587.html 数学中的Sin和Cos是什么意思 （在一个平面直角坐标系中，以原点为圆心，1 为半径画一个圆，这个圆交 x 轴于 A 点。以 O 为旋转中心，将 A 点逆时针旋转一定的角度α至 B 点，设此时 B 点的坐标是(x,y)，那么此时 y 的值就叫做α的正弦，记作 sinα；此时 x 的值就叫做α的余弦，）\n",
    "18、https://github.com/antonvladyka/neuralnetworksanddeeplearning.com.pdf\n",
    "19、https://github.com/harvardnlp/im2markup \n",
    "20、https://github.com/luopeixiang ！！！！！\n",
    "21、https://www.zhihu.com/question/347678607/answer/834903728 如何理解Transformer论文中的positional encoding，和三角函数有什么关系？\n",
    "22、https://blog.csdn.net/qq_16234613/article/details/83012046 NLP 自然语言处理 集束搜索beam search和贪心搜索greedy search\n",
    "23、https://github.com/jtyoui/Jtyoui/tree/master/jtyoui/statistics/maths \n",
    "24、https://github.com/rsmith-nl/texcalc 计算latex 结果\n",
    "25、https://github.com/roniemartinez/latex2mathml MathML H5数学表达式\n",
    "26、https://www.cnblogs.com/3daytears/p/9236175.html 生成latex格式公式\n",
    "27、https://pylatexenc.readthedocs.io/en/latest/ convert latex to regular mathematical\n",
    "28、https://www.cnblogs.com/3daytears/p/9236175.html ！！！\n",
    "29、https://pylatexenc.readthedocs.io/en/latest/latexwalker/ Simple Parser for LaTeX Code\n",
    "30、https://github.com/augustt198/latex2sympy\n",
    "31、https://github.com/jungomi/math-formula-recognition Multi-Scale Attention with Dense Encoder for Handwritten Mathematical Expression Recognition.\n",
    "32、https://blog.csdn.net/cj151525/article/details/95756847 python sympy\n",
    "33、https://cloud.tencent.com/act/event/ocrdemo 腾讯云识别\n",
    "34、https://blog.csdn.net/m_buddy/article/details/85178900 《SCA-CNN：Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning》论文笔记 ！！！！\n",
    "35、https://blog.csdn.net/qq_37014750/article/details/83989334 论文阅读：A2-Nets: Double Attention Networks\n",
    "36、 https://www.jianshu.com/p/a617d20162cf 详解编辑距离(Edit Distance)及其代码实现\n",
    "37、https://arxiv.org/abs/1805.09461 Deep Reinforcement Learning For Sequence to Sequence Models  ！！！\n",
    "38、https://www.techleer.com/articles/460-practical_rl-reinforcement-learning-for-seq2seq-pytorch-tensorflow-theano/ ！！！\n",
    "39、https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/cs224n-2017-lecture14-highlight.pdf ！\n",
    "40、https://blog.csdn.net/tudaodiaozhale/article/details/98511081 【自然语言处理】聊聊曝光误差（Exposure Bias）怎么被解决的 \n",
    "41、https://www.jianshu.com/p/9643cba47655  Pytorch中的学习率衰减方法\n",
    "42、https://www.jianshu.com/p/53576b2a7122  CTPN：自然图像文本检测\n",
    "convert LaTeX into a regular mathematical expression ????\n",
    "goolge seq2seq reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latex  数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:35:33.030519Z",
     "start_time": "2020-05-01T14:35:32.983649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import Latex\n",
    "from pytexit import py2tex\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "import io\n",
    "\n",
    "# ss = py2tex(r'sqrt(2/2 + 100)')\n",
    "# print('ss ->', ss)\n",
    "# Latex(ss)\n",
    "# 下载 anltr https://www.cnblogs.com/solvit/p/10097234.html,\n",
    "# 执行 antlr4 PS.g4 -o gen 生成解释代码  https://github.com/augustt198/latex2sympy\n",
    "# import sympy\n",
    "# sympy.simplify('sqrt(2/2-0.5)')\n",
    "#注意 sympy开根号不显示无理数，只会sqrt方式显示。比如8–√=22–√8 ​=22​\n",
    "\n",
    "# latex 转图稿保存\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "def latex_to_img(tex, family='serif'):\n",
    "    buf = io.BytesIO()\n",
    "    plt.axis('off')\n",
    "    plt.rc('text', usetex=False)\n",
    "# params = {'mathtext.fontset': 'stixsans'}\n",
    "# plt.rcParams.update(params)    \n",
    "    # ['dejavusans', 'dejavuserif', 'cm', 'stix', 'stixsans', 'custom']\n",
    "    plt.rc('mathtext',fontset='cm')\n",
    "#     plt.rc('mathtext',default='regular')\n",
    "    plt.rc('font', family=family)\n",
    "    plt.text(0.5, 0.5, r\"$%s$\" % tex,fontsize = 40, ha='center', va='center')\n",
    "    plt.savefig(buf,format='png',transparent=False,pad_inches=0,dpi=300)\n",
    "    plt.close()\n",
    "    image = Image.open(buf)\n",
    "    return image\n",
    "\n",
    "def image_ract(image_array):\n",
    "    x_array, y_array = np.where(image_array==1)\n",
    "    return x_array[np.argmin(x_array)] -2 , y_array[np.argmin(y_array)] -2 , x_array[np.argmax(x_array)] + 2, y_array[np.argmax(y_array)] + 2\n",
    "\n",
    "def get_latex_image(tex,family='serif'):\n",
    "    image = latex_to_img(tex,family)\n",
    "    image_array = np.array(image.copy().convert(\"L\"))\n",
    "    image_array_new = 1 - image_array/255 \n",
    "    x1,y1,x2,y2 = image_ract(image_array_new)\n",
    "    image = image.crop((y1,x1,y2,x2))\n",
    "    image = image.convert('RGB')\n",
    "    return image\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:36:18.252677Z",
     "start_time": "2020-05-01T14:36:17.690216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "latex ='\\\\int _ { - \\\\epsilon } ^ { \\\\infty } d l \\\\:  \\\\int _ { - \\\\epsilon }  \\\\sqrt { 4  } + \\\\frac { 99 } { 8 } '\n",
    "# latex = '\\\\sqrt { 4  } + \\\\frac { 9 } { 8 }'\n",
    "image = get_latex_image(latex)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T14:26:38.118698Z",
     "start_time": "2020-04-17T14:26:37.765861Z"
    }
   },
   "outputs": [],
   "source": [
    "# 生成训练，测试数据\n",
    "# https://haolaoshi.blog.csdn.net/article/details/89531570 Latex四则运算符号\n",
    "# -*- coding: UTF-8 -*-\n",
    "from pytexit import py2tex\n",
    "from random import randint\n",
    "\n",
    "formul_files = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\latex_formul_normal.txt'\n",
    "\n",
    "fms = ['\\\\frac {{ {0} }} {{ {1} }} + {{ {2} }}', \n",
    "        '{{ {0} }} \\\\times {{ {1} }} - {2} {3}', \n",
    "       '{0} - \\\\frac {{ {1} ^ {2} }} {{ {3} }} = {4}',\n",
    "       '\\\\sqrt {{ {0} {1} }} + \\\\frac {{ {3} }} {{ {4} }}',\n",
    "       '{0} \\\\div {1} \\\\times \\sqrt {{ {2} }} = {3}']\n",
    "\n",
    "size = 20000\n",
    "formul_lists = []\n",
    "for idx in range(len(fms),len(fms)+size):\n",
    "    fm = fms[idx % len(fms)]\n",
    "    fm = fm.format(randint(0,9),randint(0,9),randint(0,9),randint(0,9),randint(0,9),randint(0,9),randint(0,9),randint(0,9),randint(0,9))\n",
    "    fm = fm + '\\n'\n",
    "    formul_lists.append(fm)\n",
    "\n",
    "with open(formul_files, 'w', encoding='utf8') as f:\n",
    "    f.writelines(formul_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T15:14:27.545499Z",
     "start_time": "2020-04-17T14:55:43.057329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen image : 0\n",
      "gen image : 1000\n",
      "gen image : 2000\n",
      "gen image : 3000\n",
      "gen image : 4000\n",
      "gen image : 5000\n",
      "gen image : 6000\n",
      "gen image : 7000\n",
      "gen image : 8000\n",
      "gen image : 9000\n",
      "gen image : 10000\n",
      "gen image : 11000\n",
      "gen image : 12000\n",
      "gen image : 13000\n",
      "gen image : 14000\n",
      "gen image : 15000\n",
      "gen image : 16000\n",
      "gen image : 17000\n",
      "gen image : 18000\n",
      "gen image : 19000\n",
      "生成图片完成 \n"
     ]
    }
   ],
   "source": [
    "# 生成图片数据\n",
    "import os\n",
    "formul_files = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\latex_formul_normal.txt'\n",
    "formul_image_path = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\gen_images'\n",
    "\n",
    "formul_lists = None\n",
    "with open(formul_files,'r', encoding='utf8') as f:\n",
    "    formul_lists = f.readlines()\n",
    "# print(formul_lists)\n",
    "\n",
    "for idx in range(len(formul_lists)):\n",
    "    tex = formul_lists[idx]\n",
    "    tex = tex.replace('\\n','')\n",
    "#     print('tex:', tex)\n",
    "#     print('\\\\frac{8}{6}+{7}')\n",
    "#     tex = '\\\\frac{8}{6}+{7}'\n",
    "    if idx % 1000 == 0:\n",
    "        print('gen image :', idx)\n",
    "    image = get_latex_image(tex)\n",
    "    image.save(os.path.sep.join([formul_image_path,f'{idx}.png']))\n",
    "    \n",
    "print('生成图片完成 ')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T09:46:18.292487Z",
     "start_time": "2020-04-17T09:46:18.285489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RGB'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open('D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\gen_images\\\\0.png')\n",
    "# image = image.convert('RGB')\n",
    "image.mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:34:03.967593Z",
     "start_time": "2020-04-20T00:34:03.452886Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://github.com/luopeixiang/im2latex\n",
    "https://blog.csdn.net/SHU15121856/article/details/104448734 nn.LSTM和nn.LSTMCell的使用\n",
    "\n",
    "'''\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch.distributions.uniform import Uniform\n",
    "\n",
    "def add_positional_features(tensor: torch.Tensor,\n",
    "                            min_timescale: float = 1.0,\n",
    "                            max_timescale: float = 1.0e4):\n",
    "    \"\"\"\n",
    "    Implements the frequency-based positional encoding described\n",
    "    in `Attention is all you Need\n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : ``torch.Tensor``\n",
    "        a Tensor with shape (batch_size, timesteps, hidden_dim).\n",
    "    min_timescale : ``float``, optional (default = 1.0)\n",
    "        The largest timescale to use.\n",
    "    Returns\n",
    "    -------\n",
    "    The input tensor augmented with the sinusoidal frequencies.\n",
    "    \"\"\"\n",
    "    _, timesteps, hidden_dim = tensor.size()\n",
    "    timestep_range = get_range_vector(timesteps, tensor.device).data.float()\n",
    "    # We're generating both cos and sin frequencies,\n",
    "    # so half for each.\n",
    "    num_timescales = hidden_dim // 2\n",
    "    timescale_range = get_range_vector(num_timescales, tensor.device).data.float()\n",
    "\n",
    "    log_timescale_increments = math.log(float(max_timescale) / float(min_timescale)) / float(num_timescales - 1)\n",
    "    inverse_timescales = min_timescale * \\\n",
    "        torch.exp(timescale_range * -log_timescale_increments)\n",
    "\n",
    "    # Broadcasted multiplication - shape (timesteps, num_timescales)\n",
    "    scaled_time = timestep_range.unsqueeze(1) * inverse_timescales.unsqueeze(0)\n",
    "    # shape (timesteps, 2 * num_timescales)\n",
    "    sinusoids = torch.randn(scaled_time.size(0), 2*scaled_time.size(1), device=tensor.device)\n",
    "    sinusoids[:, ::2] = torch.sin(scaled_time)\n",
    "    sinusoids[:, 1::2] = torch.cos(scaled_time)\n",
    "    if hidden_dim % 2 != 0:\n",
    "        # if the number of dimensions is odd, the cos and sin\n",
    "        # timescales had size (hidden_dim - 1) / 2, so we need\n",
    "        # to add a row of zeros to make up the difference.\n",
    "        sinusoids = torch.cat([sinusoids, sinusoids.new_zeros(timesteps, 1)], 1)\n",
    "    return tensor + sinusoids.unsqueeze(0)\n",
    "\n",
    "def get_range_vector(size: int, device) -> torch.Tensor:\n",
    "    return torch.arange(0, size, dtype=torch.long, device=device)\n",
    "\n",
    "INIT = 1e-2\n",
    "\n",
    "class Im2LatexModel(nn.Module):\n",
    "    def __init__(self, out_size, emb_size, dec_rnn_h,\n",
    "                 enc_out_dim=512,  n_layer=1,\n",
    "                 add_pos_feat=False, dropout=0.):\n",
    "        super(Im2LatexModel, self).__init__()\n",
    "\n",
    "        self.cnn_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 1),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1), 0),\n",
    "\n",
    "            nn.Conv2d(256, enc_out_dim, 3, 1, 0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.rnn_decoder = nn.LSTMCell(dec_rnn_h+emb_size, dec_rnn_h)\n",
    "        self.embedding = nn.Embedding(out_size, emb_size)\n",
    "\n",
    "        self.init_wh = nn.Linear(enc_out_dim, dec_rnn_h)\n",
    "        self.init_wc = nn.Linear(enc_out_dim, dec_rnn_h)\n",
    "        self.init_wo = nn.Linear(enc_out_dim, dec_rnn_h)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.beta = nn.Parameter(torch.Tensor(enc_out_dim))\n",
    "        init.uniform_(self.beta, -INIT, INIT)\n",
    "        self.W_1 = nn.Linear(enc_out_dim, enc_out_dim, bias=False)\n",
    "        self.W_2 = nn.Linear(dec_rnn_h, enc_out_dim, bias=False)\n",
    "\n",
    "        self.W_3 = nn.Linear(dec_rnn_h+enc_out_dim, dec_rnn_h, bias=False)\n",
    "        self.W_out = nn.Linear(dec_rnn_h, out_size, bias=False)\n",
    "\n",
    "        self.add_pos_feat = add_pos_feat\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.uniform = Uniform(0, 1)\n",
    "\n",
    "    def forward(self, imgs, formulas, epsilon=1.):\n",
    "        \"\"\"args:\n",
    "        imgs: [B, C, H, W]\n",
    "        formulas: [B, MAX_LEN]\n",
    "        epsilon: probability of the current time step to  use the true previous token\n",
    "        return:\n",
    "        logits: [B, MAX_LEN, VOCAB_SIZE]\n",
    "        \"\"\"\n",
    "        # encoding\n",
    "        encoded_imgs = self.encode(imgs)  # [B, H*W, 512]\n",
    "        # init decoder's states  ？？？？？？？？？？\n",
    "        dec_states, o_t = self.init_decoder(encoded_imgs)\n",
    "        max_len = formulas.size(1)\n",
    "        logits = []\n",
    "        for t in range(max_len):\n",
    "            tgt = formulas[:, t:t+1]\n",
    "            # schedule sampling\n",
    "            if logits and self.uniform.sample().item() > epsilon:\n",
    "                tgt = torch.argmax(torch.log(logits[-1]), dim=1, keepdim=True)\n",
    "            # ont step decoding\n",
    "            dec_states, O_t, logit = self.step_decoding(dec_states, o_t, encoded_imgs, tgt)\n",
    "            logits.append(logit)\n",
    "        logits = torch.stack(logits, dim=1)  # [B, MAX_LEN, out_size]\n",
    "        return logits\n",
    "\n",
    "    def encode(self, imgs):\n",
    "        encoded_imgs = self.cnn_encoder(imgs)  # [B, 512, H', W']\n",
    "        encoded_imgs = encoded_imgs.permute(0, 2, 3, 1)  # [B, H', W', 512]\n",
    "        B, H, W, _ = encoded_imgs.shape\n",
    "        encoded_imgs = encoded_imgs.contiguous().view(B, H*W, -1)\n",
    "        if self.add_pos_feat:\n",
    "            encoded_imgs = add_positional_features(encoded_imgs)\n",
    "        return encoded_imgs\n",
    "\n",
    "    \n",
    "    def step_decoding(self, dec_states, o_t, enc_out, tgt):\n",
    "        \"\"\"\n",
    "            Runing one step decoding\n",
    "            dec_states : (h_t, c_t)\n",
    "            o_t: atten scores\n",
    "            enc_out: pre logits\n",
    "            tgt: pre true target\n",
    "        \"\"\"\n",
    "\n",
    "        prev_y = self.embedding(tgt).squeeze(1)  # [B, emb_size]\n",
    "        inp = torch.cat([prev_y, o_t], dim=1)  # [B, emb_size+dec_rnn_h]\n",
    "        h_t, c_t = self.rnn_decoder(inp, dec_states)  # h_t:[B, dec_rnn_h]\n",
    "        h_t = self.dropout(h_t)\n",
    "        c_t = self.dropout(c_t)\n",
    "\n",
    "        # context_t : [B, C]\n",
    "        context_t, attn_scores = self._get_attn(enc_out, h_t)\n",
    "\n",
    "        # [B, dec_rnn_h]\n",
    "        o_t = self.W_3(torch.cat([h_t, context_t], dim=1)).tanh()\n",
    "        o_t = self.dropout(o_t)\n",
    "\n",
    "        # calculate logit\n",
    "        logit = F.softmax(self.W_out(o_t), dim=1)  # [B, out_size]\n",
    "\n",
    "        return (h_t, c_t), o_t, logit\n",
    "\n",
    "    def _get_attn(self, enc_out, h_t):\n",
    "        \"\"\"Attention mechanism\n",
    "        args:\n",
    "            enc_out: row encoder's output [B, L=H*W, C]\n",
    "            h_t: the current time step hidden state [B, dec_rnn_h]\n",
    "        return:\n",
    "            context: this time step context [B, C]\n",
    "            attn_scores: Attention scores\n",
    "        \"\"\"\n",
    "        # cal alpha\n",
    "        alpha = torch.tanh(self.W_1(enc_out)+self.W_2(h_t).unsqueeze(1))\n",
    "        alpha = torch.sum(self.beta*alpha, dim=-1)  # [B, L]\n",
    "        alpha = F.softmax(alpha, dim=-1)  # [B, L]\n",
    "\n",
    "        # cal context: [B, C]\n",
    "        context = torch.bmm(alpha.unsqueeze(1), enc_out)\n",
    "        context = context.squeeze(1)\n",
    "        return context, alpha\n",
    "\n",
    "    def init_decoder(self, enc_out):\n",
    "        \"\"\"args:\n",
    "            enc_out: the output of row encoder [B, H*W, C]\n",
    "          return:\n",
    "            h_0, c_0:  h_0 and c_0's shape: [B, dec_rnn_h]\n",
    "            init_O : the average of enc_out  [B, dec_rnn_h]\n",
    "            for decoder\n",
    "        \"\"\"\n",
    "        mean_enc_out = enc_out.mean(dim=1)\n",
    "        h = self._init_h(mean_enc_out)\n",
    "        c = self._init_c(mean_enc_out)\n",
    "        init_o = self._init_o(mean_enc_out)\n",
    "        return (h, c), init_o\n",
    "\n",
    "    def _init_h(self, mean_enc_out):\n",
    "        return torch.tanh(self.init_wh(mean_enc_out))\n",
    "\n",
    "    def _init_c(self, mean_enc_out):\n",
    "        return torch.tanh(self.init_wc(mean_enc_out))\n",
    "\n",
    "    def _init_o(self, mean_enc_out):\n",
    "        return torch.tanh(self.init_wo(mean_enc_out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T04:03:58.515550Z",
     "start_time": "2020-04-20T04:02:46.677079Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process train dataset...\n",
      "D:\\PROJECT_TW\\git\\data\\im2latex\\latex_train_filter.txt\n",
      "Save train dataset to D:\\PROJECT_TW\\git\\data\\im2latex\\train.pkl\n",
      "Process validate dataset...\n",
      "D:\\PROJECT_TW\\git\\data\\im2latex\\latex_validate_filter.txt\n",
      "Save validate dataset to D:\\PROJECT_TW\\git\\data\\im2latex\\validate.pkl\n",
      "Process test dataset...\n",
      "D:\\PROJECT_TW\\git\\data\\im2latex\\latex_test_filter.txt\n",
      "Save test dataset to D:\\PROJECT_TW\\git\\data\\im2latex\\test.pkl\n",
      "Writing Vocab File in  D:\\PROJECT_TW\\git\\data\\im2latex\\vocab.pkl len : 24\n"
     ]
    }
   ],
   "source": [
    "# 执行 build_vocab 生成latex 向量表\n",
    "import lib.im2latex.preprocess as pre \n",
    "import lib.im2latex.build_vocab as bv\n",
    "import importlib\n",
    "from os.path import join\n",
    "importlib.reload(pre)\n",
    "importlib.reload(bv)\n",
    "size = 1000\n",
    "splits = [\"validate\", \"test\", \"train\"]\n",
    "data_path = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex'\n",
    "\n",
    "def write_filter_data(split, datas):\n",
    "    split_file = join(data_path, \"latex_{}_filter.txt\".format(split))\n",
    "    with open(split_file,'w') as f:\n",
    "        wdatas = [f'{x}\\n' for x in datas]\n",
    "        f.writelines(wdatas)\n",
    "# bv.build_vocab(data_path)\n",
    "train_data = [f'{x}.png {x}' for x in range(18000)]\n",
    "valid_data = [f'{x}.png {x}' for x in range(18000,19000)]\n",
    "test_data = [f'{x}.png {x}' for x in range(19000,20000)]\n",
    "write_filter_data('train', train_data)\n",
    "write_filter_data('validate', valid_data)\n",
    "write_filter_data('test', test_data)\n",
    "pre.preprocess(data_path,'train')\n",
    "pre.preprocess(data_path,'validate')\n",
    "pre.preprocess(data_path,'test')\n",
    "bv.build_vocab(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T06:22:19.676580Z",
     "start_time": "2020-04-19T06:22:19.574639Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载gc模块\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:34:19.131276Z",
     "start_time": "2020-04-20T00:34:19.081306Z"
    },
    "code_folding": [
     80,
     104
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from lib.im2latex.utils import cal_loss, cal_epsilon\n",
    "import gc\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, optimizer, model, lr_scheduler,\n",
    "                 train_loader, val_loader, args,\n",
    "                 use_cuda=False, init_epoch=1, last_epoch=15):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.args = args\n",
    "        self.step = 0\n",
    "        self.epoch = init_epoch\n",
    "        self.total_step = (init_epoch-1)*len(train_loader)\n",
    "        self.last_epoch = last_epoch\n",
    "        self.best_val_loss = 1e18\n",
    "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    def train(self):\n",
    "        mes = \"Epoch {}, step:{}/{} {:.2f}%, Loss:{:.4f}, Perplexity:{:.4f}\"\n",
    "        self.last_epoch = 1000\n",
    "        args.print_freq = 25\n",
    "        print('max epoch --> ', self.last_epoch,  ' print freq:', args.print_freq)\n",
    "        while self.epoch <= self.last_epoch:\n",
    "            self.model.train()\n",
    "            losses = 0.0\n",
    "            #  tgt4training 字符串开始标记<s>, tgt4cal_loss 字符串结束标记</s>    \n",
    "            for imgs, tgt4training, tgt4cal_loss in self.train_loader:\n",
    "                step_loss = self.train_step(imgs, tgt4training, tgt4cal_loss)\n",
    "                losses += step_loss\n",
    "\n",
    "                # log message\n",
    "                if self.step % self.args.print_freq == 0:\n",
    "                    avg_loss = losses / self.args.print_freq\n",
    "                    print(mes.format(\n",
    "                        self.epoch, self.step, len(self.train_loader),\n",
    "                        100 * self.step / len(self.train_loader),\n",
    "                        avg_loss,\n",
    "                        2**avg_loss\n",
    "                    ))\n",
    "                    losses = 0.0\n",
    "                    gc.collect()\n",
    "\n",
    "            # one epoch Finished, calcute val loss\n",
    "            val_loss = self.validate()\n",
    "            self.lr_scheduler.step(val_loss)\n",
    "\n",
    "            self.save_model('ckpt-{}-{:.4f}'.format(self.epoch, val_loss))\n",
    "            self.epoch += 1\n",
    "            self.step = 0\n",
    "\n",
    "    def train_step(self, imgs, tgt4training, tgt4cal_loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        imgs = imgs.to(self.device)\n",
    "        tgt4training = tgt4training.to(self.device)\n",
    "        tgt4cal_loss = tgt4cal_loss.to(self.device)\n",
    "\n",
    "        '''\n",
    "        Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks\n",
    "        See details in https://arxiv.org/pdf/1506.03099.pdf\n",
    "        '''\n",
    "        epsilon = cal_epsilon(self.args.decay_k, self.total_step, self.args.sample_method)\n",
    "\n",
    "        '''\n",
    "        https://www.zhihu.com/question/60751553 如何理解深度学习源码里经常出现的logits？\n",
    "        logits: 一个事件发生与该事件不发生的比值的对数（统计学习方法-李航 p78）。假设一个事件发生的概率为 p，那么该事件的 logits 为 logit(p) = log(p/1-p) .\n",
    "        '''\n",
    "        logits = self.model(imgs, tgt4training, epsilon)\n",
    "\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = cal_loss(logits, tgt4cal_loss)\n",
    "        self.step += 1\n",
    "        self.total_step += 1\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.model.parameters(), self.args.clip)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0.0\n",
    "        mes = \"Epoch {}, validation average loss:{:.4f}, Perplexity:{:.4f}\"\n",
    "        with torch.no_grad():\n",
    "            for imgs, tgt4training, tgt4cal_loss in self.val_loader:\n",
    "                imgs = imgs.to(self.device)\n",
    "                tgt4training = tgt4training.to(self.device)\n",
    "                tgt4cal_loss = tgt4cal_loss.to(self.device)\n",
    "\n",
    "                epsilon = cal_epsilon(\n",
    "                    self.args.decay_k, self.total_step, self.args.sample_method)\n",
    "                logits = self.model(imgs, tgt4training, epsilon)\n",
    "                loss = cal_loss(logits, tgt4cal_loss)\n",
    "                val_total_loss += loss\n",
    "            avg_loss = val_total_loss / len(self.val_loader)\n",
    "            print(mes.format(\n",
    "                self.epoch, avg_loss, 2**avg_loss\n",
    "            ))\n",
    "#             print('predict:',logits)\n",
    "#             print('result:',tgt4cal_loss)\n",
    "            \n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = avg_loss\n",
    "            self.save_model('best_ckpt')\n",
    "        return avg_loss\n",
    "\n",
    "    def save_model(self, model_name):\n",
    "        if not os.path.isdir(self.args.save_dir):\n",
    "            os.makedirs(self.args.save_dir)\n",
    "        save_path = join(self.args.save_dir, model_name+'.pt')\n",
    "        print(\"Saving checkpoint to {}\".format(save_path))\n",
    "\n",
    "        # torch.save(self.model, model_path)\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': self.epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'lr_sche': self.lr_scheduler.state_dict(),\n",
    "            'epoch': self.epoch,\n",
    "            'args': self.args\n",
    "        }, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:34:26.502984Z",
     "start_time": "2020-04-20T00:34:26.455007Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from lib.im2latex.utils import collate_fn, get_checkpoint\n",
    "from lib.im2latex.data import Im2LatexDataset\n",
    "from lib.im2latex.build_vocab import Vocab, load_vocab\n",
    "def init_loader():\n",
    "    # get args\n",
    "    parser = argparse.ArgumentParser(description=\"Im2Latex Training Program\")\n",
    "    # parser.add_argument('--path', required=True, help='root of the model')\n",
    "    # model args\n",
    "    parser.add_argument(\"--emb_dim\", type=int, default=80, help=\"Embedding size\")\n",
    "    parser.add_argument(\"--dec_rnn_h\", type=int, default=512, help=\"The hidden state of the decoder RNN\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\", help=\"The dataset's dir\")\n",
    "    parser.add_argument(\"--add_position_features\", action='store_true', default=True, help=\"Use position embeddings or not\")\n",
    "    # training args\n",
    "    parser.add_argument(\"--max_len\", type=int, default=150, help=\"Max size of formula\")\n",
    "    parser.add_argument(\"--dropout\", type=float,default=0., help=\"Dropout probility\")\n",
    "    parser.add_argument(\"--cuda\", action='store_true',default=True, help=\"Use cuda or not\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--epoches\", type=int, default=200)\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-4,help=\"Learning Rate\")\n",
    "    parser.add_argument(\"--min_lr\", type=float, default=3e-5, help=\"Learning Rate\")\n",
    "    parser.add_argument(\"--sample_method\", type=str, default=\"teacher_forcing\", choices=('teacher_forcing', 'exp', 'inv_sigmoid'), help=\"The method to schedule sampling\")\n",
    "    parser.add_argument(\"--decay_k\", type=float, default=1.,\n",
    "                        help=\"Base of Exponential decay for Schedule Sampling. \"\n",
    "                        \"When sample method is Exponential deca;\"\n",
    "                        \"Or a constant in Inverse sigmoid decay Equation. \"\n",
    "                        \"See details in https://arxiv.org/pdf/1506.03099.pdf\")\n",
    "    parser.add_argument(\"--lr_decay\", type=float, default=0.5, help=\"Learning Rate Decay Rate\")\n",
    "    parser.add_argument(\"--lr_patience\", type=int, default=3,  help=\"Learning Rate Decay Patience\")\n",
    "    parser.add_argument(\"--clip\", type=float, default=2.0, help=\"The max gradient norm\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\ckpts\", help=\"The dir to save checkpoints\")\n",
    "    parser.add_argument(\"--print_freq\", type=int, default=100, help=\"The frequency to print message\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=2020, help=\"The random seed for reproducing \")\n",
    "    parser.add_argument(\"--from_check_point\", action='store_true', default=False, help=\"Training from checkpoint or not\")\n",
    "    #  注意在 jupyter notebook 需带args=[] 这个参数\n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    from_check_point = args.from_check_point\n",
    "    if from_check_point:\n",
    "        checkpoint_path = get_checkpoint(args.save_dir)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        args = checkpoint['args']\n",
    "    args.epoches=200\n",
    "    args.from_check_point = True\n",
    "    max_epoch = args.epoches\n",
    "    print(\"Training args:\", args)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    # Building vocab\n",
    "    print(\"Load vocab...\")\n",
    "    vocab = load_vocab(args.data_path)\n",
    "#     print('vocab -->', vocab.sign2id)\n",
    "    use_cuda = True if args.cuda and torch.cuda.is_available() else False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # data loader\n",
    "    print(\"Construct data loader...\")\n",
    "#     train_loader = None\n",
    "    train_loader = DataLoader(\n",
    "        Im2LatexDataset(args.data_path, 'train', args.max_len),\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=partial(collate_fn, vocab.sign2id),\n",
    "        pin_memory=True if use_cuda else False,\n",
    "        num_workers=1)\n",
    "    val_loader = DataLoader(\n",
    "        Im2LatexDataset(args.data_path, 'validate', args.max_len),\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=partial(collate_fn, vocab.sign2id),\n",
    "        pin_memory=True if use_cuda else False,\n",
    "        num_workers=1)\n",
    "    print(\"Construct data loader over\")\n",
    "    return train_loader, val_loader,vocab, args\n",
    "\n",
    "def train(train_loader,val_loader, vocab, args):\n",
    "    use_cuda = True if args.cuda and torch.cuda.is_available() else False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")    \n",
    "    # construct model\n",
    "    print(\"Construct model\")\n",
    "    vocab_size = len(vocab)\n",
    "    model = Im2LatexModel(\n",
    "        vocab_size, args.emb_dim, args.dec_rnn_h,\n",
    "        add_pos_feat=args.add_position_features,\n",
    "        dropout=args.dropout\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    print(\"Model Settings:\")\n",
    "    print(model)  \n",
    "    # construct optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    lr_scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        \"min\",\n",
    "        factor=args.lr_decay,\n",
    "        patience=args.lr_patience,\n",
    "        verbose=True,\n",
    "        min_lr=args.min_lr)    \n",
    "    \n",
    "        \n",
    "    if args.from_check_point:\n",
    "        print('from check point .. ')\n",
    "        checkpoint_path = get_checkpoint(args.save_dir)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        args = checkpoint['args']\n",
    "    \n",
    "    if args.from_check_point:\n",
    "        print('from check point .. ')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_sche'])\n",
    "        max_epoch = args.epoches\n",
    "        # init trainer from checkpoint\n",
    "        trainer = Trainer(optimizer, model, lr_scheduler,\n",
    "                          train_loader, val_loader, args,\n",
    "                          use_cuda=use_cuda,\n",
    "                          init_epoch=epoch, last_epoch=max_epoch)\n",
    "    else:\n",
    "        trainer = Trainer(optimizer, model, lr_scheduler,\n",
    "                          train_loader, val_loader, args,\n",
    "                          use_cuda=use_cuda,\n",
    "                          init_epoch=1, last_epoch=1000)\n",
    "    # begin training\n",
    "    trainer.train()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:34:38.447719Z",
     "start_time": "2020-04-20T00:34:29.017800Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training args: Namespace(add_position_features=True, batch_size=32, clip=2.0, cuda=True, data_path='D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex', dec_rnn_h=512, decay_k=1.0, dropout=0.0, emb_dim=80, epoches=200, from_check_point=True, lr=0.0003, lr_decay=0.5, lr_patience=3, max_len=150, min_lr=3e-05, print_freq=100, sample_method='teacher_forcing', save_dir='D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\ckpts', seed=2020)\n",
      "Load vocab...\n",
      "Load vocab including 24 words!\n",
      "Construct data loader...\n",
      "Construct data loader over\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "train_loader, val_loader,vocab, args =  init_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T02:05:12.083561Z",
     "start_time": "2020-04-20T00:34:50.114306Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct model\n",
      "Model Settings:\n",
      "Im2LatexModel(\n",
      "  (cnn_encoder): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (12): ReLU()\n",
      "  )\n",
      "  (rnn_decoder): LSTMCell(592, 512)\n",
      "  (embedding): Embedding(24, 80)\n",
      "  (init_wh): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (init_wc): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (init_wo): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (W_1): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (W_2): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (W_3): Linear(in_features=1024, out_features=512, bias=False)\n",
      "  (W_out): Linear(in_features=512, out_features=24, bias=False)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "from check point .. \n",
      "Get checkpoint from D:\\PROJECT_TW\\git\\data\\im2latex\\ckpts\\ckpt-7-0.8330.pt for training\n",
      "max epoch -->  1000  print freq: 25\n",
      "Epoch 1, step:100/563 17.76%, Loss:1.1925, Perplexity:2.2854\n",
      "Epoch 1, step:200/563 35.52%, Loss:1.1619, Perplexity:2.2375\n",
      "Epoch 1, step:300/563 53.29%, Loss:0.9388, Perplexity:1.9169\n",
      "Epoch 1, step:400/563 71.05%, Loss:1.3174, Perplexity:2.4922\n",
      "Epoch 1, step:500/563 88.81%, Loss:1.0652, Perplexity:2.0924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8f16a2c1ea67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_check_point\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-e6f7acb0f76e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, val_loader, vocab, args)\u001b[0m\n\u001b[0;32m    128\u001b[0m                           init_epoch=1, last_epoch=1000)\n\u001b[0;32m    129\u001b[0m     \u001b[1;31m# begin training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3c7e34dab433>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m#  tgt4training 字符串开始标记<s>, tgt4cal_loss 字符串结束标记</s>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt4training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt4cal_loss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[0mstep_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt4training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt4cal_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstep_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3c7e34dab433>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, imgs, tgt4training, tgt4cal_loss)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project_tw\\twedu\\venv37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\project_tw\\twedu\\venv37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager().update('notebook', {'limit_output': 10000})\n",
    "args.from_check_point=True\n",
    "args.print_freq=25\n",
    "train(train_loader, val_loader,vocab, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T00:33:25.860439Z",
     "start_time": "2020-04-20T00:32:33.893869Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load vocab including 24 words!\n",
      "data loader\n",
      "add_position_features: True\n",
      "begin evulate ..\n",
      "results: ['{ 1 } { 1 } { 5 } - 5 9']\n",
      "tgt4cal_loss: ['{ 1 } \\\\times { 1 } - 5 9']\n",
      "results: ['9 { 1 9 } { 9 9 } 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9', '7 - \\\\frac { 1 } { 1 } - 7 7', '2 { 7 2 2 } { 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2', '1 { 3 1 } + { 5 } - 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1', '9 { 1 9 } + { 6 }', '6 { 1 2 } + { 2 } - 2 2 2 2 2 2 2 2 2 2 2 - 2 { 7 } { 0 } - 2 2 2 2 2 2 2 2 2 2 - 2 { 7 } { 0 } - 2 2 2 2 2 2 2 2 2 2 - 2 { 7 }', '8 { 3 9 } + { 4 }', '8 { 3 2 } { 2 2 } 2 2 2 2 2 2 2 2 2 2 2 2 - 8 { 1 2 } { 2 2 2 2 2 2 2 2 2 2 2 2 2 - 8 { 1 2 } { 2 2 2 2 2 2 2 2 2 2 2 2 2 2 - {', '4 { 5 1 } + { 7 } - 4 4', '4 { 5 1 } + { 2 } - 4 4', '2 { 5 2 } + { 4 } - 1 2 2 2 2 2 2 2 2 2 2 2 2 - { 4 2 } - 2 1 2 2 2 2 2 2 2 2 - { 4 2 } - 2 1 2 2 2 2 2 2 2 2 - { 4 2 } - 2 1 2', '4 { 1 4 } + { 6 }', '3 - { 3 3 3 - 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3', '3 - \\\\frac { 9 3 } { 3 } 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3', '0 { 1 5 } + { 1 } - 5 5']\n",
      "tgt4cal_loss: ['{ 1 } \\\\times { 8 } - 9 9', '{ 3 } \\\\times { 1 } - 7 1', '{ 7 } \\\\times { 2 } - 2 2', '{ 3 } \\\\times { 5 } - 1 1', '{ 1 } \\\\times { 6 } - 9 9', '{ 1 } \\\\times { 7 } - 2 6', '{ 3 } \\\\times { 4 } - 8 5', '{ 3 } \\\\times { 1 } - 8 2', '{ 5 } \\\\times { 7 } - 4 1', '{ 5 } \\\\times { 2 } - 4 1', '{ 5 } \\\\times { 4 } - 1 2', '{ 1 } \\\\times { 8 } - 4 0', '{ 1 } \\\\times { 3 } - 3 8', '{ 7 } \\\\times { 9 } - 3 1', '{ 1 } \\\\times { 1 } - 5 0']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-06c095b2f365>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beam search result:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-06c095b2f365>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mreference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlatex_producer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_idx2formulas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt4cal_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlatex_producer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'results:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tgt4cal_loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\PROJECT_TW\\git\\myproject\\OCR\\lib\\im2latex\\decoding.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, imgs)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_greedy_decoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_beam_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\PROJECT_TW\\git\\myproject\\OCR\\lib\\im2latex\\decoding.py\u001b[0m in \u001b[0;36m_batch_beam_search\u001b[1;34m(self, imgs)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'enc_outs'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc_outs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         all_top_k_predictions, log_probabilities = self._beam_search.search(\n\u001b[1;32m--> 185\u001b[1;33m             start_predictions, state, self._take_step)\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mall_top_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_top_k_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\PROJECT_TW\\git\\myproject\\OCR\\lib\\im2latex\\beam_search.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, start_predictions, start_state, step)\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m                     \u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mlast_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m                     \u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_backpointer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m                     \u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mlast_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "from functools import partial\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import lib.im2latex.data as idata # import Im2LatexDataset\n",
    "from lib.im2latex.build_vocab import Vocab, load_vocab\n",
    "from lib.im2latex.utils import collate_fn\n",
    "from lib.im2latex.decoding import LatexProducer\n",
    "from lib.im2latex.score import score_files\n",
    "import importlib\n",
    "importlib.reload(idata)\n",
    "\n",
    "def evaluate():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Im2Latex Evaluating Program\")\n",
    "    parser.add_argument('--model_path', required=False, help='path of the evaluated model')\n",
    "\n",
    "    # model args\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\", help=\"The dataset's dir\")\n",
    "    parser.add_argument(\"--cuda\", action='store_true',default=False, help=\"Use cuda or not\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--beam_size\", type=int, default=5)\n",
    "    parser.add_argument(\"--result_path\", type=str, default=\"D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\result.txt\", help=\"The file to store result\")\n",
    "    parser.add_argument(\"--ref_path\", type=str, default=\"D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\ref.txt\", help=\"The file to store reference\")\n",
    "    parser.add_argument(\"--max_len\", type=int, default=64, help=\"Max step of decoding\")\n",
    "    parser.add_argument(\"--split\", type=str,default=\"validate\", help=\"The data split to decode\")\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    # 加载 模型\n",
    "    model_path = 'D:\\\\PROJECT_TW\\\\git\\\\data\\\\im2latex\\\\ckpts\\\\best_ckpt.pt'\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model_args = checkpoint['args']\n",
    "\n",
    "    # 读入词典,设置其他相关参数\n",
    "    vocab = load_vocab(args.data_path)\n",
    "    use_cuda = True if args.cuda and torch.cuda.is_available() else False\n",
    "\n",
    "    # 加载测试集\n",
    "    print('data loader')\n",
    "    data_loader = DataLoader(\n",
    "        idata.Im2LatexDataset(args.data_path, args.split, args.max_len),\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=partial(collate_fn, vocab.sign2id),\n",
    "        pin_memory=True if use_cuda else False,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    print('add_position_features:', model_args.add_position_features)\n",
    "    model = Im2LatexModel(\n",
    "        len(vocab), model_args.emb_dim, model_args.dec_rnn_h,\n",
    "        add_pos_feat=model_args.add_position_features,\n",
    "        dropout=model_args.dropout\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    \n",
    "    result_file = open(args.result_path, 'w')\n",
    "    ref_file = open(args.ref_path, 'w')\n",
    "\n",
    "    \n",
    "    latex_producer = LatexProducer(\n",
    "        model, vocab, max_len=args.max_len,\n",
    "        use_cuda=use_cuda, beam_size=args.beam_size)\n",
    "    \n",
    "    print('begin evulate ..')\n",
    "#     for imgs, tgt4training, tgt4cal_loss in tqdm(data_loader):\n",
    "    for imgs, tgt4training, tgt4cal_loss in data_loader:\n",
    "        \n",
    "        try:\n",
    "            reference = latex_producer._idx2formulas(tgt4cal_loss)\n",
    "            results = latex_producer(imgs)\n",
    "            print('results:', results)\n",
    "            print('tgt4cal_loss:', reference)\n",
    "        except RuntimeError:\n",
    "            break\n",
    "\n",
    "        result_file.write('\\n'.join(results))\n",
    "        ref_file.write('\\n'.join(reference))\n",
    "\n",
    "    result_file.close()\n",
    "    ref_file.close()\n",
    "    score = score_files(args.result_path, args.ref_path)\n",
    "    print(\"beam search result:\", score)\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T14:45:40.888809Z",
     "start_time": "2020-04-19T14:45:40.779875Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
