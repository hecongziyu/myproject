{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1、参看 game\n",
    "2、https://www.cnblogs.com/pinard/p/9385570.html 强化学习（一）\n",
    "3、https://www.cnblogs.com/pinard/p/10137696.html 强化学习（十三） 策略梯度(Policy Gradient)\n",
    "4、https://www.cnblogs.com/HongjianChen/p/8718988.html 梯度算法之梯度上升和梯度下降 （在机器学习算法中，在最小化损失函数时，可以通过梯度下降思想来求得最小化的\n",
    "   损失函数和对应的参数值，反过来，如果要求最大化的损失函数，可以通过梯度上升思想来求取。在求极值的问题中，有梯度上升和梯度下降两个最优化方法。梯度上升用于求最大值，梯度下降用于求最小值） ！！！！\n",
    "5、https://blog.csdn.net/u010310527/article/details/77150888  关于梯度上升和梯度下降的理解 ！！！！！\n",
    "6、https://github.com/yandexdataschool/Practical_RL\n",
    "7、https://github.com/pemami4911/neural-combinatorial-rl-pytorch\n",
    "8、https://github.com/yaserkl/RLSeq2Seq\n",
    "9、https://www.jianshu.com/p/1213de861491  Deep Reinforcement Learning For Sequence to Sequence ！！！！！！！！\n",
    "10、https://blog.csdn.net/pacificL/article/details/80396903 强化学习之Q函数的个人理解\n",
    "11、https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6851458.pdf  ！！！\n",
    "12、https://pathmind.com/wiki/deep-reinforcement-learning  A Beginner's Guide to Deep Reinforcement Learning\n",
    "13、https://openreview.net/attachment?id=HygnDhEtvr&name=original_pdf\n",
    "15、https://blog.csdn.net/joshuaxx316/article/details/58696552 BLEU，ROUGE，METEOR，ROUGE-浅述自然语言处理机器翻译常用评价度量\n",
    "16、https://blog.csdn.net/lucky_rocks/article/details/79676095 torch.gather, 根据维度取指定索引位置数据，在更新策略参数时可能有用，参看game     \n",
    "   lib/Double_DQN_RL_brain.py\n",
    "   q_eval = self.eval_net(b_s).gather(1, b_a), q_target = self.target_net(b_s).gather(1,next)\n",
    "   loss(q_eval, q_target), 更新指定位置参数的值\n",
    "17、https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture16-guest.pdf ！\n",
    "18、https://books.google.com.hk/books?id=8cmcDwAAQBAJ&pg=PA591&lpg=PA591&dq=seq2seq+reward&source=bl&ots=gdYwkkRzm6&sig=ACfU3U0LGlG4qDczLRVgxK5JQgi8lSlioQ&hl=zh-CN&sa=X&redir_esc=y&sourceid=cndr#v=onepage&q&f=false ！\n",
    "\n",
    "19、https://www.groundai.com/project/natural-language-generation-using-reinforcement-learning-with-external-rewards/1\n",
    "20、http://www.macs.hw.ac.uk/InteractionLab/E2E/final_papers/E2E-NLE.pdf 这篇论文是一篇综述性质的文章吧，研究了现有的Seq2Seq模型的应用和 ... Reward：奖励考虑立即的奖励和未来的奖励，这里的奖励可以理解为当"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
