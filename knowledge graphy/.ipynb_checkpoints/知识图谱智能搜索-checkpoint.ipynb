{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "0、https://www.zhihu.com/question/358469127/answer/1028144909  机器阅读理解方向有什么值得follow的大佬，网站等等?\n",
    "1、https://baijiahao.baidu.com/s?id=1598439941574842768&wfr=spider&for=pc  智能搜索时代：且看知识的力量\n",
    "2、https://www.opensemanticsearch.org/doc/search/graph\n",
    "3、https://www.zhihu.com/topic/19605866/hot  自动构建知识图谱\n",
    "4、 https://blog.csdn.net/u012879957/article/details/80925076 语义搜索\n",
    "5、https://blog.csdn.net/tianguiyuyu/article/details/81775372?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task 从零起步构建自己的快速语义搜索模型\n",
    "6、https://nlp.stanford.edu/projects/glove/ GloVe: Global Vectors for Word Representation\n",
    "7、https://blog.csdn.net/malefactor/article/details/50878936 使用深度RNN模型构建语义搜索引擎\n",
    "8、https://blog.csdn.net/c313450619/article/details/54408191?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task 使用自然语言查询知识库\n",
    "9、https://zhuanlan.zhihu.com/p/69356170 丁香园在语义匹配任务上的探索与实践\n",
    "10、https://zhuanlan.zhihu.com/p/33537217 深度语义模型以及在淘宝搜索中的应用\n",
    "11、http://km.aifb.kit.edu/ws/sumpre2015/paper4.pdf\n",
    "12、https://zhuanlan.zhihu.com/p/42155415 算法集锦13|自然语言处理| Python代码的语义搜索引擎\n",
    "13、https://blog.csdn.net/xiabenshu/article/details/88854985\n",
    "14、https://blog.csdn.net/zl_best/article/details/53433072?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task word2vec词向量训练及gensim的使用\n",
    "15、http://www.52nlp.cn/%e7%8e%a9%e8%bd%ac%e8%85%be%e8%ae%af%e8%af%8d%e5%90%91%e9%87%8f-%e8%af%8d%e8%af%ad%e7%9b%b8%e4%bc%bc%e5%ba%a6%e8%ae%a1%e7%ae%97%e5%92%8c%e5%9c%a8%e7%ba%bf%e6%9f%a5%e8%af%a2 玩转腾讯词向量：词语相似度计算和在线查询\n",
    "16、http://www.52nlp.cn/%E8%85%BE%E8%AE%AF%E8%AF%8D%E5%90%91%E9%87%8F%E5%AE%9E%E6%88%98-%E9%80%9A%E8%BF%87annoy%E8%BF%9B%E8%A1%8C%E7%B4%A2%E5%BC%95%E5%92%8C%E5%BF%AB%E9%80%9F%E6%9F%A5%E8%AF%A2 腾讯词向量实战：通过Annoy进行索引和快速查询\n",
    "17、https://markroxor.github.io/gensim/static/notebooks/annoytutorial.html  Similarity Queries using Annoy Tutorial\n",
    "18、https://ai.tencent.com/ailab/nlp/embedding.html Tencent AI Lab Embedding Corpus for Chinese Words and Phrases"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "查询分为 关键字查询（如：平行四边形面积计算公式）， 基于内容的查询（如：2的五次方是多少？ ， 有花园是一个长方形， 其中一半面积是。。。 ？） 这两类， 怎么统一 ？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关键字搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "限制，搜索串不长于N， 通过Jaccard相似度匹配到相关知识点。 \n",
    "适用于知识点查找 \n",
    "'''\n",
    "# source 源句子分词后队列，dest需比较的字符串，注意（这里不需要再分词，已经是抽取出来的知识点）\n",
    "def Jaccrad(source, dest):\n",
    "    grams_source = set(source) # 去重；如果不需要就改为list\n",
    "    grams_dest = set(dest)\n",
    "    temp=0\n",
    "    for item in grams_dest:\n",
    "        if item in grams_source:\n",
    "            temp=temp+1\n",
    "    fenmu=len(grams_model)+len(grams_reference)-temp #并集\n",
    "    jaccard_coefficient=float(temp/fenmu)#交集\n",
    "    return jaccard_coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内容分析搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "预先根据习题内容进行知识点分类训练，根据训练好的模型匹配到适合的知识点\n",
    "适用于用户查找习题相关知识点. 注意：习题可以包含多个知识点？？？ 多标签任务 ？？？\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词，实体，ES查询 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n处理流程 一 ：\\n1、资源存入ES\\n2、分词后在ES中进行查询，找到匹配最高的内容，再根据该内容查找到其相关实体再到知识图谱进行查询\\n\\n处理流程 二 ：\\n1、资源信息做词向量类似方式处理，并保存向量信息\\n2、通过计算查询向量相似性找到相关资源，再根据其预设的实体信息到知识图谱中进行查询\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "处理流程 一 ：\n",
    "1、资源存入ES\n",
    "2、分词后在ES中进行查询，找到匹配最高的内容，再根据该内容查找到其相关实体再到知识图谱进行查询\n",
    "2、或直接对查找出来的内容用BERT（或其它）分类模型直接进行分类到知识点\n",
    "\n",
    "处理流程 二 ：\n",
    "1、资源信息做词向量类似方式处理，并保存向量信息\n",
    "2、通过计算查询向量相似性找到相关资源，再根据其预设的实体信息到知识图谱中进行查询\n",
    "\n",
    "处理流程 三：\n",
    "1、通过word2vec, doc2vec模式，采用Annoy索引工具查找分词后相似doc ????  doc量大的情况下怎么满足???\n",
    "\n",
    "本处采用流程一的处理方式，只是ES采用Jaccrad方式本地查找\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022647438713693013"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1、https://blog.csdn.net/u011984148/article/details/99921480 BERT词向量\n",
    "# 2、https://github.com/terrifyzhao/bert-utils 一行代码使用BERT生成句向量，BERT做文本分类、文本相似度计算\n",
    "# 3、https://cloud.tencent.com/developer/article/1461418\n",
    "# 4、https://blog.csdn.net/xuxiatian/article/details/91388480 python向量之间相似性的计算方法\n",
    "# pip install bert-serving-server  # server\n",
    "# pip install bert-serving-client  # client, independent of `bert-serving-server`\n",
    "# bert-serving-start -model_dir D:/PROJECT_TW/git/data/bert/chinese_L-12_H-768_A-12 -num_worker=1\n",
    "from bert_serving.client import BertClient\n",
    "from scipy.spatial.distance import cosine\n",
    "bc = BertClient()\n",
    "embed = bc.encode([\"2的5次方是多少\",\"2的5次方等于多少\"]).tolist()\n",
    "# print(embed[0])\n",
    "cosine(embed[0], embed[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型进行识别 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据文本内容进行分类，可用于习题分类，分类指向不同知识点"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
